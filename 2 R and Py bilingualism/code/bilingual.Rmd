---
title: "R Notebook"
output: html_notebook
---

# Install packages
R libraries are stored and managed in a repository called CRAN. You can download R packages with the install.packages() function

```{r}
install.packages("reticulate")
```
You only need to install packages once, but you need to mount those packages with the library() function each time you open R. 

```{r}
library(reticulate)
```

Python libraries are stored and managed in a few different libraries and their dependencies are not regulated as strictly as R libraries are in CRAN. It's easier to publish a python package but it can also be more cumbersome for users because you need to manage dependencies yourself. You can download python packages using both R and Python code
```{r}
py_install("pandas")

```


# Load packages and change settings
```{r}
options(java.parameters = "-Xmx5G")

library(r5r)
library(sf)
library(data.table)
library(ggplot2)
library(interp)
library(dplyr)
library(osmdata)
library(ggthemes)
library(sf)
library(data.table)
library(ggplot2)
library(akima)
library(dplyr)
library(raster)
library(osmdata)
library(mapview)
library(cowplot)
library(here)
library(testthat)
```

```{python}
import sys
sys.argv.append(["--max-memory", "5G"])

import pandas as pd
import geopandas
import matplotlib.pyplot as plt
import numpy as np
import plotnine
import contextily as cx
import r5py
import seaborn as sns
```

# Load saved data
```{r}
data("iris")

load(file=here("2 R and Py bilingualism", "data", "iris_example_data.rdata"))
objects()
```

```{python}

```

# Save data
```{r}
save(iris, file=here("2 R and Py bilingualism", "data", "iris_example_data.rdata"))

write.csv(iris, file=here("2 R and Py bilingualism", "data", "iris_example_data.csv"))
```

```{python}

```


# Write a function
```{r}
library(rvest)

scrape_wikipedia_table <- function(url) {
  page <- read_html(url)
  table <- html_table(html_nodes(page, "table")[[2]])
  return(table)
}

# Unit tests
library(testthat)

test_that("scrape_wikipedia_table returns a data frame", {
  url <- "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"
  table <- scrape_wikipedia_table(url)
  expect_is(table, "data.frame")
})

test_that("scrape_wikipedia_table returns the correct table", {
  url <- "https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)"
  table <- scrape_wikipedia_table(url)
  expect_equal(dim(table), c(235, 6))
})
```

```{python}
import requests
from bs4 import BeautifulSoup

def scrape_wikipedia_table(url):
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find_all('table')[1]
    return pd.read_html(str(table))[0]
  
  url = "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"
table = scrape_wikipedia_table(url)
```




```{python}
plt.clf()
sns.countplot(x='Species', data=r.iris)
plt.show()
```


```{python}
plt.clf()
sns.pairplot(r.iris, hue = 'Species')
plt.show()
```


```{python}
plt.clf()
correlation_matrix = r.iris.corr()
plt.figure(figsize=(1,1))
sns.heatmap(correlation_matrix, cbar=True, fmt='.1f', annot=True, cmap='Blues')
plt.show()
plt.savefig('Correlation Heat map')
```







```{r, eval=FALSE}
library(shiny)
xy <- c(784,479) #output of grDevices::dev.size("px")
url <- "https://www.youtube.com/watch?v=VO1bQo4PXV4" #copy yt link here
url <- gsub("watch\\?v=","embed/",url)
ui <- fluidPage(
  HTML(paste0('<iframe width="',xy[1],'" height="',xy[2],'" src="',url,'" frameborder="0"></iframe>'))
)
server <- function(input, output, session) {
}

runGadget(shinyApp(ui, server,options=c("launch.browser"=FALSE,"port"=1111)),port=1111,viewer = paneViewer())
```




# Retrieve data from an API








