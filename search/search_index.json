{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#virtual-training-for-collaboration-in-environmental-data-science","title":"Virtual Training for Collaboration in Environmental Data Science","text":"<p>This work is licensed under a Creative Commons Attribution 4.0 International License.</p> <p>Welcome to the Environmental Data Science Innovation &amp; Inclusion Lab (ESIIL)! These training sessions are intended to provide attendees to the ESIIL Innovation Summit with some technical background to help foster innovation and collaboartion during the summit. Through this training, attendees will learn how to access and engage with cloud-based communication and computational tools that will be relied on throughout the summit, fundamental Environmental Data Science (EDS) skills using R and Python, and ways to make your science open and reproducible. These sessions are being recorded and published on KIStorm for anyone who is unable to attend, or for those who would like to go back and revisit these lessons. Finally, we will have  super-fun  virtual and in-person help desks set up during the summit as questions come up and for anyone who would like help troubleshooting.</p>"},{"location":"#agenda","title":"Agenda","text":"<p>Each session will run from 12pm - 2pm Mountain Daylight Time and will meet virtually over Zoom.</p> Current local time inBoulder, United States"},{"location":"#day-1-wednesday-may-17","title":"Day 1 (Wednesday May, 17)","text":""},{"location":"#collaborating-with-other-people-using-cloud-computing","title":"Collaborating with Other People Using Cloud Computing","text":"<ul> <li>Instructors: Culler, CyVerse Team, Greaves, Tuff, Verleye</li> <li>Welcome!</li> <li>Navigating KIStorm (Greaves) \u26a1</li> <li>Navigating the Constellation of Cyberinfrastructure (CyVerse Team, Tuff, Verleye)<ul> <li>CyVerse Discovery Environment \u2728</li> <li>JetStream2 \ud83d\ude80</li> </ul> </li> <li>GitHub for Collaboration (Culler, Tuff, Verleye) \ud83e\udd1d<ul> <li>GitHub Pages Instructions</li> </ul> </li> </ul>"},{"location":"#day-2-thursday-may-18","title":"Day 2 (Thursday May, 18)","text":""},{"location":"#r-and-python-bilingualism-how-to-talk-to-everybody","title":"R and Python Bilingualism: How to Talk to Everybody","text":"<ul> <li>Instructors: Culler, Tuff</li> </ul>"},{"location":"#day-3-friday-may-19","title":"Day 3 (Friday May, 19)","text":""},{"location":"#tools-for-keeping-your-science-open-and-reproducible","title":"Tools for Keeping Your Science Open and Reproducible","text":"<p>(borrowed from the CyVerse Foundational Open Science Skills 2023 open online course)</p> <ul> <li>Instructors: CyVerse Team<ul> <li>Introduction to Open Science</li> <li>Documentation &amp; Communication</li> <li>Repeatability and Reproducibility</li> </ul> </li> </ul>"},{"location":"#links","title":"Links","text":"<p>Hack pad: https://hackmd.io/@cyverse-foss/SJV9X6WHh/edit</p> <p>GitHub repository: https://github.com/CU-ESIIL/pre-innovation-summit-training</p> <p>KIStorm: https://kistorm.com/login</p> <p>Slack: esiilinnovationsummit.slack.com</p> <p>CyVerse User Portal: https://user.cyverse.org</p> <p>Open Earth Data Science Textbook: https://www.earthdatascience.org/</p>"},{"location":"#code-of-conduct-amended-from-the-carpentries","title":"Code of Conduct (amended from The Carpentries)","text":"<ul> <li>Use welcoming and use inclusive language</li> <li>Be respectful of different viewpoints and experiences</li> <li>Gracefully accept constructive criticism</li> <li>Focus on what is best for the community</li> <li>Show courtesy and respect towards other community members</li> </ul>"},{"location":"#our-team","title":"Our team","text":"<p>Michele Cosi</p> <ul> <li>Data Scientist, CyVerse</li> <li>Website</li> </ul> <p></p> <p>Elsa Culler</p> <ul> <li>Education Trainer - Earth Lab; ESIIL</li> <li>Website</li> </ul> <p></p> <p>Malachi Greaves</p> <ul> <li>Know Innovation</li> <li>Website</li> </ul> <p></p> <p>Ming Posa</p> <ul> <li>Executive Assistant - ESIIL</li> <li>Website</li> </ul> <p></p> <p>Nate Quarderer </p> <ul> <li>Education Director - Earth Lab; ESIIL</li> <li>Website</li> </ul> <p></p> <p>Jim Sanovia</p> <ul> <li>Tribal Resilience Data Scientist - ESIIL</li> <li>Website</li> </ul> <p></p> <p>Tyson Swetnam</p> <ul> <li>CoPI, CyVerse, ESIIL</li> <li>Website</li> </ul> <p></p> <p>Ty Tuff</p> <ul> <li>Data Scientist - Earth Lab; ESIIL</li> <li>Website</li> </ul> <p></p> <p>Erick Verleye</p> <ul> <li>Software Developer - Earth Lab; ESIIL</li> <li>Website</li> </ul>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This work is funded by the National Science Foundation (NSF Award Number DBI-2153040).</p>"},{"location":"1_Collaboration_on_Cloud/collaboration/","title":"Navigating KIStorm","text":""},{"location":"1_Collaboration_on_Cloud/collaboration/#instructors-knowinnovation-team","title":"Instructors: Knowinnovation Team","text":""},{"location":"1_Collaboration_on_Cloud/collaboration/#time-15-min","title":"Time: 15 min","text":"<p>About Knowinnovation: At Knowinnovation, we believe interdisciplinary teams of scientists who engage in deliberate creativity have the power to solve the world\u2019s most complex challenges. We have been guiding diverse teams around the globe and witnessing the birth of these transformative solutions for over ... well since 2004. The team at KI is dedicated to accelerating scientific innovation. As students of the science of creativity, we specialize in designing and facilitating in-person and virtual events that create cross-disciplinary collaborations and build scientific communities.</p>"},{"location":"1_Collaboration_on_Cloud/collaboration/#the-cyverse-discovery-environment-data-management","title":"The CyVerse Discovery Environment &amp; Data Management","text":""},{"location":"1_Collaboration_on_Cloud/collaboration/#instructors-cyverse-team","title":"Instructors: CyVerse team","text":""},{"location":"1_Collaboration_on_Cloud/collaboration/#time-45-min","title":"Time: 45 min","text":"<p>About CyVerse: CyVerse provides scientists with powerful platforms to handle huge datasets and complex analyses, thus enabling data-driven discovery. Our extensible platforms provide data storage, bioinformatics tools, data visualization, interactive analyses, cloud services, APIs, and more. CyVerse was created in 2008 by the National Science Foundation and now supports over 100,000 researchers in 169 countries. CyVerse has appeared in more than 1,500 peer reviewed publications, trained over 40K researchers and instructors, and supports $255M in additional research funding by NSF. Led by the University of Arizona in partnership with the Texas Advanced Computing Center and Cold Spring Harbor Laboratory, CyVerse is a dynamic virtual organization that fulfills a broad mission to enable data-driven, collaborative research.</p>"},{"location":"1_Collaboration_on_Cloud/collaboration/#opening-an-instance-in-jetstream2","title":"Opening an Instance in JetStream2","text":""},{"location":"1_Collaboration_on_Cloud/collaboration/#instructors-verleye","title":"Instructors: Verleye","text":""},{"location":"1_Collaboration_on_Cloud/collaboration/#time-30-min","title":"Time: 30 min","text":"<p>About JetStream2: Jetstream2 makes cutting-edge high-performance computing and software easy to use for your research regardless of your project\u2019s scale\u2014even if you have limited experience with supercomputing systems. Cloud-based and on-demand, the 24/7 system includes discipline-specific apps. You can even create virtual machines that look and feel like your lab workstation or home machine, with thousands of times the computing power.</p>"},{"location":"1_Collaboration_on_Cloud/collaboration/#github-essentials","title":"GitHub Essentials","text":""},{"location":"1_Collaboration_on_Cloud/collaboration/#instructors-culler-tuff","title":"Instructors: Culler, Tuff","text":""},{"location":"1_Collaboration_on_Cloud/collaboration/#time-30-min_1","title":"Time: 30 min","text":"<p>About GitHub: GitHub is a web-based platform that provides version control and collaboration features using Git, a distributed version control system. It enables developers to work together on projects, track changes to code, and efficiently manage different versions of the project. GitHub is widely used in the software development industry and is an essential tool for collaborative projects and maintaining code quality.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/","title":"Github essentials","text":""},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#i-introduction-2-minutes","title":"I. Introduction (2 minutes)","text":""},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#a-brief-overview-of-github","title":"A. Brief overview of GitHub:","text":"<p>GitHub is a web-based platform that provides version control and collaboration features using Git, a distributed version control system. It enables developers to work together on projects, track changes to code, and efficiently manage different versions of the project. GitHub is widely used in the software development industry and is an essential tool for collaborative projects and maintaining code quality.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#b-introduce-github-desktop-and-jupyterhub-github-widget","title":"B. Introduce GitHub Desktop and JupyterHub GitHub widget:","text":"<p>GitHub Desktop is a graphical user interface (GUI) application that simplifies working with Git and GitHub by providing a more visual and intuitive way to manage repositories, branches, commits, and other Git features. JupyterHub GitHub widget, on the other hand, is a built-in widget that integrates Git and GitHub functionality directly into Jupyter notebooks, allowing users to perform version control and collaboration tasks within the Jupyter environment. Both tools help streamline the process of working with GitHub and make it more accessible to users with varying levels of experience with Git and version control.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#1-download-github-desktop","title":"1. Download GitHub Desktop","text":""},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#step-1-download-github-desktop","title":"Step 1: Download GitHub Desktop","text":"<p>Go to the GitHub Desktop download page: https://desktop.github.com/</p> <p>Click on the \u201cDownload for Windows\u201d or \u201cDownload for macOS\u201d button, depending on your operating system. The download should start automatically.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#step-2-install-github-desktop","title":"Step 2: Install GitHub Desktop","text":"<p>For Windows:</p> <p>Locate the downloaded installer file (usually in the Downloads folder) and double-click on it to run the installer.</p> <p>Follow the installation instructions that appear on the screen, accepting the default settings or customizing them as desired.</p> <p>Once the installation is complete, GitHub Desktop will launch automatically. For macOS:</p> <p>Locate the downloaded .zip file (usually in the Downloads folder) and double-click on it to extract the GitHub Desktop application.</p> <p>Drag the extracted \u201cGitHub Desktop\u201d application into the \u201cApplications\u201d folder.</p> <p>Open the \u201cApplications\u201d folder and double-click on \u201cGitHub Desktop\u201d to launch the application.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#step-3-set-up-github-desktop","title":"Step 3: Set up GitHub Desktop","text":"<p>When GitHub Desktop launches for the first time, you will be prompted to sign in with your GitHub account. If you don\u2019t have one, you can create one at https://github.com/join.</p> <p>Enter your GitHub username (or email) and password, and click on \u201cSign in.\u201d</p> <p>You will then be prompted to configure Git. Enter your name and email address, which will be used for your commit messages. Click \u201cContinue\u201d when you\u2019re done. Choose whether you want to submit usage data to help improve GitHub Desktop. Click \u201cFinish\u201d to complete the setup.</p> <p>Now, you have successfully installed and set up GitHub Desktop. You can start using it to clone repositories, make changes, commit, and sync with the remote repositories on GitHub.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#1-download-github-for-jupyterhub-cloud-service","title":"1. Download GitHub for JupyterHub cloud service","text":""},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#step-1-accessing-jupyterhub-on-the-cloud","title":"Step 1: Accessing JupyterHub on the cloud","text":"<p>Visit the JupyterHub cloud service you want to use (e.g., Binder, Google Colab, or a custom JupyterHub deployment provided by your organization).</p> <p>Sign in with your credentials or authenticate using a third-party service if required.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#step-2-launch-a-new-jupyter-notebook-or-open-an-existing-one","title":"Step 2: Launch a new Jupyter Notebook or open an existing one","text":"<p>Click on the \u201cNew\u201d button (usually located in the top right corner) and select \u201cPython\u201d to create a new Jupyter Notebook or open an existing one from the file browser.</p> <p>Once the notebook is open, you will see the Jupyter Notebook interface with the familiar cells for writing and executing code.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#step-3-install-and-enable-the-jupyterlab-git-extension","title":"Step 3: Install and enable the JupyterLab Git extension","text":"<p>In your Jupyter Notebook, create a new code cell and run the following command to install the JupyterLab Git extension:</p> <p>!pip install jupyterlab-git</p> <p>Restart the Jupyter Notebook server for the changes to take effect.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#step-4-create-a-new-ssh-keypair-and-add-it-your-github-account","title":"Step 4: Create a new ssh keypair and add it your GitHub Account","text":"<p>In order to authenticate to GitHub so that you can perform write operations like pushes, you must first create a private / public keypair. To do so, you will first need to make a clone of the innovation-summit-utils repository. The URL for this repository is https://github.com/CU-ESIIL/innovation-summit-utils.git </p> <p>In the Jupyter Notebook interface, you should now see a Git icon on the left sidebar. Click on it to open the GitHub widget.</p> <p>To clone a repository, click on the \u201c+\u201d icon in the GitHub widget and enter the repository URL. This will clone the repository into your JupyterHub workspace. </p> <p>Once the repository is cloned, navigate inside it and click on the configure_github_ssh.ipynb file to open it. Once it is open, execute the first cell of notebook. It will first prompt you for your GitHub username and email. The cell should output some text, the last line of which is your ssh keypair public key. It will look something like: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIP7qv33+PZN/8uu+0rMRx2/YzQXMRHsWBKayUa6UOISJ jovyan@c0a330638f25 Go ahead and copy this last line now. </p> <p>Next, log on to your GitHub and go to your account settings. In the menu on the left side, under 'Access' you should see a \"SSH and GPG keys\" option, select this. From there, click the green \"New SSH key\" button. Give your key a title and then paste the output that you copied earlier in to the \"Key\" box. Click \"Add SSH key\" to finish adding the key. </p> <p>From now on, make sure to choose the SSH URI instead of the HTTPS URI when cloning repositories from GitHub. You will now be able to push to repositories that you have write access to from the JupyterHub.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#step-5-using-the-jupyterhub-github-widget","title":"Step 5: Using the JupyterHub GitHub widget","text":"<p>You can now navigate through cloned repositories, make changes, and use the GitHub widget to stage, commit, and push your changes back to the remote repository.</p> <p>To create and manage branches, use the branch icon in the GitHub widget. You can create new branches, switch between branches, and merge branches using this interface.</p> <p>To sync your local repository with the remote repository, use the \u201cPull\u201d and \u201cPush\u201d buttons in the GitHub widget.</p> <p>Now, you know how to access and use the JupyterHub GitHub widget running on the cloud. This allows you to work with Git and GitHub directly from your Jupyter Notebook interface, streamlining your workflow and making collaboration easier.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#c-github-in-rstudio","title":"C. GitHub in Rstudio:","text":"<p>Integrating GitHub with RStudio allows users to manage their Git repositories and collaborate on projects directly within the RStudio environment. It offers similar functionality to GitHub Desktop but caters specifically to R users working within RStudio. By configuring RStudio to work with Git, creating or opening RStudio projects, and linking projects to GitHub repositories, users can enjoy a seamless workflow for version control and collaboration. RStudio\u2019s Git pane enables users to stage, commit, and push changes to remote repositories, as well as manage branches and sync local repositories with remote ones, providing a comprehensive solution for R developers working with GitHub.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#step-1-install-git","title":"Step 1: Install Git","text":"<p>Before integrating GitHub with RStudio, you need to have Git installed on your computer. Visit the official Git website (https://git-scm.com/) to download and install the latest version of Git for your operating system.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#step-2-configure-rstudio-to-work-with-git","title":"Step 2: Configure RStudio to work with Git","text":"<p>Open RStudio.</p> <p>Go to \u201cTools\u201d &gt; \u201cGlobal Options\u201d in the top menu. In the \u201cGlobal Options\u201d window, click on the \u201cGit/SVN\u201d tab.</p> <p>Check that the \u201cGit executable\u201d field is pointing to the correct location of the installed Git. If not, click \u201cBrowse\u201d and navigate to the location of the Git executable file (usually found in the \u201cbin\u201d folder of the Git installation directory).</p> <p>Click \u201cOK\u201d to save the changes.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#step-3-create-or-open-an-rstudio-project","title":"Step 3: Create or open an RStudio project","text":"<p>To create a new RStudio project, go to \u201cFile\u201d &gt; \u201cNew Project\u201d in the top menu. You can either create a new directory or choose an existing one for your project.</p> <p>To open an existing RStudio project, go to \u201cFile\u201d &gt; \u201cOpen Project\u201d and navigate to the project\u2019s \u201c.Rproj\u201d file.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#step-4-link-your-rstudio-project-to-a-github-repository","title":"Step 4: Link your RStudio project to a GitHub repository","text":"<p>In the RStudio project, go to the \u201cTools\u201d menu and select \u201cVersion Control\u201d &gt; \u201cProject Setup.\u201d</p> <p>In the \u201cProject Setup\u201d window, select \u201cGit\u201d as the version control system and click \u201cOK.\u201d</p> <p>A new \u201c.git\u201d folder will be created in your project directory, initializing it as a Git repository. Commit any changes you have made so far by clicking on the \u201cCommit\u201d button in the \u201cGit\u201d pane in RStudio.</p> <p>To link your local repository to a remote GitHub repository, go to your GitHub account and create a new repository.</p> <p>Copy the remote repository\u2019s URL (e.g., \u201chttps://github.com/username/repository.git\u201d).</p> <p>In RStudio, open the \u201cShell\u201d by going to \u201cTools\u201d &gt; \u201cShell.\u201d</p> <p>In the shell, run the following command to add the remote repository:</p> <p>git remote add origin https://github.com/username/repository.git</p> <p>Replace the URL with the one you copied from your GitHub repository.</p> <p>Push your changes to the remote repository by running the following command in the shell:</p> <p>git push -u origin master</p> <p>Now, your RStudio project is linked to a GitHub repository. You can use the \u201cGit\u201d pane in RStudio to stage, commit, and push changes to the remote repository, as well as manage branches and sync your local repository with the remote one.</p> <p>By integrating GitHub with RStudio, you can streamline your workflow, collaborate more effectively with your team, and manage your Git repositories directly from the RStudio interface.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#ii-github-basics-4-minutes","title":"II. GitHub Basics (4 minutes)","text":""},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#a-repository","title":"A. Repository:","text":"<p>A repository, often abbreviated as \u201crepo,\u201d is the fundamental building block of GitHub. It is a storage space for your project files, including the code, documentation, and other related resources. Each repository also contains the complete history of all changes made to the project files, which is crucial for effective version control. Repositories can be public, allowing anyone to access and contribute, or private, restricting access to specific collaborators.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#b-fork-and-clone","title":"B. Fork and Clone:","text":"<p>Forking and cloning are two essential operations for working with repositories on GitHub. Forking creates a personal copy of someone else\u2019s repository under your GitHub account, enabling you to make changes to the project without affecting the original repo. Cloning, on the other hand, is the process of downloading a remote repository to your local machine for offline development. In GitHub Desktop, you can clone a repository by selecting \u201cClone a repository from the Internet\u201d and entering the repository URL. In JupyterHub GitHub widget, you can clone a repository by entering the repo URL in the \u201cClone Repository\u201d section of the widget.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#c-branches","title":"C. Branches:","text":"<p>Branches are a critical aspect of Git version control, as they allow you to create multiple parallel versions of your project within a single repository. This is particularly useful when working on new features or bug fixes, as it prevents changes from interfering with the main (or \u201cmaster\u201d) branch until they are ready to be merged. Creating a new branch in GitHub Desktop can be done by clicking the \u201cCurrent Branch\u201d dropdown and selecting \u201cNew Branch.\u201d In JupyterHub GitHub widget, you can create a new branch by clicking the \u201cNew Branch\u201d button in the \u201cBranches\u201d section of the widget.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#d-replace-master-with-main","title":"D. Replace \u2018master\u2019 with \u2018main\u2019:","text":"<p>In recent years, there has been a growing awareness of the importance of inclusive language in technology. One such example is the use of the term \u201cmaster\u201d in the context of the default branch in a GitHub repository. The term \u201cmaster\u201d has historical connections to the \u201cmaster/slave\u201d file structure, which evokes an unsavory colonial past associated with slavery. In light of this, many developers and organizations have begun to replace the term \u201cmaster\u201d with more neutral terms, such as \u201cmain.\u201d We encourage you to follow this practice and change the default branch name in your repositories from \u201cmaster\u201d to \u201cmain\u201d or another suitable alternative. This small change can help promote a more inclusive and welcoming environment within the technology community.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#iii-collaboration-and-version-control-5-minutes","title":"III. Collaboration and Version Control (5 minutes)","text":""},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#a-commits","title":"A. Commits:","text":"<p>Commits are snapshots of your project\u2019s changes at a specific point in time, serving as the fundamental building blocks of Git\u2019s version control system. Commits make it possible to track changes, revert to previous versions, and collaborate with others. In GitHub Desktop, you can make a commit by staging the changes you want to include, adding a descriptive commit message, and clicking \u201cCommit to [branch_name].\u201d In JupyterHub GitHub widget, you can create a commit by selecting the files with changes, entering a commit message, and clicking the \u201cCommit\u201d button.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#b-push","title":"B. Push:","text":"<p>In GitHub, \u201cpush\u201d is a fundamental operation in the version control process that transfers commits from your local repository to a remote repository, such as the one hosted on GitHub. When you push changes, you synchronize the remote repository with the latest updates made to your local repository, making those changes accessible to other collaborators working on the same project. This operation ensures that the remote repository reflects the most recent state of your work and allows your team members to stay up to date with your changes. Pushing is an essential step in distributed version control systems like Git, as it promotes efficient collaboration among multiple contributors and provides a centralized location for tracking the project\u2019s history and progress.</p> <p>In GitHub, the concepts of \u201ccommit\u201d and \u201cpush\u201d represent two distinct steps in the version control process. A \u201ccommit\u201d is the action of saving changes to your local repository. When you commit changes, you create a snapshot of your work, accompanied by a unique identifier and an optional descriptive message. Commits allow you to track the progress of your work over time and make it easy to revert to a previous state if necessary. On the other hand, \u201cpush\u201d is the action of transferring your local commits to a remote repository, such as the one hosted on GitHub. Pushing makes your changes accessible to others collaborating on the same project and ensures that the remote repository stays up to date with your local repository. In summary, committing saves changes locally, while pushing synchronizes those changes with a remote repository, allowing for seamless collaboration among multiple contributors.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#c-pull-requests","title":"C. Pull Requests:","text":"<p>Pull requests are a collaboration feature on GitHub that enables developers to propose changes to a repository, discuss those changes, and ultimately merge them into the main branch. To create a pull request, you must first push your changes to a branch on your fork of the repository. Then, using either GitHub Desktop or JupyterHub GitHub widget, you can navigate to the original repository, click the \u201cPull Request\u201d tab, and create a new pull request. After the pull request is reviewed and approved, it can be merged into the main branch.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#d-merging-and-resolving-conflicts","title":"D. Merging and Resolving Conflicts:","text":"<p>Merging is the process of combining changes from one branch into another. This is typically done when a feature or bugfix has been completed and is ready to be integrated into the main branch. Conflicts can arise during the merging process if the same lines of code have been modified in both branches. To resolve conflicts, you must manually review the changes and decide which version to keep. In GitHub Desktop, you can merge branches by selecting the target branch and choosing \u201cMerge into Current Branch.\u201d Conflicts will be highlighted, and you can edit the files to resolve them before committing the changes. In JupyterHub GitHub widget, you can merge branches by selecting the target branch in the \u201cBranches\u201d section and clicking the \u201cMerge\u201d button. If conflicts occur, the widget will prompt you to resolve them before completing the merge.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#iv-additional-features-2-minutes","title":"IV. Additional Features (2 minutes)","text":""},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#a-issues-and-project-management","title":"A. Issues and Project Management:","text":"<p>Issues are a powerful feature in GitHub that allows developers to track and manage bugs, enhancements, and other tasks within a project. Issues can be assigned to collaborators, labeled for easy organization, and linked to specific commits or pull requests. They provide a centralized location for discussing and addressing project-related concerns, fostering collaboration and transparent communication among team members. Using issues effectively can significantly improve the overall management and organization of your projects.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#b-github-pages","title":"B. GitHub Pages:","text":"<p>GitHub Pages is a service offered by GitHub that allows you to host static websites directly from a repository. By creating a new branch named \u201cgh-pages\u201d in your repository and adding the necessary files (HTML, CSS, JavaScript, etc.), GitHub will automatically build and deploy your website to a publicly accessible URL. This is particularly useful for showcasing project documentation, creating personal portfolios, or hosting project demos. With GitHub Pages, you can take advantage of the version control and collaboration features of GitHub while easily sharing your work with others.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#v-conclusion-2-minutes","title":"V. Conclusion (2 minutes)","text":""},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#a-recap-of-the-essentials-of-github","title":"A. Recap of the essentials of GitHub:","text":"<p>In this brief introduction, we have covered the essentials of GitHub, including the basics of repositories, forking, cloning, branching, commits, pull requests, merging, and resolving conflicts. We have also discussed additional features like issues for project management and GitHub Pages for hosting websites directly from a repository.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#b-encourage-further-exploration-and-learning","title":"B. Encourage further exploration and learning:","text":"<p>While this introduction provides a solid foundation for understanding and using GitHub, there is still much more to learn and explore. As you continue to use GitHub in your projects, you will discover new features and workflows that can enhance your productivity and collaboration. We encourage you to dive deeper into the platform and experiment with different tools and techniques.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#c-share-resources-for-learning-more-about-github","title":"C. Share resources for learning more about GitHub:","text":"<p>There are many resources available for learning more about GitHub and expanding your skills. Some popular resources include GitHub Guides (https://guides.github.com/), which offers a collection of tutorials and best practices, the official GitHub documentation (https://docs.github.com/), and various online tutorials and courses. By engaging with these resources and participating in the GitHub community, you can further develop your understanding of the platform and become a more proficient user.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#v-conclusion-2-minutes_1","title":"V. Conclusion (2 minutes)","text":""},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#a-recap-of-the-essentials-of-github_1","title":"A. Recap of the essentials of GitHub:","text":"<p>In this brief introduction, we have covered the essentials of GitHub, including the basics of repositories, forking, cloning, branching, commits, pull requests, merging, and resolving conflicts. We have also discussed additional features like issues for project management and GitHub Pages for hosting websites directly from a repository.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#b-encourage-further-exploration-and-learning_1","title":"B. Encourage further exploration and learning:","text":"<p>While this introduction provides a solid foundation for understanding and using GitHub, there is still much more to learn and explore. As you continue to use GitHub in your projects, you will discover new features and workflows that can enhance your productivity and collaboration. We encourage you to dive deeper into the platform and experiment with different tools and techniques.</p>"},{"location":"1_Collaboration_on_Cloud/github_lesson/github_lesson/#c-share-resources-for-learning-more-about-github_1","title":"C. Share resources for learning more about GitHub:","text":"<p>There are many resources available for learning more about GitHub and expanding your skills. Some popular resources include GitHub Guides (https://guides.github.com/), which offers a collection of tutorials and best practices, the official GitHub documentation (https://docs.github.com/), and various online tutorials and courses. By engaging with these resources and participating in the GitHub community, you can further develop your understanding of the platform and become a more proficient user.</p>"},{"location":"1_Collaboration_on_Cloud/jetstream2_lesson/jetstream2_lesson/","title":"Jetstream2","text":""},{"location":"1_Collaboration_on_Cloud/jetstream2_lesson/jetstream2_lesson/#i-introduction-2-minutes","title":"I. Introduction (2 minutes)","text":""},{"location":"1_Collaboration_on_Cloud/jetstream2_lesson/jetstream2_lesson/#a-brief-overview-of-jetstream2","title":"A. Brief Overview of Jetstream2:","text":"<p>Jetstream2 is a cloud-based high performace computing service funded by the NSF. It is designed to simplify data analysis, boost discovery, and increase availability of AI resources. The primary system for Jetstream2 is hosted at Indiana University, with four regional centers at Arizona State University, Cornell University, University of Hawaii, and the Texas Advanced Computing Center. </p> <p>Allocations for accessing Jetstream2 resources are available through the Advanced Cyberinfrastructure Coordination Ecosystem: Services and Suppport (ACCESS).</p>"},{"location":"1_Collaboration_on_Cloud/jetstream2_lesson/jetstream2_lesson/#ii-how-we-use-jetstream2-for-collaboration","title":"II. How We Use Jetstream2 for Collaboration:","text":""},{"location":"1_Collaboration_on_Cloud/jetstream2_lesson/jetstream2_lesson/#a-jupyterhub","title":"A. JupyterHub","text":"<p>JupyterHub is a service which provides the infrastructure for multi-user interactive computing. Each user on the JupyterHub server has their own Jupyter Notebook interface which allows them to run Python and R code interactively. Users can also download their own datasets and manage file versioning with git  </p>"},{"location":"1_Collaboration_on_Cloud/jetstream2_lesson/jetstream2_lesson/#b-cacao","title":"B. CACAO","text":"<p>JupyterHub is deployed on Jetstream2 resources using CyVerse's Cloud Automation &amp; Continuous Analysis Orchestration (CACAO) service. CACAO allows us to create a cluster of virtual machines which host a JupyterHub server. A basic diagram of the deployment architecture can be seen below: </p> <p></p> <p>A cluster consists of main node and at least one worker node. The main node is responsible for orchestrating the deployment on the cluster. When a user logs in, for example, the main node assigns them to a worker node based on the minimum computing resources required for each user.  </p>"},{"location":"1_Collaboration_on_Cloud/jetstream2_lesson/jetstream2_lesson/#iii-using-jupyterhub","title":"III. Using JupyterHub","text":""},{"location":"1_Collaboration_on_Cloud/jetstream2_lesson/jetstream2_lesson/#a-logging-in","title":"A. Logging In","text":"<p>To log in to the JupyterHub, open your web browser and go to innovation-summit-training.esiil.org. Once there, you should see a login page asking for your username and password. Your username can be found on your KI Storm page, as mentioned in the KI Storm session. The password is \"password\".</p> <p></p>"},{"location":"1_Collaboration_on_Cloud/jetstream2_lesson/jetstream2_lesson/#b-dashboard","title":"B. Dashboard","text":"<p>Once logged in you will be taken to your dashboard. In the center of the dashboard you will see all of the available applications. Here is where you create new R and Python notebooks, RStudio sessions, and terminals.</p> <p>To the left is the file browser and toolbar. Here you can see, create, delete, and move files in your work space. The toolbar allows you to manage kernels, install new plugins, and access the git UI. Using the git UI will be covered in depth in the next GitHub learning session.</p> <p></p>"},{"location":"1_Collaboration_on_Cloud/jetstream2_lesson/jetstream2_lesson/#c-hello-world","title":"C. Hello World","text":"<p>Let's go through an example of running code in both an R and a Python notebook. To begin, click on the earth-analytics-python application:</p> <p></p> <p>This should open up a new tab for the notebook called Untitled.ipynb. You will also see this new file in the file browser. Let's change the name of the file to \"hello_world_python.ipynb\" by right clicking on the file in the browser and then selecting \"Rename\".</p> <p>Once the file is renamed, let's write one simple line of code into the first and only cell of the notebook: print(\"Hello world\") To execute code in a cell, you can either press the play button in the top toolbar, or press shift+enter on your keyboard. If all goes well, you should see the text \"Hello world\" output underneath the cell.</p> <p>Let's repeat these steps for an R notebook. Press on the \"+\" button on your tabs view to create a new tab. You should see the application menu again. This time choose the \"R\" application. Another \"Untitled.ipynb\" file should now appear. Change the name of it if you would like. The R code is the same as the Python code for hello world. Type it into the first cell and execute it, you should see the same output.</p>"},{"location":"1_Collaboration_on_Cloud/jetstream2_lesson/jetstream2_lesson/#other-applications","title":"Other applications","text":"<p>For those familiar, feel free to use the terminal, rstudio, and shiny applications as you see fit. </p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/","title":"R and Python bilingualism","text":"<p>Welcome to the R and Python bilingualism reference guide! If you\u2019re fluent in one of these languages but hesitant to learn the other, you\u2019re in the right place. The good news is that there are many similarities between R and Python that make it easy to switch between the two.</p> <p>Both R and Python are widely used in data science and are open-source, meaning that they are free to use and constantly being improved by the community. They both have extensive libraries for data analysis, visualization, and machine learning. In fact, many of the libraries in both languages have similar names and functions, such as Pandas in Python and data.table in R.</p> <p>While there are differences between the two languages, they can complement each other well. Python is versatile and scalable, making it ideal for large and complex projects such as web development and artificial intelligence. R, on the other hand, is known for its exceptional statistical capabilities and is often used in data analysis and modeling. Visualization is also easier in R, making it a popular choice for creating graphs and charts.</p> <p>By learning both R and Python, you\u2019ll be able to take advantage of the strengths of each language and create more efficient and robust data analysis workflows. Don\u2019t let the differences between the two languages intimidate you - once you become familiar with one, learning the other will be much easier.</p> <p>So, whether you\u2019re a Python enthusiast looking to expand your statistical analysis capabilities, or an R user interested in exploring the world of web development and artificial intelligence, this guide will help you become bilingual in R and Python.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#install-packages","title":"Install packages","text":"<p>In R, packages can be installed from CRAN repository by using the install.packages() function:</p> <p>R code:</p> <pre><code># Install the dplyr package from CRAN\ninstall.packages(\"dplyr\")\n</code></pre> <p>In Python, packages can be installed from the Anaconda repository by using the conda install command:</p> <p>Python code:</p> <pre><code># Install the pandas package from Anaconda\n!conda install pandas\n</code></pre> <p>Loading libraries in R and Python</p> <p>In R, libraries can be loaded in the same way as before, using the library() function:</p> <p>R code:</p> <pre><code># Load the dplyr library\nlibrary(dplyr)\n</code></pre> <p>In Python, libraries can be loaded in the same way as before, using the import statement. Here\u2019s an example:</p> <p>Python code:</p> <pre><code># Load the pandas library\nimport pandas as pd\n</code></pre> <p>Note that the package or library must be installed from the respective repository before it can be loaded. Also, make sure you have the correct repository specified in your system before installing packages. By default, R uses CRAN as its primary repository, whereas Anaconda uses its own repository by default.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#reticulate","title":"reticulate","text":"<p>The reticulate package lets you run both R and Python together in the R environment.</p> <p>R libraries are stored and managed in a repository called CRAN. You can download R packages with the install.packages() function</p> <pre><code>install.packages(\"reticulate\")\n</code></pre> <p>You only need to install packages once, but you need to mount those packages with the library() function each time you open R.</p> <pre><code>library(reticulate)\n</code></pre> <p>Python libraries are stored and managed in a few different libraries and their dependencies are not regulated as strictly as R libraries are in CRAN. It\u2019s easier to publish a python package but it can also be more cumbersome for users because you need to manage dependencies yourself. You can download python packages using both R and Python code</p> <pre><code>py_install(\"laspy\")\n</code></pre> <pre><code>## + '/Users/ty/opt/miniconda3/bin/conda' 'install' '--yes' '--prefix' '/Users/ty/opt/miniconda3/envs/earth-analytics-python' '-c' 'conda-forge' 'laspy'\n</code></pre> <p>Now, let\u2019s create a Python list and assign it to a variable py_list:</p> <p>R code:</p> <pre><code>py_list &lt;- r_to_py(list(1, 2, 3))\n</code></pre> <p>We can now print out the py_list variable in Python using the py_run_string() function:</p> <p>R code:</p> <pre><code>py_run_string(\"print(r.py_list)\")\n</code></pre> <p>This will output [1, 2, 3] in the Python console.</p> <p>Now, let\u2019s create an R vector and assign it to a variable r_vec:</p> <p>R code:</p> <pre><code>r_vec &lt;- c(4, 5, 6)\n</code></pre> <p>We can now print out the r_vec variable in R using the py$ syntax to access Python variables:</p> <p>R code:</p> <pre><code>print(py$py_list)\n</code></pre> <p>This will output [1, 2, 3] in the R console.</p> <p>We can also call Python functions from R using the py_call() function. For example, let\u2019s call the Python sum() function on the py_list variable and assign the result to an R variable r_sum:</p> <p>R code:</p> <pre><code>r_sum &lt;- py_call(\"sum\", args = list(py_list))\n</code></pre> <p>We can now print out the r_sum variable in R:</p> <p>R code:</p> <pre><code>print(r_sum)\n</code></pre> <p>This will output 6 in the R console.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#load-packages-and-change-settings","title":"Load packages and change settings","text":"<pre><code>options(java.parameters = \"-Xmx5G\")\n\nlibrary(r5r)\nlibrary(sf)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(interp)\nlibrary(dplyr)\nlibrary(osmdata)\nlibrary(ggthemes)\nlibrary(sf)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(akima)\nlibrary(dplyr)\nlibrary(raster)\nlibrary(osmdata)\nlibrary(mapview)\nlibrary(cowplot)\nlibrary(here)\nlibrary(testthat)\n</code></pre> <pre><code>import sys\nsys.argv.append([\"--max-memory\", \"5G\"])\n\nimport pandas as pd\nimport geopandas\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotnine\nimport contextily as cx\nimport r5py\nimport seaborn as sns\n</code></pre> <p>R and Python are two popular programming languages used for data analysis, statistics, and machine learning. Although they share some similarities, there are some fundamental differences between them. Here\u2019s an example code snippet in R and Python to illustrate some of the differences:</p> <p>R Code:</p> <pre><code># Create a vector of numbers from 1 to 10\nx &lt;- 1:10\n\n# Compute the mean of the vector\nmean_x &lt;- mean(x)\n\n# Print the result\nprint(mean_x)\n</code></pre> <pre><code>## [1] 5.5\n</code></pre> <p>Python Code:</p> <pre><code># Import the numpy library for numerical operations\nimport numpy as np\n\n# Create a numpy array of numbers from 1 to 10\nx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n# Compute the mean of the array\nmean_x = np.mean(x)\n\n# Print the result\nprint(mean_x)\n</code></pre> <pre><code>## 5.5\n</code></pre> <p>In this example, we can see that there are several differences between R and Python:</p> <p>Syntax: R uses the assignment operator \\&lt;- while Python uses the equals sign = for variable assignment.</p> <p>Libraries: Python relies heavily on external libraries such as numpy, pandas, and matplotlib for data analysis, while R has built-in functions for many data analysis tasks.</p> <p>Data types: R is designed to work with vectors and matrices, while Python uses lists and arrays. In the example above, we used the numpy library to create a numerical array in Python.</p> <p>Function names: Function names in R and Python can differ significantly. In the example above, we used the mean() function in R and the np.mean() function in Python to calculate the mean of the vector/array.</p> <p>These are just a few of the many differences between R and Python. Ultimately, the choice between the two languages will depend on your specific needs and preferences.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#load-saved-data","title":"Load saved data","text":"<p>R Code:</p> <pre><code>data(\"iris\")\nhere()\nload(file=here(\"2_R_and_Py_bilingualism\", \"data\", \"iris_example_data.rdata\"))\nobjects()\n</code></pre> <p>Python code:</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#save-data","title":"Save data","text":"<p>R Code:</p> <pre><code>save(iris, file=here(\"2_R_and_Py_bilingualism\", \"data\", \"iris_example_data.rdata\"))\n\nwrite.csv(iris, file=here(\"2_R_and_Py_bilingualism\", \"data\", \"iris_example_data.csv\"))\n</code></pre> <p>Python code:</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#functions","title":"functions","text":"<p>Both R and Python are powerful languages for writing functions that can take input, perform a specific task, and return output. R Code:</p> <pre><code># Define a function that takes two arguments and returns their sum\nsum_r &lt;- function(a, b) {\n  return(a + b)\n}\n\n# Call the function with two arguments and print the result\nresult_r &lt;- sum_r(3, 5)\nprint(result_r)\n</code></pre> <pre><code>## [1] 8\n</code></pre> <p>Python code:</p> <pre><code># Define a function that takes two arguments and returns their sum\ndef sum_py(a, b):\n    return a + b\n\n# Call the function with two arguments and print the result\nresult_py = sum_py(3, 5)\nprint(result_py)\n</code></pre> <pre><code>## 8\n</code></pre> <p>In both cases, we define a function that takes two arguments and returns their sum. In R, we use the function keyword to define a function, while in Python, we use the def keyword. The function body in R is enclosed in curly braces, while in Python it is indented.</p> <p>There are a few differences in the syntax and functionality between the two approaches:</p> <p>Function arguments: In R, function arguments are separated by commas, while in Python they are enclosed in parentheses. The syntax for specifying default arguments and variable-length argument lists can also differ between the two languages. Return statement: In R, we use the return keyword to specify the return value of a function, while in Python, we simply use the return statement. Function names: Function names in R and Python can differ significantly. In the example above, we used the sum_r() function in R and the sum_py() function in Python to calculate the sum of two numbers.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#data-plots","title":"Data Plots","text":"<p>R Code:</p> <pre><code># Load the \"ggplot2\" package for plotting\nlibrary(ggplot2)\n\n# Generate some sample data\nx &lt;- seq(1, 10, 1)\ny &lt;- x + rnorm(10)\n\n# Create a scatter plot\nggplot(data.frame(x, y), aes(x = x, y = y)) +\n  geom_point()\n</code></pre> <p> Python code:</p> <pre><code># Load the \"matplotlib\" library\nimport matplotlib.pyplot as plt\n\n# Generate some sample data\nimport numpy as np\nx = np.arange(1, 11)\ny = x + np.random.normal(0, 1, 10)\n\n#clear last plot\nplt.clf()\n\n# Create a scatter plot\nplt.scatter(x, y)\nplt.show()\n</code></pre> <p></p> <p>In both cases, we generate some sample data and create a scatter plot to visualize the relationship between the variables.</p> <p>There are a few differences in the syntax and functionality between the two approaches:</p> <p>Library and package names: In R, we use the ggplot2 package for plotting, while in Python, we use the matplotlib library. Data format: In R, we use a data frame to store the input data, while in Python, we use numpy arrays. Plotting functions: In R, we use the ggplot() function to create a new plot object, and then use the geom_point() function to create a scatter plot layer. In Python, we use the scatter() function from the matplotlib.pyplot module to create a scatter plot directly.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#linear-regression","title":"Linear regression","text":"<p>R Code:</p> <pre><code># Load the \"ggplot2\" package for plotting\nlibrary(ggplot2)\n\n# Generate some sample data\nx &lt;- seq(1, 10, 1)\ny &lt;- x + rnorm(10)\n\n# Perform linear regression\nmodel_r &lt;- lm(y ~ x)\n\n# Print the model summary\nsummary(model_r)\n</code></pre> <pre><code>## \n## Call:\n## lm(formula = y ~ x)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.69344 -0.42336  0.08961  0.34778  1.56728 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  -0.1676     0.6781  -0.247    0.811    \n## x             0.9750     0.1093   8.921 1.98e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9926 on 8 degrees of freedom\n## Multiple R-squared:  0.9087, Adjusted R-squared:  0.8972 \n## F-statistic: 79.59 on 1 and 8 DF,  p-value: 1.976e-05\n</code></pre> <pre><code># Plot the data and regression line\nggplot(data.frame(x, y), aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n</code></pre> <pre><code>## `geom_smooth()` using formula = 'y ~ x'\n</code></pre> <p></p> <p>Python code:</p> <pre><code># Load the \"matplotlib\" and \"scikit-learn\" libraries\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate some sample data\nimport numpy as np\nx = np.arange(1, 11)\ny = x + np.random.normal(0, 1, 10)\n\n# Perform linear regression\nmodel_py = LinearRegression().fit(x.reshape(-1, 1), y)\n\n# Print the model coefficients\nprint(\"Coefficients: \", model_py.coef_)\n</code></pre> <pre><code>## Coefficients:  [1.15539692]\n</code></pre> <pre><code>print(\"Intercept: \", model_py.intercept_)\n\n#clear last plot\n</code></pre> <pre><code>## Intercept:  -1.1291396173221218\n</code></pre> <pre><code>plt.clf()\n\n# Plot the data and regression line\nplt.scatter(x, y)\nplt.plot(x, model_py.predict(x.reshape(-1, 1)), color='red')\nplt.show()\n</code></pre> <p></p> <p>In both cases, we generate some sample data with a linear relationship between x and y, and then perform a simple linear regression to estimate the slope and intercept of the line. We then plot the data and regression line to visualize the fit.</p> <p>There are a few differences in the syntax and functionality between the two approaches:</p> <p>Library and package names: In R, we use the lm() function from the base package to perform linear regression, while in Python, we use the LinearRegression() class from the scikit-learn library. Additionally, we use the ggplot2 package in R for plotting, while we use the matplotlib library in Python. Data format: In R, we can specify the dependent and independent variables in the formula used for regression. In Python, we need to reshape the input data to a two-dimensional array before fitting the model. Model summary: In R, we can use the summary() function to print a summary of the model, including the estimated coefficients, standard errors, and p-values. In Python, we need to print the coefficients and intercept separately.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#random-forest","title":"Random Forest","text":"<p>R Code:</p> <pre><code># Load the \"randomForest\" package\nlibrary(randomForest)\n\n# Load the \"iris\" dataset\ndata(iris)\n\n# Split the data into training and testing sets\nset.seed(123)\ntrain_idx &lt;- sample(1:nrow(iris), nrow(iris) * 0.7, replace = FALSE)\ntrain_data &lt;- iris[train_idx, ]\ntest_data &lt;- iris[-train_idx, ]\n\n# Build a random forest model\nrf_model &lt;- randomForest(Species ~ ., data = train_data, ntree = 500)\n\n# Make predictions on the testing set\npredictions &lt;- predict(rf_model, test_data)\n\n# Calculate accuracy of the model\naccuracy &lt;- sum(predictions == test_data$Species) / nrow(test_data)\nprint(paste(\"Accuracy:\", accuracy))\n</code></pre> <pre><code>## [1] \"Accuracy: 0.977777777777778\"\n</code></pre> <p>Python code:</p> <pre><code># Load the \"pandas\", \"numpy\", and \"sklearn\" libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load the \"iris\" dataset\niris = load_iris()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=123)\n\n# Build a random forest model\nrf_model = RandomForestClassifier(n_estimators=500, random_state=123)\nrf_model.fit(X_train, y_train)\n\n# Make predictions on the testing set\n</code></pre> <pre><code>## RandomForestClassifier(n_estimators=500, random_state=123)\n</code></pre> <pre><code>predictions = rf_model.predict(X_test)\n\n# Calculate accuracy of the model\naccuracy = sum(predictions == y_test) / len(y_test)\nprint(\"Accuracy:\", accuracy)\n</code></pre> <pre><code>## Accuracy: 0.9555555555555556\n</code></pre> <p>In both cases, we load the iris dataset and split it into training and testing sets. We then build a random forest model using the training data and evaluate its accuracy on the testing data.</p> <p>There are a few differences in the syntax and functionality between the two approaches:</p> <p>Library and package names: In R, we use the randomForest package to build random forest models, while in Python, we use the RandomForestClassifier class from the sklearn.ensemble module. We also use different libraries for loading and manipulating data (pandas and numpy in Python, and built-in datasets in R). Model parameters: The syntax for setting model parameters is slightly different in R and Python. For example, in R, we specify the number of trees using the ntree parameter, while in Python, we use the n_estimators parameter. Data format: In R, we use a data frame to store the input data, while in Python, we use numpy arrays.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#basic-streetmap-from-open-street-map","title":"Basic streetmap from Open Street Map","text":"<p>R Code:</p> <pre><code># Load the \"osmdata\" package for mapping\nlibrary(osmdata)\nlibrary(tmap)\n\n# Define the map location and zoom level\nbbox &lt;- c(left = -0.16, bottom = 51.49, right = -0.13, top = 51.51)\n\n# Get the OpenStreetMap data\nosm_data &lt;- opq(bbox) %&gt;% \n  add_osm_feature(key = \"highway\") %&gt;% \n  osmdata_sf()\n\n# Plot the map using tmap\ntm_shape(osm_data$osm_lines) + \n  tm_lines()\n</code></pre> <p> Python code:</p> <pre><code># Load the \"osmnx\" package for mapping\nimport osmnx as ox\n\n# Define the map location and zoom level\nbbox = (51.49, -0.16, 51.51, -0.13)\n\n# Get the OpenStreetMap data\nosm_data = ox.graph_from_bbox(north=bbox[2], south=bbox[0], east=bbox[3], west=bbox[1], network_type='all')\n\n# Plot the map using osmnx\nox.plot_graph(osm_data)\n</code></pre> <pre><code>## (&lt;Figure size 1600x1600 with 0 Axes&gt;, &lt;AxesSubplot:&gt;)\n</code></pre> <p></p> <p>In both cases, we define the map location and zoom level, retrieve the OpenStreetMap data using the specified bounding box, and plot the map.</p> <p>The main differences between the two approaches are:</p> <p>Package names and syntax: In R, we use the osmdata package and its syntax to download and process the OpenStreetMap data, while in Python, we use the osmnx package and its syntax. Mapping libraries: In R, we use the tmap package to create a static map of the OpenStreetMap data, while in Python, we use the built-in ox.plot_graph function from the osmnx package to plot the map.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#cnn-on-raster-data","title":"CNN on Raster data","text":"<p>R Code:</p> <pre><code># Load the \"keras\" package for building the CNN\nlibrary(tensorflow)\nlibrary(keras)\n\n# Load the \"raster\" package for working with raster data\nlibrary(raster)\n\n# Load the \"magrittr\" package for pipe operator\nlibrary(magrittr)\n\n# Load the data as a raster brick\nraster_data &lt;- brick(\"raster_data.tif\")\n\n# Split the data into training and testing sets\nsplit_data &lt;- sample(1:nlayers(raster_data), size = nlayers(raster_data)*0.8, replace = FALSE)\ntrain_data &lt;- raster_data[[split_data]]\ntest_data &lt;- raster_data[[setdiff(1:nlayers(raster_data), split_data)]]\n\n# Define the CNN model\nmodel &lt;- keras_model_sequential() %&gt;% \n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\", input_shape = c(ncol(train_data), nrow(train_data), ncell(train_data))) %&gt;% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% \n  layer_dropout(rate = 0.25) %&gt;% \n  layer_flatten() %&gt;% \n  layer_dense(units = 128, activation = \"relu\") %&gt;% \n  layer_dropout(rate = 0.5) %&gt;% \n  layer_dense(units = nlayers(train_data), activation = \"softmax\")\n\n# Compile the model\nmodel %&gt;% compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\n# Train the model\nhistory &lt;- model %&gt;% fit(x = array(train_data), y = to_categorical(1:nlayers(train_data)), epochs = 10, validation_split = 0.2)\n\n# Evaluate the model\nmodel %&gt;% evaluate(x = array(test_data), y = to_categorical(1:nlayers(test_data)))\n\n# Plot the model accuracy over time\nplot(history)\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#piping","title":"Piping","text":"<p>Piping is a powerful feature in both R and Python that allows for a more streamlined and readable code. However, the syntax for piping is slightly different between the two languages.</p> <p>In R, piping is done using the %&gt;% operator from the magrittr package, while in Python, it is done using the | operator from the pandas package.</p> <p>Let\u2019s compare and contrast piping in R and Python with some examples:</p> <p>Piping in R In R, we can use the %&gt;% operator to pipe output from one function to another, which can make our code more readable and easier to follow. Here\u2019s an example:</p> <p>R code:</p> <pre><code>library(dplyr)\n\n# create a data frame\ndf &lt;- data.frame(x = c(1,2,3), y = c(4,5,6))\n\n# calculate the sum of column x and y\ndf %&gt;%\n  mutate(z = x + y) %&gt;%\n  summarize(sum_z = sum(z))\n</code></pre> <pre><code>##   sum_z\n## 1    21\n</code></pre> <p>In this example, we first create a data frame df with two columns x and y. We then pipe the output of df to mutate, which adds a new column z to the data frame that is the sum of x and y. Finally, we pipe the output to summarize, which calculates the sum of z and returns the result.</p> <p>Piping in Python In Python, we can use the | operator to pipe output from one function to another. However, instead of piping output from one function to another, we pipe a DataFrame to a method of the DataFrame. Here\u2019s an example:</p> <p>Python code:</p> <pre><code>import pandas as pd\n\n# create a DataFrame\ndf = pd.DataFrame({'x': [1,2,3], 'y': [4,5,6]})\n\n# calculate the sum of column x and y\n(df.assign(z = df['x'] + df['y'])\n   .agg(sum_z = ('z', 'sum')))\n</code></pre> <pre><code>##         z\n## sum_z  21\n</code></pre> <p>In this example, we first create a DataFrame df with two columns x and y. We then use the assign() method to add a new column z to the DataFrame that is the sum of x and y. Finally, we use the agg() method to calculate the sum of z and return the result.</p> <p>As we can see, the syntax for piping is slightly different between R and Python, but the concept remains the same. Piping can make our code more readable and easier to follow, which is an important aspect of creating efficient and effective code.</p> <p>R code:</p> <pre><code>library(dplyr)\nlibrary(ggplot2)\n\niris %&gt;%\n  filter(Species == \"setosa\") %&gt;%\n  group_by(Sepal.Width) %&gt;%\n  summarise(mean.Petal.Length = mean(Petal.Length)) %&gt;%\n  mutate(Sepal.Width = as.factor(Sepal.Width)) %&gt;%\n  ggplot(aes(x = Sepal.Width, y = mean.Petal.Length)) +\n  geom_bar(stat = \"identity\", fill = \"dodgerblue\") +\n  labs(title = \"Mean Petal Length of Setosa by Sepal Width\",\n       x = \"Sepal Width\",\n       y = \"Mean Petal Length\")\n</code></pre> <p></p> <p>In this example, we start with the iris dataset and filter it to only include rows where the Species column is \u201csetosa\u201d. We then group the remaining rows by the Sepal.Width column and calculate the mean Petal.Length for each group. Next, we convert Sepal.Width to a factor variable to ensure that it is treated as a categorical variable in the visualization. Finally, we create a bar plot using ggplot2, with Sepal.Width on the x-axis and mean.Petal.Length on the y-axis. The resulting plot shows the mean petal length of setosa flowers for each sepal width category.</p> <p>Python code:</p> <pre><code>import pandas as pd\n\n# Load the iris dataset and pipe it into the next function\n( pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])\n\n  # Select columns and pivot the dataset\n  .loc[:, ['sepal_length', 'sepal_width', 'petal_length']]\n  .melt(var_name='variable', value_name='value')\n\n  # Group by variable and calculate mean\n  .groupby('variable', as_index=False)\n  .mean()\n\n  # Filter for mean greater than 3.5 and sort by descending mean\n  .query('value &gt; 3.5')\n  .sort_values('value', ascending=False)\n)\n</code></pre> <pre><code>##        variable     value\n## 1  sepal_length  5.843333\n## 0  petal_length  3.758667\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#for-loops","title":"for loops","text":"<p>Here is an example of a for loop in R:</p> <p>R code</p> <pre><code># Create a vector of numbers\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\n# Use a for loop to print out each number in the vector\nfor (i in numbers) {\n  print(i)\n}\n</code></pre> <pre><code>## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n</code></pre> <p>In this example, the for loop iterates over each element in the numbers vector, assigning the current element to the variable i. The print(i) statement is then executed for each iteration, outputting the value of i.</p> <p>Here is the equivalent example in Python:</p> <p>Python code</p> <pre><code># Create a list of numbers\nnumbers = [1, 2, 3, 4, 5]\n\n# Use a for loop to print out each number in the list\nfor i in numbers:\n  print(i)\n</code></pre> <pre><code>## 1\n## 2\n## 3\n## 4\n## 5\n</code></pre> <p>In Python, the for loop iterates over each element in the numbers list, assigning the current element to the variable i. The print(i) statement is then executed for each iteration, outputting the value of i.</p> <p>Both languages also support nested for loops, which can be used to perform iterations over multiple dimensions, such as looping through a 2D array.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#parallel","title":"Parallel","text":"<p>Parallel computing is a technique used to execute multiple computational tasks simultaneously, which can significantly reduce the time required to complete a task. Both R and Python have built-in support for parallel computing, although the approaches are slightly different. In this answer, we will compare and contrast the parallel computing capabilities of R and Python, and provide working examples in code.</p> <p>Parallel computing in R In R, there are several packages that support parallel computing, such as parallel, foreach, and doParallel. The parallel package provides basic functionality for parallel computing, while foreach and doParallel provide higher-level abstractions that make it easier to write parallel code.</p> <p>Here is an example of using the foreach package to execute a loop in parallel:</p> <p>R code:</p> <pre><code>library(foreach)\nlibrary(doParallel)\n\n# Set up a parallel backend with 4 workers\ncl &lt;- makeCluster(4)\nregisterDoParallel(cl)\n\n# Define a function to apply in parallel\nmyfunc &lt;- function(x) {\n  # some computation here\n  return(x^2)\n}\n\n# Generate some data\nmydata &lt;- 1:1000\n\n# Apply the function to the data in parallel\nresult &lt;- foreach(i = mydata) %dopar% {\n  myfunc(i)\n}\n\n# Stop the cluster\nstopCluster(cl)\n</code></pre> <p>In this example, we use the makeCluster() function to set up a cluster with 4 workers, and the registerDoParallel() function to register the cluster as the parallel backend for foreach. We then define a function myfunc() that takes an input x and returns x^2. We generate some data mydata and use foreach to apply myfunc() to each element of mydata in parallel, using the %dopar% operator.</p> <p>R Tidyverse parallel</p> <p>In R Tidyverse, we can use the furrr package for parallel computing. Here\u2019s an example of using furrr to parallelize a map function:</p> <p>R Tidy code:</p> <pre><code>library(tidyverse)\nlibrary(furrr)\n\n# Generate a list of numbers\nnumbers &lt;- 1:10\n\n# Use the future_map function from furrr to parallelize the map function\nplan(multisession)\nsquares &lt;- future_map(numbers, function(x) x^2)\n</code></pre> <p>In this example, we first load the Tidyverse and furrr libraries. We then generate a list of numbers from 1 to 10. We then use the plan function to set the parallelization strategy to \u201cmultisession\u201d, which will use multiple CPU cores to execute the code. Finally, we use the future_map function from furrr to apply the function x^2 to each number in the list in parallel.</p> <p>Parallel computing in Python In Python, the standard library includes the multiprocessing module, which provides basic support for parallel computing. Additionally, there are several third-party packages that provide higher-level abstractions, such as joblib and dask.</p> <p>Here is an example of using the multiprocessing module to execute a loop in parallel:</p> <p>Python code:</p> <pre><code>def square(x):\n    return x**2\n\nfrom multiprocessing import Pool\n\n# Generate a list of numbers\nnumbers = list(range(1, 11))\n\n# Use the map function and a pool of workers to parallelize the square function\nwith Pool() as pool:\n    squares = pool.map(square, numbers)\n\nprint(squares)\n</code></pre> <p>In this example, we define a function myfunc() that takes an input x and returns x^2. We generate some data mydata and use the Pool class from the multiprocessing module to set up a pool of 4 workers. We then use the map() method of the Pool class to apply myfunc() to each element of mydata in parallel.</p> <p>Comparison and contrast Both R and Python have built-in support for parallel computing, with similar basic functionality for creating and managing parallel processes. However, the higher-level abstractions differ between the two languages. In R, the foreach package provides a high-level interface that makes it easy to write parallel code, while in Python, the multiprocessing module provides a basic interface that can be extended using third-party packages like joblib and dask.</p> <p>Additionally, Python has better support for distributed computing using frameworks like Apache Spark, while R has better support for shared-memory parallelism using tools like data.table and ff.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#data-wrangling","title":"Data wrangling","text":"<p>Data wrangling is an important part of any data analysis project, and both R and Python provide tools and libraries for performing this task. In this answer, we will compare and contrast data wrangling in R\u2019s tidyverse and Python\u2019s pandas library, with working examples in code.</p> <p>Data Wrangling in R Tidyverse</p> <p>The tidyverse is a collection of R packages designed for data science, and it includes several packages that are useful for data wrangling. One of the most popular packages is dplyr, which provides a grammar of data manipulation for data frames.</p> <p>Here is an example of using dplyr to filter, mutate, and summarize a data frame:</p> <p>R code</p> <pre><code>library(dplyr)\n\n# Load data\ndata(mtcars)\n\n# Filter for cars with more than 100 horsepower\nmtcars %&gt;%\n  filter(hp &gt; 100) %&gt;%\n  # Add a new column with fuel efficiency in km per liter\n  mutate(kmpl = 0.425 * mpg) %&gt;%\n  # Group by number of cylinders and summarize\n  group_by(cyl) %&gt;%\n  summarize(mean_hp = mean(hp),\n            mean_kmpl = mean(kmpl))\n</code></pre> <pre><code>## # A tibble: 3 \u00d7 3\n##     cyl mean_hp mean_kmpl\n##   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n## 1     4    111      11.0 \n## 2     6    122.      8.39\n## 3     8    209.      6.42\n</code></pre> <p>In this example, we first filter the mtcars data frame to only include cars with more than 100 horsepower. We then use mutate to create a new column with fuel efficiency in kilometers per liter. Finally, we group the data by the number of cylinders and calculate the mean horsepower and fuel efficiency.</p> <p>Data Wrangling in Python Pandas</p> <p>Pandas is a popular library for data manipulation in Python. It provides a data frame object similar to R\u2019s data frames, along with a wide range of functions for data wrangling.</p> <p>Here is an example of using pandas to filter, transform, and group a data frame:</p> <p>Python code:</p> <pre><code>import pandas as pd\n\n# Load data\nmtcars = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mtcars.csv')\n\n# Filter for cars with more than 100 horsepower\nfiltered_mtcars = mtcars[mtcars['hp'] &gt; 100]\n\n# Add a new column with fuel efficiency in km per liter\nfiltered_mtcars['kmpl'] = 0.425 * filtered_mtcars['mpg']\n\n# Group by number of cylinders and calculate mean horsepower and fuel efficiency\ngrouped_mtcars = filtered_mtcars.groupby('cyl').agg({'hp': 'mean',\n                                                     'kmpl': 'mean'})\n</code></pre> <p>In this example, we first load the mtcars data from a CSV file. We then filter the data to only include cars with more than 100 horsepower, using boolean indexing. We use the assign function to create a new column with fuel efficiency in kilometers per liter. Finally, we group the data by the number of cylinders and calculate the mean horsepower and fuel efficiency.</p> <p>Comparison</p> <p>Overall, both R\u2019s tidyverse and Python\u2019s pandas provide similar functionality for data wrangling. Both allow for filtering, transforming, and aggregating data frames. The syntax for performing these operations is slightly different between the two languages, with R using the %&gt;% operator for chaining operations and Python using method chaining or the apply family of functions.</p> <p>One key difference between the two languages is that R\u2019s tidyverse provides a consistent grammar for data manipulation across its various packages, making it easier to learn and use. However, Python\u2019s pandas library has a larger developer community and is more versatile for use in other applications, such as web development or machine learning.</p> <p>In conclusion, both R and Python provide powerful tools for data wrangling, and the choice between the two ultimately depends on the specific needs of the user and their familiarity</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#data-from-api","title":"Data from API","text":"<p>Retrieving data from an API is a common task in both R and Python. Here are examples of how to retrieve data from an API in both languages:</p> <p>Python</p> <p>To retrieve data from an API in Python, we can use the requests library. Here\u2019s an example of how to retrieve weather data from the OpenWeatherMap API:</p> <p>Python code:</p> <pre><code>import requests\n\nurl = 'https://api.openweathermap.org/data/2.5/weather?q=London,uk&amp;appid=API_KEY'\n\nresponse = requests.get(url)\n\ndata = response.json()\n\nprint(data)\n</code></pre> <p>This code retrieves the current weather data for London from the OpenWeatherMap API. We first construct the API URL with the location and API key, then use the requests.get() function to make a request to the API. We then extract the JSON data from the response using the .json() method and print the resulting data.</p> <p>R</p> <p>In R, we can use the httr package to retrieve data from an API. Here\u2019s an example of how to retrieve weather data from the OpenWeatherMap API in R:</p> <p>R code:</p> <pre><code>library(httr)\n\nurl &lt;- 'https://api.openweathermap.org/data/2.5/weather?q=London,uk&amp;appid=API_KEY'\n\nresponse &lt;- GET(url)\n\ndata &lt;- content(response, 'text')\n\nprint(data)\n</code></pre> <p>This code is similar to the Python code above. We first load the httr library, then construct the API URL and use the GET() function to make a request to the API. We then extract the data from the response using the content() function and print the resulting data.</p> <p>Retrieving Data from an API in R Tidyverse In R Tidyverse, we can use the httr and jsonlite packages to retrieve and process data from an API.</p> <p>R code:</p> <pre><code># Load required packages\nlibrary(httr)\nlibrary(jsonlite)\n\n# Define API endpoint\nendpoint &lt;- \"https://jsonplaceholder.typicode.com/posts\"\n\n# Retrieve data from API\nresponse &lt;- GET(endpoint)\n\n# Extract content from response\ncontent &lt;- content(response, \"text\")\n\n# Convert content to JSON\njson &lt;- fromJSON(content)\n\n# Convert JSON to a data frame\ndf &lt;- as.data.frame(json)\n</code></pre> <p>In the above example, we use the GET() function from the httr package to retrieve data from an API endpoint, and the content() function to extract the content of the response. We then use the fromJSON() function from the jsonlite package to convert the JSON content to a list, and the as.data.frame() function to convert the list to a data frame.</p> <p>Retrieving Data from an API in Python In Python, we can use the requests library to retrieve data from an API, and the json library to process the JSON data.</p> <p>Python code:</p> <pre><code># Load required libraries\nimport requests\nimport json\n\n# Define API endpoint\nendpoint = \"https://jsonplaceholder.typicode.com/posts\"\n\n# Retrieve data from API\nresponse = requests.get(endpoint)\n\n# Extract content from response\ncontent = response.content\n\n# Convert content to JSON\njson_data = json.loads(content)\n\n# Convert JSON to a list of dictionaries\ndata = [dict(row) for row in json_data]\n</code></pre> <p>In the above example, we use the get() function from the requests library to retrieve data from an API endpoint, and the content attribute to extract the content of the response. We then use the loads() function from the json library to convert the JSON content to a list of dictionaries.</p> <p>Comparison Both R Tidyverse and Python provide powerful tools for retrieving and processing data from an API. In terms of syntax, the two languages are somewhat similar. In both cases, we use a library to retrieve data from the API, extract the content of the response, and then process the JSON data. However, there are some differences in the specific functions and methods used. For example, in R Tidyverse, we use the content() function to extract the content of the response, whereas in Python, we use the content attribute. Additionally, in R Tidyverse, we use the fromJSON() function to convert the JSON data to a list, whereas in Python, we use the loads() function.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#census-data","title":"Census data","text":"<p>Retrieving USA census data in R, R Tidy, and Python can be done using different packages and libraries. Here are some working examples in code for each language:</p> <p>R:</p> <p>To retrieve census data in R, we can use the tidycensus package. Here\u2019s an example of how to retrieve the total population for the state of California:</p> <p>R code:</p> <pre><code>library(tidycensus)\nlibrary(tidyverse)\n\n# Set your Census API key\ncensus_api_key(\"your_api_key\")\n\n# Get the total population for the state of California\nca_pop &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01003_001\",\n  state = \"CA\"\n) %&gt;% \n  rename(total_population = estimate) %&gt;% \n  select(total_population)\n\n# View the result\nca_pop\n</code></pre> <p>R Tidy:</p> <p>To retrieve census data in R Tidy, we can also use the tidycensus package. Here\u2019s an example of how to retrieve the total population for the state of California using pipes and dplyr functions:</p> <p>R tidy code:</p> <pre><code>library(tidycensus)\nlibrary(tidyverse)\n\n# Set your Census API key\ncensus_api_key(\"your_api_key\")\n\n# Get the total population for the state of California\nca_pop &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01003_001\",\n  state = \"CA\"\n) %&gt;% \n  rename(total_population = estimate) %&gt;% \n  select(total_population)\n\n# View the result\nca_pop\n</code></pre> <p>Python:</p> <p>To retrieve census data in Python, we can use the census library. Here\u2019s an example of how to retrieve the total population for the state of California:</p> <p>Python code:</p> <pre><code>from census import Census\nfrom us import states\nimport pandas as pd\n\n# Set your Census API key\nc = Census(\"your_api_key\")\n\n# Get the total population for the state of California\nca_pop = c.acs5.state((\"B01003_001\"), states.CA.fips, year=2019)\n\n# Convert the result to a Pandas DataFrame\nca_pop_df = pd.DataFrame(ca_pop)\n\n# Rename the column\nca_pop_df = ca_pop_df.rename(columns={\"B01003_001E\": \"total_population\"})\n\n# Select only the total population column\nca_pop_df = ca_pop_df[[\"total_population\"]]\n\n# View the result\nca_pop_df\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#lidar-data","title":"Lidar data","text":"<p>To find Lidar data in R and Python, you typically need to start by identifying sources of Lidar data and then accessing them using appropriate packages and functions. Here are some examples of how to find Lidar data in R and Python:</p> <p>R:</p> <p>Identify sources of Lidar data: The USGS National Map Viewer provides access to Lidar data for the United States. You can also find Lidar data on state and local government websites, as well as on commercial data providers\u2019 websites. Access the data: You can use the lidR package in R to download and read Lidar data in the LAS format. For example, the following code downloads and reads Lidar data for a specific area:</p> <p>R code:</p> <pre><code>library(lidR)\n\n# Download Lidar data\nLASfile &lt;- system.file(\"extdata\", \"Megaplot.laz\", package=\"lidR\")\nlidar &lt;- readLAS(LASfile)\n\n# Visualize the data\nplot(lidar)\n</code></pre> <p>Python:</p> <p>Identify sources of Lidar data: The USGS 3DEP program provides access to Lidar data for the United States. You can also find Lidar data on state and local government websites, as well as on commercial data providers\u2019 websites. Access the data: You can use the pylastools package in Python to download and read Lidar data in the LAS format. For example, the following code downloads and reads Lidar data for a specific area:</p> <p>Python code:</p> <pre><code>py_install(\"requests\")\npy_install(\"pylas\")\npy_install(\"laspy\")\n</code></pre> <pre><code>import requests\nfrom pylas import read\nimport laspy\nimport numpy as np\n\n# Download Lidar data\nurl = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/USGS_LPC_CA_SanFrancisco_2016_LAS_2018.zip\"\nlasfile = \"USGS_LPC_CA_SanFrancisco_2016_LAS_2018.las\"\nr = requests.get(url, allow_redirects=True)\nopen(lasfile, 'wb').write(r.content)\n\n# Read the data\nlidar = read(lasfile)\n\n# Visualize the data\nlaspy.plot.plot(lidar)\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#data-for-black-lives","title":"Data for black lives","text":"<p>Data for Black Lives (https://d4bl.org/) is a movement that uses data science to create measurable change in the lives of Black people. While the Data for Black Lives website provides resources, reports, articles, and datasets related to racial equity, it doesn\u2019t provide a direct API for downloading data.</p> <p>Instead, you can access the Data for Black Lives GitHub repository (https://github.com/Data4BlackLives) to find datasets and resources to work with. In this example, we\u2019ll use a sample dataset available at https://github.com/Data4BlackLives/covid-19/tree/master/data. The dataset \u201cCOVID19_race_data.csv\u201d contains COVID-19 race-related data.</p> <p>R: In R, we\u2019ll use the \u2018readr\u2019 and \u2018dplyr\u2019 packages to read, process, and analyze the dataset.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\n\nlibrary(readr)\nlibrary(dplyr)\n\n# Read the CSV file\nurl &lt;- \"https://raw.githubusercontent.com/Data4BlackLives/covid-19/master/data/COVID19_race_data.csv\"\ndata &lt;- read_csv(url)\n\n# Basic information about the dataset\nprint(dim(data))\nprint(head(data))\n\n# Example analysis: calculate the mean of 'cases_total' by 'state'\ndata %&gt;%\n  group_by(state) %&gt;%\n  summarize(mean_cases_total = mean(cases_total, na.rm = TRUE)) %&gt;%\n  arrange(desc(mean_cases_total))\n</code></pre> <p>Python: In Python, we\u2019ll use the \u2018pandas\u2019 library to read, process, and analyze the dataset.</p> <p>Python code:</p> <pre><code>import pandas as pd\n\n# Read the CSV file\nurl = \"https://raw.githubusercontent.com/Data4BlackLives/covid-19/master/data/COVID19_race_data.csv\"\ndata = pd.read_csv(url)\n\n# Basic information about the dataset\nprint(data.shape)\nprint(data.head())\n\n# Example analysis: calculate the mean of 'cases_total' by 'state'\nmean_cases_total = data.groupby(\"state\")[\"cases_total\"].mean().sort_values(ascending=False)\nprint(mean_cases_total)\n</code></pre> <p>In conclusion, both R and Python provide powerful libraries and tools for downloading, processing, and analyzing datasets, such as those found in the Data for Black Lives repository. The \u2018readr\u2019 and \u2018dplyr\u2019 libraries in R offer a simple and intuitive way to read and manipulate data, while the \u2018pandas\u2019 library in Python offers similar functionality with a different syntax. Depending on your preferred programming language and environment, both options can be effective in working with social justice datasets.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#propublica-congress-api","title":"Propublica Congress API","text":"<p>The ProPublica Congress API provides information about the U.S. Congress members and their voting records. In this example, we\u2019ll fetch data about the current Senate members and calculate the number of members in each party.</p> <p>R: In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the ProPublica Congress API.</p> <p>R code:</p> <pre><code># load necessary libraries\nlibrary(httr)\nlibrary(jsonlite)\n\n# Replace 'your_api_key' with your ProPublica API key\n\n#\n\n# Fetch data about the current Senate members\nurl &lt;- \"https://api.propublica.org/congress/v1/117/senate/members.json\"\nresponse &lt;- GET(url, add_headers(`X-API-Key` = api_key))\n\n# Check if the request was successful\nif (http_status(response)$category == \"Success\") {\n  data &lt;- content(response, \"parsed\")\n  members &lt;- data$results[[1]]$members\n\n  # Calculate the number of members in each party\n  party_counts &lt;- table(sapply(members, function(x) x$party))\n  print(party_counts)\n} else {\n  print(http_status(response)$message)\n}\n</code></pre> <pre><code>## \n##  D  I ID  R \n## 49  1  2 51\n</code></pre> <p>Python: In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the ProPublica Congress API and \u2018pandas\u2019 library to process the data.</p> <p>python code:</p> <pre><code># Install necessary libraries\n\nimport requests\nimport pandas as pd\n\n# Replace 'your_api_key' with your ProPublica API key\napi_key = \"your_api_key\"\nheaders = {\"X-API-Key\": api_key}\n\n# Fetch data about the current Senate members\nurl = \"https://api.propublica.org/congress/v1/117/senate/members.json\"\nresponse = requests.get(url, headers=headers)\n\n# Check if the request was successful\nif response.status_code == 200:\n    data = response.json()\n    members = data[\"results\"][0][\"members\"]\n\n    # Calculate the number of members in each party\n    party_counts = pd.DataFrame(members)[\"party\"].value_counts()\n    print(party_counts)\nelse:\n    print(f\"Error: {response.status_code}\")\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the ProPublica Congress API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like table() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the ProPublica Congress API.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#nonprofit-explorer-api-by-propublica","title":"Nonprofit Explorer API by ProPublica","text":"<p>The Nonprofit Explorer API by ProPublica provides data on tax-exempt organizations in the United States. In this example, we\u2019ll search for organizations with the keyword \u201ceducation\u201d and analyze the results.</p> <p>R: In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the Nonprofit Explorer API.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\nlibrary(httr)\nlibrary(jsonlite)\n\n# Fetch data for organizations with the keyword \"education\"\nurl &lt;- \"https://projects.propublica.org/nonprofits/api/v2/search.json?q=education\"\nresponse &lt;- GET(url)\n\n# Check if the request was successful\nif (http_status(response)$category == \"Success\") {\n  data &lt;- content(response, \"parsed\")\n  organizations &lt;- data$organizations\n\n  # Count the number of organizations per state\n  state_counts &lt;- table(sapply(organizations, function(x) x$state))\n  print(state_counts)\n} else {\n  print(http_status(response)$message)\n}\n</code></pre> <pre><code>## \n##      AZ      CA      CO      DC      FL      GA      HI      IL Indiana      LA \n##       3      22       6       5       3       2       1       2       1       1 \n##      MD      MI      MN      MO      MP      MS      NC      NE      NJ      NM \n##       1       2       5       3       1       1       2       2       2       1 \n##      NY      OH      OK  Oregon      PA      TX      UT      VA      WA      WV \n##       1       5       1       2       2      12       1       4       3       1 \n##      ZZ \n##       2\n</code></pre> <p>Python: In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the Nonprofit Explorer API and \u2018pandas\u2019 library to process the data.</p> <p>Python code:</p> <pre><code># Install necessary libraries\nimport requests\nimport pandas as pd\n\n# Fetch data for organizations with the keyword \"education\"\nurl = \"https://projects.propublica.org/nonprofits/api/v2/search.json?q=education\"\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    data = response.json()\n    organizations = data[\"organizations\"]\n\n    # Count the number of organizations per state\n    state_counts = pd.DataFrame(organizations)[\"state\"].value_counts()\n    print(state_counts)\nelse:\n    print(f\"Error: {response.status_code}\")\n</code></pre> <pre><code>## CA         22\n## TX         12\n## CO          6\n## MN          5\n## OH          5\n## DC          5\n## VA          4\n## AZ          3\n## WA          3\n## MO          3\n## FL          3\n## IL          2\n## GA          2\n## NC          2\n## MI          2\n## Oregon      2\n## NE          2\n## ZZ          2\n## PA          2\n## NJ          2\n## HI          1\n## MS          1\n## NY          1\n## Indiana     1\n## NM          1\n## LA          1\n## UT          1\n## MD          1\n## MP          1\n## WV          1\n## OK          1\n## Name: state, dtype: int64\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the Nonprofit Explorer API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like table() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the Nonprofit Explorer API.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#campaign-finance-api-by-propublica","title":"Campaign Finance API by ProPublica","text":"<p>The Campaign Finance API by the Federal Election Commission (FEC) provides data on campaign finance in U.S. federal elections. In this example, we\u2019ll fetch data about individual contributions for the 2020 election cycle and analyze the results.</p> <p>R: In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the Campaign Finance API.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\nlibrary(httr)\nlibrary(jsonlite)\n\n# Fetch data about individual contributions for the 2020 election cycle\nurl &lt;- \"https://api.open.fec.gov/v1/schedules/schedule_a/?api_key='OGwpkX7tH5Jihs1qQcisKfVAMddJzmzouWKtKoby'&amp;two_year_transaction_period=2020&amp;sort_hide_null=false&amp;sort_null_only=false&amp;per_page=20&amp;page=1\"\nresponse &lt;- GET(url)\n\n# Check if the request was successful\nif (http_status(response)$category == \"Success\") {\n  data &lt;- content(response, \"parsed\")\n  contributions &lt;- data$results\n\n  # Calculate the total contributions per state\n  state_totals &lt;- aggregate(contributions$contributor_state, by = list(contributions$contributor_state), FUN = sum)\n  colnames(state_totals) &lt;- c(\"State\", \"Total_Contributions\")\n  print(state_totals)\n} else {\n  print(http_status(response)$message)\n}\n</code></pre> <pre><code>## [1] \"Client error: (403) Forbidden\"\n</code></pre> <p>Python: In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the Campaign Finance API and \u2018pandas\u2019 library to process the data.</p> <p>Python code:</p> <pre><code># Install necessary libraries\n\nimport requests\nimport pandas as pd\n\n# Fetch data about individual contributions for the 2020 election cycle\nurl = \"https://api.open.fec.gov/v1/schedules/schedule_a/?api_key=your_api_key&amp;two_year_transaction_period=2020&amp;sort_hide_null=false&amp;sort_null_only=false&amp;per_page=20&amp;page=1\"\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    data = response.json()\n    contributions = data[\"results\"]\n\n    # Calculate the total contributions per state\n    df = pd.DataFrame(contributions)\n    state_totals = df.groupby(\"contributor_state\")[\"contribution_receipt_amount\"].sum()\n    print(state_totals)\nelse:\n    print(f\"Error: {response.status_code}\")\n</code></pre> <pre><code>## Error: 403\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the Campaign Finance API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like aggregate() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the Campaign Finance API.</p> <p>Note: Remember to replace your_api_key with your actual FEC API key in the code examples above.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#historic-redlining","title":"Historic Redlining","text":"<p>Historic redlining data refers to data from the Home Owners\u2019 Loan Corporation (HOLC) that created residential security maps in the 1930s, which contributed to racial segregation and disinvestment in minority neighborhoods. One popular source for this data is the Mapping Inequality project (https://dsl.richmond.edu/panorama/redlining/).</p> <p>In this example, we\u2019ll download historic redlining data for Philadelphia in the form of a GeoJSON file and analyze the data in R and Python.</p> <p>R: In R, we\u2019ll use the \u2018sf\u2019 and \u2018dplyr\u2019 packages to read and process the GeoJSON data.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\nlibrary(sf)\nlibrary(dplyr)\n\n# Download historic redlining data for Philadelphia\nurl &lt;- \"https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/PAPhiladelphia1937.geojson\"\nphilly_geojson &lt;- read_sf(url)\n\n# Count the number of areas per HOLC grade\ngrade_counts &lt;- philly_geojson %&gt;%\n  group_by(holc_grade) %&gt;%\n  summarize(count = n())\n\nplot(grade_counts)\n</code></pre> <p></p> <p>Python: In Python, we\u2019ll use the \u2018geopandas\u2019 library to read and process the GeoJSON data.</p> <p>Python code:</p> <pre><code># Install necessary libraries\n\n\nimport geopandas as gpd\n\n# Download historic redlining data for Philadelphia\nurl = \"https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/PAPhiladelphia1937.geojson\"\nphilly_geojson = gpd.read_file(url)\n\n# Count the number of areas per HOLC grade\ngrade_counts = philly_geojson[\"holc_grade\"].value_counts()\nprint(grade_counts)\n</code></pre> <pre><code>## B    28\n## D    26\n## C    18\n## A    10\n## Name: holc_grade, dtype: int64\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to download and process historic redlining data in the form of GeoJSON files. The \u2018sf\u2019 package in R provides a simple way to read and manipulate spatial data, while the \u2018geopandas\u2019 library in Python offers similar functionality. The \u2018dplyr\u2019 package in R can be used for data manipulation and analysis, and Python\u2019s built-in functions like value_counts() can be used for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with historic redlining data.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#american-indian-and-alaska-native-areas-aiannh","title":"American Indian and Alaska Native Areas (AIANNH)","text":"<p>In this example, we\u2019ll download and analyze the American Indian and Alaska Native Areas (AIANNH) TIGER/Line Shapefile from the U.S. Census Bureau. We\u2019ll download the data for the year 2020, and analyze the number of AIANNH per congressional district</p> <p>R: In R, we\u2019ll use the \u2018sf\u2019 and \u2018dplyr\u2019 packages to read and process the Shapefile data.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\nlibrary(sf)\nlibrary(dplyr)\n\n# Download historic redlining data for Philadelphia\nurl &lt;- \"https://www2.census.gov/geo/tiger/TIGER2020/AIANNH/tl_2020_us_aiannh.zip\"\ntemp_file &lt;- tempfile(fileext = \".zip\")\ndownload.file(url, temp_file, mode = \"wb\")\nunzip(temp_file, exdir = tempdir())\n\n# Read the Shapefile\nshapefile_path &lt;- file.path(tempdir(), \"tl_2020_us_aiannh.shp\")\naiannh &lt;- read_sf(shapefile_path)\n\n# Count the number of AIANNH per congressional district\nstate_counts &lt;- aiannh %&gt;%\n  group_by(LSAD) %&gt;%\n  summarize(count = n())\n\nprint(state_counts[order(-state_counts$count),])\n</code></pre> <pre><code>## Simple feature collection with 26 features and 2 fields\n## Geometry type: GEOMETRY\n## Dimension:     XY\n## Bounding box:  xmin: -174.236 ymin: 18.91069 xmax: -67.03552 ymax: 71.34019\n## Geodetic CRS:  NAD83\n## # A tibble: 26 \u00d7 3\n##    LSAD  count                                                          geometry\n##    &lt;chr&gt; &lt;int&gt;                                                &lt;MULTIPOLYGON [\u00b0]&gt;\n##  1 79      221 (((-166.5331 65.33918, -166.5331 65.33906, -166.533 65.33699, -1\u2026\n##  2 86      206 (((-83.38811 35.46645, -83.38342 35.46596, -83.38316 35.46593, -\u2026\n##  3 OT      155 (((-92.32972 47.81374, -92.3297 47.81305, -92.32967 47.81196, -9\u2026\n##  4 78       75 (((-155.729 20.02457, -155.7288 20.02428, -155.7288 20.02427, -1\u2026\n##  5 85       46 (((-122.3355 37.95215, -122.3354 37.95206, -122.3352 37.95199, -\u2026\n##  6 92       35 (((-93.01356 31.56287, -93.01354 31.56251, -93.01316 31.56019, -\u2026\n##  7 88       25 (((-97.35299 36.908, -97.35291 36.90801, -97.35287 36.908, -97.3\u2026\n##  8 96       19 (((-116.48 32.63814, -116.48 32.63718, -116.4794 32.63716, -116.\u2026\n##  9 84       16 (((-105.5937 36.40379, -105.5937 36.40324, -105.5937 36.40251, -\u2026\n## 10 89       11 (((-95.91705 41.28037, -95.91653 41.28036, -95.91653 41.28125, -\u2026\n## # \u2139 16 more rows\n</code></pre> <p>Python: In Python, we\u2019ll use the \u2018geopandas\u2019 library to read and process the Shapefile data.</p> <p>Python code:</p> <pre><code>import geopandas as gpd\nimport pandas as pd\nimport requests\nimport zipfile\nimport os\nfrom io import BytesIO\n\n# Download historic redlining data for Philadelphia\nurl = \"https://www2.census.gov/geo/tiger/TIGER2020/AIANNH/tl_2020_us_aiannh.zip\"\nresponse = requests.get(url)\nzip_file = zipfile.ZipFile(BytesIO(response.content))\n\n# Extract Shapefile\ntemp_dir = \"temp\"\nif not os.path.exists(temp_dir):\n    os.makedirs(temp_dir)\n\nzip_file.extractall(path=temp_dir)\nshapefile_path = os.path.join(temp_dir, \"tl_2020_us_aiannh.shp\")\n\n# Read the Shapefile\naiannh = gpd.read_file(shapefile_path)\n\n# Count the number of AIANNH per congressional district\nstate_counts = aiannh.groupby(\"LSAD\").size().reset_index(name=\"count\")\n\n# Sort by descending count\nstate_counts_sorted = state_counts.sort_values(by=\"count\", ascending=False)\n\nprint(state_counts_sorted)\n</code></pre> <pre><code>##    LSAD  count\n## 2    79    221\n## 9    86    206\n## 25   OT    155\n## 1    78     75\n## 8    85     46\n## 15   92     35\n## 11   88     25\n## 19   96     19\n## 7    84     16\n## 12   89     11\n## 5    82      8\n## 3    80      7\n## 4    81      6\n## 21   98      5\n## 20   97      5\n## 13   90      4\n## 18   95      3\n## 6    83      3\n## 17   94      2\n## 16   93      1\n## 14   91      1\n## 10   87      1\n## 22   99      1\n## 23   9C      1\n## 24   9D      1\n## 0    00      1\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to download and process AIANNH TIGER/Line Shapefile data from the U.S. Census Bureau. The \u2018sf\u2019 package in R provides a simple way to read and manipulate spatial data, while the \u2018geopandas\u2019 library in Python offers similar functionality. The \u2018dplyr\u2019 package in R can be used for data manipulation and analysis, and Python\u2019s built-in functions like value_counts() can be used for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with AIANNH data.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#indian-entities-recognized-and-eligible-to-receive-services-by-bia","title":"Indian Entities Recognized and Eligible To Receive Services by BIA","text":"<p>The Bureau of Indian Affairs (BIA) provides a PDF document containing a list of Indian Entities Recognized and Eligible To Receive Services. To analyze the data, we\u2019ll first need to extract the information from the PDF. In this example, we\u2019ll extract the names of the recognized tribes and count the number of tribes per state.</p> <p>R: In R, we\u2019ll use the \u2018pdftools\u2019 package to extract text from the PDF and the \u2018stringr\u2019 package to process the text data.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\nlibrary(pdftools)\nlibrary(stringr)\nlibrary(dplyr)\n\n# Download the BIA PDF\nurl &lt;- \"https://www.govinfo.gov/content/pkg/FR-2022-01-28/pdf/2022-01789.pdf\"\ntemp_file &lt;- tempfile(fileext = \".pdf\")\ndownload.file(url, temp_file, mode = \"wb\")\n\n# Extract text from the PDF\npdf_text &lt;- pdf_text(temp_file)\ntribe_text &lt;- pdf_text[4:length(pdf_text)]\n\n# Define helper functions\ntribe_state_extractor &lt;- function(text_line) {\n  regex_pattern &lt;- \"(.*),\\\\s+([A-Z]{2})$\"\n  tribe_state &lt;- str_match(text_line, regex_pattern)\n  return(tribe_state)\n}\n\nis_valid_tribe_line &lt;- function(text_line) {\n  regex_pattern &lt;- \"^\\\\d+\\\\s+\"\n  return(!is.na(str_match(text_line, regex_pattern)))\n}\n\n# Process text data to extract tribes and states\ntribe_states &lt;- sapply(tribe_text, tribe_state_extractor)\nvalid_lines &lt;- sapply(tribe_text, is_valid_tribe_line)\ntribe_states &lt;- tribe_states[valid_lines, 2:3]\n\n# Count the number of tribes per state\ntribe_data &lt;- as.data.frame(tribe_states)\ncolnames(tribe_data) &lt;- c(\"Tribe\", \"State\")\nstate_counts &lt;- tribe_data %&gt;%\n  group_by(State) %&gt;%\n  summarise(Count = n())\n\nprint(state_counts)\n</code></pre> <pre><code>## # A tibble: 0 \u00d7 2\n## # \u2139 2 variables: State &lt;chr&gt;, Count &lt;int&gt;\n</code></pre> <p>Python: In Python, we\u2019ll use the \u2018PyPDF2\u2019 library to extract text from the PDF and the \u2018re\u2019 module to process the text data.</p> <p>Python code:</p> <pre><code># Install necessary libraries\nimport requests\nimport PyPDF2\nimport io\nimport re\nfrom collections import Counter\n\n# Download the BIA PDF\nurl = \"https://www.bia.gov/sites/bia.gov/files/assets/public/raca/online-tribal-leaders-directory/tribal_leaders_2021-12-27.pdf\"\nresponse = requests.get(url)\n\n# Extract text from the PDF\npdf_reader = PyPDF2.PdfFileReader(io.BytesIO(response.content))\ntribe_text = [pdf_reader.getPage(i).extractText() for i in range(3, pdf_reader.numPages)]\n\n# Process text data to extract tribes and states\ntribes = [re.findall(r'^\\d+\\s+(.+),\\s+([A-Z]{2})', line) for text in tribe_text for line in text.split('\\n') if line]\ntribe_states = [state for tribe, state in tribes]\n\n# Count the number of tribes per state\nstate_counts = Counter(tribe_states)\nprint(state_counts)\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to download and process the list of Indian Entities Recognized and Eligible To Receive Services from the BIA. The \u2018pdftools\u2019 package in R provides a simple way to extract text from PDF files, while the \u2018PyPDF2\u2019 library in Python offers similar functionality. The \u2018stringr\u2019 package in R and the \u2018re\u2019 module in Python can be used to process and analyze text data. Depending on your preferred programming language and environment, both options can be effective for working with BIA data.</p>"},{"location":"2_R_and_Py_bilingualism/code/bilingualism_md/#national-atlas-indian-lands-of-the-united-states-dataset","title":"National Atlas - Indian Lands of the United States dataset","text":"<p>In this example, we will download and analyze the National Atlas - Indian Lands of the United States dataset in both R and Python. We will read the dataset and count the number of Indian lands per state.</p> <p>R: In R, we\u2019ll use the \u2018sf\u2019 package to read the Shapefile and the \u2018dplyr\u2019 package to process the data.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\n\nlibrary(sf)\nlibrary(dplyr)\n\n# Download the Indian Lands dataset\nurl &lt;- \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/indlanp010g.shp_nt00968.tar.gz\"\ntemp_file &lt;- tempfile(fileext = \".tar.gz\")\ndownload.file(url, temp_file, mode = \"wb\")\nuntar(temp_file, exdir = tempdir())\n\n# Read the Shapefile\nshapefile_path &lt;- file.path(tempdir(), \"indlanp010g.shp\")\nindian_lands &lt;- read_sf(shapefile_path)\n\n# Count the number of Indian lands per state\n# state_counts &lt;- indian_lands %&gt;%\n#   group_by(STATE) %&gt;%\n#   summarize(count = n())\n\nplot(indian_lands)\n</code></pre> <pre><code>## Warning: plotting the first 9 out of 23 attributes; use max.plot = 23 to plot\n## all\n</code></pre> <p></p> <p>Python: In Python, we\u2019ll use the \u2018geopandas\u2019 and \u2018pandas\u2019 libraries to read the Shapefile and process the data.</p> <p>Python code:</p> <pre><code>import geopandas as gpd\nimport pandas as pd\nimport requests\nimport tarfile\nimport os\nfrom io import BytesIO\n\n# Download the Indian Lands dataset\nurl = \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/indlanp010g.shp_nt00966.tar.gz\"\nresponse = requests.get(url)\ntar_file = tarfile.open(fileobj=BytesIO(response.content), mode='r:gz')\n\n# Extract Shapefile\ntemp_dir = \"temp\"\nif not os.path.exists(temp_dir):\n    os.makedirs(temp_dir)\n\ntar_file.extractall(path=temp_dir)\nshapefile_path = os.path.join(temp_dir, \"indlanp010g.shp\")\n\n# Read the Shapefile\nindian_lands = gpd.read_file(shapefile_path)\n\n# Count the number of Indian lands per state\nstate_counts = indian_lands.groupby(\"STATE\").size().reset_index(name=\"count\")\n\nprint(state_counts)\n</code></pre> <p>Both R and Python codes download the dataset and read the Shapefile using the respective packages. They then group the data by the \u2018STATE\u2019 attribute and calculate the count of Indian lands per state.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/","title":"R and Python bilingualism","text":""},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#welcome-to-the-r-and-python-bilingualism-reference-guide","title":"Welcome to the R and Python bilingualism reference guide!","text":"<p>If you\u2019re fluent in one of these languages but hesitant to learn the other, you\u2019re in the right place. The good news is that there are many similarities between R and Python that make it easy to switch between the two.</p> <p>Both R and Python are widely used in data science and are open-source, meaning that they are free to use and constantly being improved by the community. They both have extensive libraries for data analysis, visualization, and machine learning. In fact, many of the libraries in both languages have similar names and functions, such as Pandas in Python and data.table in R.</p> <p>While there are differences between the two languages, they can complement each other well. Python is versatile and scalable, making it ideal for large and complex projects such as web development and artificial intelligence. Object-oriented programming is easier and more common in Python, which is useful for large development teams. R, on the other hand, is known for its exceptional statistical capabilities and is often used in data analysis and modeling. Visualization is also easier in R, making it a popular choice for creating graphs and charts.</p> <p>By learning both R and Python, you\u2019ll be able to take advantage of the strengths of each language and create more efficient and robust data analysis workflows. Don\u2019t let the differences between the two languages intimidate you - once you become familiar with one, learning the other will be much easier.</p> <p>So, whether you\u2019re a Python enthusiast looking to expand your statistical analysis capabilities, or an R user interested in exploring the world of web development and artificial intelligence, this guide will help you become bilingual in R and Python.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#try-it-yourself","title":"Try it yourself!","text":"<p>We have hidden most of the code in this notebook so that you can try to work it out yourself first if you like. We challenge you to pick the coding language you are least familiar with and try writing these workflows. If you need help, you can look at the code in the more familiar language, or check out our version in the less familiar for hints.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#how-to-get-help","title":"How to get help","text":"<p>If you know how to get help with one programming language, it\u2019s not too different in a different one.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#offline-in-your-coding-environment","title":"Offline in your coding environment","text":"<p>In both R and Python, you can print out the documentation for any function or object using the <code>help()</code> function. For example:</p> <pre><code>help(print)\n</code></pre> <pre><code>help(print)\n</code></pre> <pre><code>Help on built-in function print in module builtins:\n\nprint(...)\n    print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n\n    Prints the values to a stream, or to sys.stdout by default.\n    Optional keyword arguments:\n    file:  a file-like object (stream); defaults to the current sys.stdout.\n    sep:   string inserted between values, default a space.\n    end:   string appended after the last value, default a newline.\n    flush: whether to forcibly flush the stream.\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#official-documentation-pages","title":"Official documentation pages","text":"<p>There are also some great documentation of Python, R, and third party libraries on the internet. You can find the same information about parameters, attributes, returns, and methods that you get by running <code>help()</code>, online. We also recommend checking out the associated user guides and plot gallerys!</p> <p>Here are some of our favorite documentation pages. First, manuals for the programming languages:</p> <ul> <li> <p>Python manual</p> </li> <li> <p>The R manuals, formatted for   the web</p> </li> </ul> <p>Note that if you are citing a program language, it is conventional to cite the manual for the version you used unless the authors specify otherwise.</p> <p>Some of our favorite Python libraries for Earth Data Science:</p> <ul> <li> <p>xarray labeled multidimensional   arrays and rioxarray arrays with   coordinates</p> </li> <li> <p>pandas data frames: tabular data and   databases and geopandas data frames   with coordinates</p> </li> <li> <p>Seaborn data visualization</p> </li> </ul> <p>Some of our favorite R libraries for Earth Data Science:</p> <ul> <li> <p>tidyverse collection of libraries for data   science</p> </li> <li> <p>ggplot2 data visualization (technically part of tidyverse but often   used on its own) and cowplot figure   layout</p> </li> </ul> <p>We recommend checking out official documentation before heading to sites like CodeAcademy or GeeksForGeeks that summarize documentation. They are often missing key details (but can have examples if you are having trouble finding those)</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#books","title":"Books","text":"<p>There are many many books out there on how to learn both R and Python! We have heard from many learners who had success with the O\u2019Reill books. In particular, you can check out the following freely available books for download or reading on the web:</p> <ul> <li>Think Python: How to Think Like a Computer   Scientist by Dr. Allen   Downey - This book is especially targeted at developing bilingualism,   even though it is a Python book. It originally was published as Think   Java, and then:</li> </ul>     &gt; \u201cJeff Elkner, a high school teacher in Virginia, adopted my book and   &gt; translated it into Python. He sent me a copy of his translation, and   &gt; I had the unusual experience of learning Python by reading my own   &gt; book.\u201d     <ul> <li>R for Data Science by Hadley Wickham   focuses on tidyverse-style R but covers a lot of the intricacies of   the language as well.</li> </ul>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#message-boards","title":"Message boards","text":"<p>Finally, there are lots of sites out there where you can post and answer questions, or read other peoples\u2019 questions and answers. Some of our favorites:</p> <ul> <li> <p>StackOverflow</p> </li> <li> <p>GitHub Issues pages for the project in question</p> </li> <li> <p>Or you can search and see what comes up - there are other good but   more obscure sources out there like Google groups pages</p> </li> </ul>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#chatgpt","title":"ChatGPT","text":"<p>We used ChatGPT to write the first version of this document because it\u2019s pretty good at coding. Some things you can try:</p> <ul> <li> <p>Paste some code and ask ChatGPT what it does</p> </li> <li> <p>Paste some code and ask ChatGPT to find bugs in it</p> </li> <li> <p>Ask ChatGPT to write you code that does what you want</p> </li> <li> <p>Ask ChatGPT to translate from R to Python or vice versa</p> </li> </ul> <p>While there\u2019s no guarantee, we\u2019ve gotten plenty of working answers out, with minimal changes needed. ChatGPT can be loaded as an extension to many development environments like VSCode, although you may hit a pay wall.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#packages-no-need-to-write-it-from-scratch","title":"Packages: No need to write it from scratch","text":"<p>One thing Python and R have in common is they are extendable using external packages or libraries. You should have all the packages you need for today installed already.</p> <p>Note that package and library are used interchangeably</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#installing-packages","title":"Installing packages","text":"<p>R libraries are stored and managed in a repository called CRAN. You can download and install R packages with the install.packages() function:</p> <p>R code:</p> <pre><code># Install the dplyr package from CRAN\ninstall.packages(\"dplyr\")\n</code></pre> <p>Installing Python packages can be trickier than installing R packages. Python libraries are stored and managed in a few different repositories and their dependencies are not regulated as strictly as R libraries are in CRAN. Python\u2019s default package manager <code>pip</code> can be used in a terminal or command line to install packages from the <code>PyPI</code> repository, and this is a good solution if you need to install a single simple package, like the <code>requests</code> package that provides a clean interface for downloads. However, <code>pip</code> has long had some challenges, which you are very likely to run into if you are using a package that requires the GDAL library for coordinate transformations (like <code>cartopy</code>, <code>rioxarray</code> or <code>geopandas</code>).</p> <p>So what is an earth scientist to do? Most packages can also be installed from the Anaconda repository by using <code>conda install ...</code> or better yet <code>mamba install ...</code>. <code>mamba</code> is a version of <code>conda</code> that is much faster at solving environments. For most packages, we also recommending specifying the <code>conda-forge</code> channel, as below:</p> <p>bash code to install Python packages:</p> <pre><code># First install mamba with conda if needed\nconda install -c conda-forge mamba\n# Install the pandas package from Anaconda\nmamba install -c conda-forge pandas\n</code></pre> <p>It is common to install all the packages you need at once to keep from boxing yourself into a corner with incompatible version numbers, or to facilitate packaging your own library. In R, the <code>renv</code> library has a number of tools for enviroment management; in Python, your best bet is <code>mamba</code>. You should have everything you need already if you are working on JetStream2. However, if you want to run this code on your own computer, you can go ahead and install the R and Python environments as follows, provided that you have renv and mamba installed:</p> <pre><code>renv::install()\n</code></pre> <pre><code>mamba env create -f environment.yml\n</code></pre> <p>For the Python environment to work within an RMarkdown file, you may need to run the following code in R:</p> <pre><code>library(reticulate)\nSys.setenv(\n  RETICULATE_PYTHON=path.expand(\n    '~/opt/miniconda3/envs/earth-analytics-python/bin/python'))\nuse_condaenv('earth-analytics-python')\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#loading-libraries-in-r-and-python","title":"Loading libraries in R and Python","text":"<p>In R, libraries can be loaded using the library() function:</p> <p>R code:</p> <pre><code># Load the dplyr library\nlibrary(dplyr)\n</code></pre> <p>In Python, libraries can be loaded using the import statement. Here\u2019s an example:</p> <p>Python code:</p> <pre><code># Load the pandas library\nimport pandas as pd\n</code></pre> <p>Note that the package or library must be installed from the respective repository before it can be loaded. Also, make sure you have the correct repository specified in your system before installing packages. By default, R uses CRAN as its primary repository, whereas Anaconda uses its own repository by default.</p> <p>Importing libraries from R imports all the functions inside the library under their own name. For example, once you have installed the r package <code>readr</code>, you can use the included function <code>read.csv()</code> directly. On the other hand, with a typical Python import like the one for <code>pandas</code> above, you need to specify the package, e.g.\u00a0<code>pd.read_csv</code>. This can be cumbersome if you have to type the package name a lot (that\u2019s why we <code>import pandas as pd</code> instead of <code>import pandas</code> \u2013 our way we only have to type out the two-letter alias <code>pd</code>). On the other hand, it can also be really handy if you are trying to figure out what packages you need to install to use a function in your own code.</p> <p>GOTCHA ALERT: The dot <code>.</code> is a special character in Python, but not in R. It is kind of like the <code>/</code> in your file system; instead of navigating your file tree, it navigates a tree structure like a class or a library.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#reticulate-why-choose","title":"reticulate - Why choose?","text":"<p>The reticulate package lets you run share variables between R and Python.</p> <p>GOTCHA ALERT: Note that you will need to set the <code>RETICULATE_PYTHON</code> environment variable correctly in order to get Python to work using <code>reticulate</code>. The easiest way is by making a <code>.Renviron</code> file in your project directory.</p> <p>You only need to install packages once, but you need to mount those packages with the library() function each time you open R.</p> <pre><code>library(reticulate)\n</code></pre> <p>Now, let\u2019s create a Python list and assign it to a variable py_list:</p> <p>R code:</p> <pre><code>py_list = [1, 2, 3]\n</code></pre> <p>We can now print out the py_list variable in Python using the py_run_string() function:</p> <p>R code:</p> <pre><code>py_run_string(\"print(py_list)\")\n</code></pre> <p>This will output <code>[1, 2, 3]</code> in the Python console.</p> <p>Now, let\u2019s create an R vector and assign it to a variable r_vec:</p> <p>R code:</p> <pre><code>r_vec &lt;- c(4, 5, 6)\n</code></pre> <p>Notice that in Python we use <code>=</code> to assign variables, while in R we use <code>&lt;-</code>. Actually, we could use either one in R, but here we\u2019re following the popular tidyverse style guide. In R there is a distinction between the assignment operator <code>&lt;-</code> and the parameter setting operator <code>=</code>. In Python these are considered the same.</p> <p>We can now print out the py_list variable in R using the py$ syntax to access Python variables:</p> <p>R code:</p> <pre><code>print(py$py_list)\nprint(py)\n</code></pre> <p>This will output [1, 2, 3] in the R console. Conversely, we can get the R variable <code>r_vec</code> in Python:</p> <pre><code>print(r.r_vec)\n</code></pre> <p>There\u2019s that dot <code>.</code> again - in this case it is getting the r_vec variable from within the \u201cR interface object\u201d <code>r</code> similarly to how the <code>$</code> character gets attributes of the <code>py</code> object.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#load-packages-and-change-settings","title":"Load packages and change settings","text":"<p>In both R and Python, it is suggested to do all your library/package imports at the top of your file. This makes it easier for others to run or copy your code. However, for the purposes of this tutorial, we are loading libraries in the code cell where they are used so that you can copy and paste the entire snippet.</p> <p>R and Python are two popular programming languages used for data analysis, statistics, and machine learning. Although they share some similarities, there are some fundamental differences between them. Here\u2019s an example code snippet in R and Python to illustrate some of the differences:</p> <p>R Code:</p> <pre><code># Create a vector of numbers from 1 to 10\nx &lt;- 1:10\n\n# Compute the mean of the vector\nmean_x &lt;- mean(x)\n\n# Print the result\nprint(mean_x)\n</code></pre> <pre><code>[1] 5.5\n</code></pre> <p>Python Code:</p> <pre><code>import numpy as np\n\n# Create a numpy array of numbers from 1 to 10\nx_arr = np.array(range(1, 10))\n\n# Compute the mean of the array\nmean_x = x_arr.mean()\n\n# Print the result\nprint(mean_x)\n</code></pre> <pre><code>5.0\n</code></pre> <p>In this example, we can see that there are several differences between R and Python:</p> <p>Syntax: <code>&lt;-</code> in R vs.\u00a0<code>=</code> in Python; <code>$</code> in R vs.\u00a0<code>.</code> in Python</p> <p>Libraries: Python relies heavily on external libraries such as numpy, pandas, and xarray for data analysis, while R has built-in functions for many data analysis tasks. The Python libraries require extra installation steps, but they have the potential to be much faster when working with large amounts of data because they are compiled to take full advantage of your hardware and advances in mathematics kernel libraries.</p> <p>Function vs.\u00a0Methods: Python is an object-oriented language, which means that where you would use a function in R, you often must apply a method to an object in Python - see <code>mean()</code> in R vs.\u00a0<code>x_arr.mean()</code> in Python. Another way to think about this is that functions and methods are verbs (<code>pd.read_csv()</code>), classes are nouns (<code>csv.writer()</code>), and methods are also verbs but they act on the object they are part of (<code>my_csv_writer.writerows()</code>).</p> <p>Style:</p> <p>These are just a few of the many differences between R and Python. Ultimately, the choice between the two languages will depend on your specific needs and preferences.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#using-and-managing-tabular-data","title":"Using and managing tabular data","text":""},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#load-tabular-data-from-the-web","title":"Load tabular data from the web","text":"<p>For tabular data like comma separate value (CSV) files, all you need to get started is a web url. Another thing to notice in the following cells is how to add line breaks in long strings</p> <p>R Code:</p> Show the R code <pre><code>library(readr)\n\npenguins_url &lt;- paste0(\n  'https://raw.githubusercontent.com/allisonhorst/palmerpenguins/',\n  'main/inst/extdata/penguins.csv')\n\n# Load penguins data\npenguins_df &lt;- read_csv(penguins_url)\n</code></pre> <pre><code>Rows: 344 Columns: 8\n\u2500\u2500 Column specification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year\n\n\u2139 Use `spec()` to retrieve the full column specification for this data.\n\u2139 Specify the column types or set `show_col_types = FALSE` to quiet this message.\n</code></pre> Show the R code <pre><code>penguins_df\n</code></pre> <pre><code># A tibble: 344 \u00d7 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# \u2139 334 more rows\n# \u2139 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n</code></pre> <p>Python code:</p> Show the Python code <pre><code>import pandas as pd\n\npenguins_url = (\n    'https://raw.githubusercontent.com/allisonhorst/palmerpenguins/'\n    'main/inst/extdata/penguins.csv'\n)\n\n# Load in penguins data\npenguins_df = pd.read_csv(penguins_url)\npenguins_df\n</code></pre> <pre><code>       species     island  bill_length_mm  ...  body_mass_g     sex  year\n0       Adelie  Torgersen            39.1  ...       3750.0    male  2007\n1       Adelie  Torgersen            39.5  ...       3800.0  female  2007\n2       Adelie  Torgersen            40.3  ...       3250.0  female  2007\n3       Adelie  Torgersen             NaN  ...          NaN     NaN  2007\n4       Adelie  Torgersen            36.7  ...       3450.0  female  2007\n..         ...        ...             ...  ...          ...     ...   ...\n339  Chinstrap      Dream            55.8  ...       4000.0    male  2009\n340  Chinstrap      Dream            43.5  ...       3400.0  female  2009\n341  Chinstrap      Dream            49.6  ...       3775.0    male  2009\n342  Chinstrap      Dream            50.8  ...       4100.0    male  2009\n343  Chinstrap      Dream            50.2  ...       3775.0  female  2009\n\n[344 rows x 8 columns]\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#forming-reproducible-file-paths","title":"Forming reproducible file paths","text":"<p>In both R and Python, it is important to use reproducible file paths when collaborating on or sharing code. That means files should be relative to the current directory, or even better inside the home directory so it can be specified on every computer.</p> <p>First, we might want to know where the working directory is. If you build a relative file path (one that doesn\u2019t start with something like a <code>/</code> on Unix systems or <code>C://</code> on Windows systems) the working directory will be where your code looks for that path:</p> <pre><code>print(getwd())\n</code></pre> <pre><code>[1] \"/Users/elsa/04-workshops/pre-innovation-summit-training/docs/2_R_and_Py_bilingualism/code/code_demo\"\n</code></pre> <pre><code>import os\n\nos.getcwd()\n</code></pre> <pre><code>'/Users/elsa/04-workshops/pre-innovation-summit-training/docs/2_R_and_Py_bilingualism/code/code_demo'\n</code></pre> <p>Next, let\u2019s make a cross-platform file path to a data directory in your home folder and set that as your new working directory. Despite the use of the Unix-style <code>~</code> to indicate the home directory, this code should work on both Windows and Unix computers. Windows users can check out <code>help(path.expand)</code> in the R console for more information on what the home directory is.</p> <p>We know that sometimes it doesn\u2019t work to put data in your home directory. Another option is putting a configuration file with the data path in the home directory. Keeping the data in your project directory is also an option.</p> Show the R code <pre><code>data_dir &lt;- file.path(path.expand('~'), 'esiil-summit', 'r_and_py')\n\n# Make the data directory\ndir.create(data_dir, showWarnings=F, recursive=T)\nprint(data_dir)\n</code></pre> <pre><code>[1] \"/Users/elsa/esiil-summit/r_and_py\"\n</code></pre> Show the Python code <pre><code>import os\nimport pathlib\n\ndata_dir = os.path.join(pathlib.Path.home(), 'esiil-summit', 'r_and_py')\n\n# Make the data directory\nos.makedirs(data_dir, exist_ok=True)\nprint(data_dir)\n</code></pre> <pre><code>/Users/elsa/esiil-summit/r_and_py\n</code></pre> <p>You should see your username in both paths.</p> <p>Parameter notes:</p> <ul> <li> <p>In R we get a warning if the directory we\u2019re creating already exists,   which is suppressed by the <code>showWarnings=F</code> parameter. In Python, it   is an error unless the <code>exist_ok=True</code> parameter is supplied.</p> </li> <li> <p>In R, we can use the same function to create a single directory and   multiple nested directories, as long as we use the <code>recursive=T</code>   parameter. In Python, there is a different <code>os.mkdir()</code> function for   creating single directories.</p> </li> </ul> <p>GOTCHA ALERT: In Python, boolean values are <code>True</code> and <code>False</code>; in R they are <code>TRUE</code> or <code>T</code> and <code>FALSE</code> or <code>F</code></p> <p>Finally, let\u2019s practice copying the data files included in this lesson to the reproducible data directory:</p> Show the R code <pre><code>project_data_dir &lt;- file.path('..', '..', 'data')\ndata_dir &lt;- file.path(path.expand('~'), 'esiil-summit', 'r_and_py')\nfile.copy(from=project_data_dir, to=data_dir, recursive=T)\n</code></pre> <pre><code>[1] TRUE\n</code></pre> Show the Python code <pre><code>import os\nimport pathlib\nimport shutil\n\nproject_data_dir = os.path.join('..', '..', 'data')\ndata_dir = os.path.join(pathlib.Path.home(), 'esiil-summit', 'r_and_py')\nshutil.move(project_data_dir, data_dir)\n</code></pre> <pre><code>Error: shutil.Error: Destination path '/Users/elsa/esiil-summit/r_and_py/data' already exists\n</code></pre> <p>Note that Python will not replace an existing directory, so you mush do only one of these operations (or delete the files)</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#getting-tabular-data-to-and-from-text-files","title":"Getting tabular data to and from text files","text":"<p>You can also load data from comma-separated value (<code>.csv</code>) and other tabular text files.</p> Show the R code <pre><code>library(readr)\n\ndata_dir &lt;- file.path(path.expand('~'), 'esiil-summit', 'r_and_py')\npenguins_csv_path = file.path(data_dir, \"penguins.csv\")\n\npenguins_df &lt;- read_csv(penguins_csv_path)\n</code></pre> <pre><code>New names:\nRows: 344 Columns: 9\n\u2500\u2500 Column specification\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Delimiter: \",\" chr\n(3): species, island, sex dbl (6): ...1, bill_length_mm, bill_depth_mm,\nflipper_length_mm, body_mass_g...\n\u2139 Use `spec()` to retrieve the full column specification for this data. \u2139\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n\u2022 `` -&gt; `...1`\n</code></pre> Show the R code <pre><code>penguins_df\n</code></pre> <pre><code># A tibble: 344 \u00d7 9\n    ...1 species island    bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n 1     0 Adelie  Torgersen           39.1          18.7               181\n 2     1 Adelie  Torgersen           39.5          17.4               186\n 3     2 Adelie  Torgersen           40.3          18                 195\n 4     3 Adelie  Torgersen           NA            NA                  NA\n 5     4 Adelie  Torgersen           36.7          19.3               193\n 6     5 Adelie  Torgersen           39.3          20.6               190\n 7     6 Adelie  Torgersen           38.9          17.8               181\n 8     7 Adelie  Torgersen           39.2          19.6               195\n 9     8 Adelie  Torgersen           34.1          18.1               193\n10     9 Adelie  Torgersen           42            20.2               190\n# \u2139 334 more rows\n# \u2139 3 more variables: body_mass_g &lt;dbl&gt;, sex &lt;chr&gt;, year &lt;dbl&gt;\n</code></pre> Show the Python code <pre><code>import pandas as pd\n\npenguins_csv_path = os.path.join(data_dir, \"penguins.csv\")\n\npenguins_df = pd.read_csv(penguins_csv_path)\npenguins_df\n</code></pre> <pre><code>     Unnamed: 0    species     island  ...  body_mass_g     sex  year\n0             0     Adelie  Torgersen  ...       3750.0    male  2007\n1             1     Adelie  Torgersen  ...       3800.0  female  2007\n2             2     Adelie  Torgersen  ...       3250.0  female  2007\n3             3     Adelie  Torgersen  ...          NaN     NaN  2007\n4             4     Adelie  Torgersen  ...       3450.0  female  2007\n..          ...        ...        ...  ...          ...     ...   ...\n339         339  Chinstrap      Dream  ...       4000.0    male  2009\n340         340  Chinstrap      Dream  ...       3400.0  female  2009\n341         341  Chinstrap      Dream  ...       3775.0    male  2009\n342         342  Chinstrap      Dream  ...       4100.0    male  2009\n343         343  Chinstrap      Dream  ...       3775.0  female  2009\n\n[344 rows x 9 columns]\n</code></pre> <p>Let\u2019s save that data so if we like we can work offline (and avoid hitting the server too many times). You can also save your own results or processed data this way.</p> <p>Notice that in R this is a function and in Python it is a method of our <code>pd.DataFrame</code> object.</p> Show the R code <pre><code>library(readr)\n\ndata('iris')\n\ndata_dir &lt;- file.path(path.expand('~'), 'esiil-summit', 'r_and_py')\n\n# Write iris data to CSV\niris_r_csv_path = file.path(data_dir, \"iris_r.csv\")\nwrite_csv(iris, file=iris_r_csv_path)\n</code></pre> Show the Python code <pre><code>import seaborn as sns\n</code></pre> <pre><code>/Users/elsa/opt/miniconda3/envs/earth-analytics-python/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n</code></pre> Show the Python code <pre><code>iris_df = sns.load_dataset('iris')\n\n# Write iris data to CSV\niris_py_csv_path = os.path.join(data_dir, \"iris_py.csv\")\niris_df.to_csv(iris_py_csv_path, index=False)\n</code></pre> <p>GOTCHA ALERT: The pandas <code>pd.DataFrame.to_csv()</code> method is not quite symmetrical with <code>pd.read_csv()</code>. This is because pandas <code>DataFrame</code>s have an index, or row identifier, and you need to choose how to deal with it when reading and writing files. Since in this case the index is simply a row number and not critical information, we\u2019ve solved this problem by eliminating the index altogether in the file using the <code>index=False</code> parameter. Go ahead and try removing it to see what happens when we reload!</p> <p>You can also load data from comma-separated value (<code>.csv</code>) and other tabular text files.</p> Show the R code <pre><code>library(readr)\n\npenguins_csv_path = file.path(data_dir, \"penguins.csv\")\n\npenguins_df &lt;- read_csv(penguins_csv_path)\n</code></pre> <pre><code>New names:\nRows: 344 Columns: 9\n\u2500\u2500 Column specification\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Delimiter: \",\" chr\n(3): species, island, sex dbl (6): ...1, bill_length_mm, bill_depth_mm,\nflipper_length_mm, body_mass_g...\n\u2139 Use `spec()` to retrieve the full column specification for this data. \u2139\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n\u2022 `` -&gt; `...1`\n</code></pre> Show the R code <pre><code>penguins_df\n</code></pre> <pre><code># A tibble: 344 \u00d7 9\n    ...1 species island    bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n 1     0 Adelie  Torgersen           39.1          18.7               181\n 2     1 Adelie  Torgersen           39.5          17.4               186\n 3     2 Adelie  Torgersen           40.3          18                 195\n 4     3 Adelie  Torgersen           NA            NA                  NA\n 5     4 Adelie  Torgersen           36.7          19.3               193\n 6     5 Adelie  Torgersen           39.3          20.6               190\n 7     6 Adelie  Torgersen           38.9          17.8               181\n 8     7 Adelie  Torgersen           39.2          19.6               195\n 9     8 Adelie  Torgersen           34.1          18.1               193\n10     9 Adelie  Torgersen           42            20.2               190\n# \u2139 334 more rows\n# \u2139 3 more variables: body_mass_g &lt;dbl&gt;, sex &lt;chr&gt;, year &lt;dbl&gt;\n</code></pre> Show the Python code <pre><code>import os\n\nimport pandas as pd\n\npenguins_csv_path = os.path.join(data_dir, \"penguins.csv\")\n\npenguins_df = pd.read_csv(penguins_csv_path)\npenguins_df\n</code></pre> <pre><code>     Unnamed: 0    species     island  ...  body_mass_g     sex  year\n0             0     Adelie  Torgersen  ...       3750.0    male  2007\n1             1     Adelie  Torgersen  ...       3800.0  female  2007\n2             2     Adelie  Torgersen  ...       3250.0  female  2007\n3             3     Adelie  Torgersen  ...          NaN     NaN  2007\n4             4     Adelie  Torgersen  ...       3450.0  female  2007\n..          ...        ...        ...  ...          ...     ...   ...\n339         339  Chinstrap      Dream  ...       4000.0    male  2009\n340         340  Chinstrap      Dream  ...       3400.0  female  2009\n341         341  Chinstrap      Dream  ...       3775.0    male  2009\n342         342  Chinstrap      Dream  ...       4100.0    male  2009\n343         343  Chinstrap      Dream  ...       3775.0  female  2009\n\n[344 rows x 9 columns]\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#serialize-data","title":"Serialize data","text":"<p>In both Python and R, you can also serialize objects so they can be read back into the environment quickly (but aren\u2019t easily readable by other programming languages).</p> <p>Why serialize? This can be a great option when you want to cache intermediate analysis results, long downloads, or if you need to send objects from one worker to another in a multiprocessing context.</p> <p>R Code:</p> Show the R code <pre><code>penguins_rds_path = file.path(data_dir, \"penguins.rds\")\n\n# Serialize penguin data\nsaveRDS(penguins_df, file=penguins_rds_path)\n</code></pre> <p>Python code:</p> Show the Python code <pre><code>import os\n\npenguins_pickle_path = os.path.join(data_dir, \"penguins.pickle\")\n\n# Serialize penguins data\npenguins_df.to_pickle(penguins_pickle_path)\n</code></pre> <p>Go ahead and take a look at those files to see what they look like! One way to do so is by using a terminal and the command <code>head /path/to/penguins/file</code>.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#load-serialized-data","title":"Load serialized data","text":"Show the R code <pre><code>penguins_rds_path = file.path(data_dir, \"penguins.rds\")\n\n# Unserialize penguin data\npenguins_from_serial_df &lt;- readRDS(penguins_rds_path)\npenguins_from_serial_df\n</code></pre> <pre><code># A tibble: 344 \u00d7 9\n    ...1 species island    bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n 1     0 Adelie  Torgersen           39.1          18.7               181\n 2     1 Adelie  Torgersen           39.5          17.4               186\n 3     2 Adelie  Torgersen           40.3          18                 195\n 4     3 Adelie  Torgersen           NA            NA                  NA\n 5     4 Adelie  Torgersen           36.7          19.3               193\n 6     5 Adelie  Torgersen           39.3          20.6               190\n 7     6 Adelie  Torgersen           38.9          17.8               181\n 8     7 Adelie  Torgersen           39.2          19.6               195\n 9     8 Adelie  Torgersen           34.1          18.1               193\n10     9 Adelie  Torgersen           42            20.2               190\n# \u2139 334 more rows\n# \u2139 3 more variables: body_mass_g &lt;dbl&gt;, sex &lt;chr&gt;, year &lt;dbl&gt;\n</code></pre> <p>In R, there are a number of slightly different functions for serializing data. For example, you can check out base R <code>save()</code> and <code>load()</code> functions as well if you need to save multiple objects (or a whole environment) at once.</p> Show the Python code <pre><code>import pandas as pd\n\npenguins_pickle_path = os.path.join(data_dir, \"penguins.pickle\")\n\n# Unserialize penguin data\npenguins_from_serial_df = pd.read_pickle(penguins_pickle_path)\npenguins_from_serial_df\n</code></pre> <pre><code>     Unnamed: 0    species     island  ...  body_mass_g     sex  year\n0             0     Adelie  Torgersen  ...       3750.0    male  2007\n1             1     Adelie  Torgersen  ...       3800.0  female  2007\n2             2     Adelie  Torgersen  ...       3250.0  female  2007\n3             3     Adelie  Torgersen  ...          NaN     NaN  2007\n4             4     Adelie  Torgersen  ...       3450.0  female  2007\n..          ...        ...        ...  ...          ...     ...   ...\n339         339  Chinstrap      Dream  ...       4000.0    male  2009\n340         340  Chinstrap      Dream  ...       3400.0  female  2009\n341         341  Chinstrap      Dream  ...       3775.0    male  2009\n342         342  Chinstrap      Dream  ...       4100.0    male  2009\n343         343  Chinstrap      Dream  ...       3775.0  female  2009\n\n[344 rows x 9 columns]\n</code></pre> <p>You can also use the <code>pickle</code> library to pickle objects in Python. In this case, the <code>pd.DataFrame</code> method for pickling takes care of some of the details for us (as long as we\u2019re working only with <code>pandas</code> objects).</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#data-plots","title":"Data Plots","text":"<p>R Code:</p> Show the R code <pre><code># Plot the penguin data\nggplot(penguins_df, aes(x = bill_length_mm, y = body_mass_g, color=species)) +\n  # Create a scatter plot of the data\n  geom_point() +\n  # Label the plot\n  labs(x = 'Bill Length (mm)', y = 'Body Mass (g)', color = 'Species') +\n  # Add a title\n  ggtitle('Penguin Characteristics by Species')\n</code></pre> <pre><code>Warning: Removed 2 rows containing missing values (`geom_point()`).\n</code></pre> <p></p> <p>Python seaborn.objects code:</p> Show the Python code <pre><code>import seaborn.objects as so\n# Make a scatter plot with the penguin data\n(\n    so.Plot(penguins_df, x='bill_length_mm', y='body_mass_g')\n    # Make a scatter plot colored by species\n    .add(so.Dot(), color='species')\n    # Add labels\n    .label(\n        x='Bill Length (mm)', y='Body Mass (g)', color='Species',\n        title='Penguin Characteristics by Species')\n    # Give the legend some more room so it doesn't overlap the data\n    .layout(engine=\"constrained\")\n    # Display the plot\n    .show()\n)\n</code></pre> <pre><code>/Users/elsa/opt/miniconda3/envs/earth-analytics-python/lib/python3.8/_collections_abc.py:832: MatplotlibDeprecationWarning: \nThe savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n  self[key] = other[key]\n/Users/elsa/opt/miniconda3/envs/earth-analytics-python/lib/python3.8/_collections_abc.py:832: MatplotlibDeprecationWarning: \nThe savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n  self[key] = other[key]\n/Users/elsa/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/reticulate/1.28/86c441bf33e1d608db773cb94b848458/reticulate/python/rpytools/call.py:10: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  value, error = rpycall.call_r_function(f, *args, **kwargs)\n</code></pre> <p></p> <p>GOTCHA ALERT: In Python, you will usually need to put <code>.show()</code> or something similar at the end of your code to see your plot. Otherwise you may see something like:</p> <pre><code>&lt;seaborn._core.plot.Plot object at 0x7f81493e4070&gt;\n</code></pre> Show the Python code <pre><code>from plotnine import ggplot, aes, geom_point, labs, theme, ggtitle\n\n# Plot the penguin data\n(\n    ggplot(penguins_df, aes(x = 'bill_length_mm', y = 'body_mass_g', color='species'))\n    # Create a scatter plot of the data\n    + geom_point()\n    # For some reason the legend gets cut off\n    + theme(subplots_adjust={'right': 0.8})\n    # Label the plot\n    + labs(x = 'Bill Length (mm)', y = 'Body Mass (g)', color = 'Species')\n    # Add a title\n    + ggtitle('Penguin Characteristics by Species')\n)\n</code></pre> <pre><code>&lt;ggplot: (8778956226557)&gt;\n\n/Users/elsa/opt/miniconda3/envs/earth-analytics-python/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 2 rows containing missing values.\n</code></pre> <p></p> <p>GOTCHA ALERT: If you are used to <code>ggplot2</code> and are using <code>plotnine</code>, there are a few things to be aware of. One is that quasiquotation allows you to leave quotes out of your <code>ggplot2</code> code. There is no equivalent in Python - you must use quotes on column names and anything else that isn\u2019t a defined name in your environment!</p> <p>You also must surround the code for plotting with parentheses to get the <code>+</code> syntax for layers to work in Python.</p> <p>Finally, we had some display problems had to adjust the plot theme to see the legend. You may or may not have this problem depending on your coding environment</p> <p>In both cases, we generate some sample data and create a scatter plot to visualize the relationship between the variables.</p> <p>Some notes on plotting packages:</p> <ul> <li> <p>In R, although there is a built-in <code>base</code> plotting functionality, the   <code>ggplot2</code> package is overwhelmingly used for static plotting. <code>gg</code>   stands for Grammar of Graphics, and is an intuitive interface for   making plots that convey the information you want.</p> </li> <li> <p>In Python, there are many options, nearly all of them based on either   the Matlab-inspired <code>matplotlib</code>. <code>matplotlib</code> gives you a lot of   control over your plot, but on the downside you have to control   nearly everything about your plot. We recommend the <code>seaborn.objects</code>   or <code>so</code> interface used above, though it is relatively new, because it   is a flexible and powerful interface that does not require you to   learn any <code>matplotlib</code>. You can also plot with nearly identical syntax   to <code>ggplot2</code> using the <code>plotnine</code> library.</p> </li> <li> <p>For making interactive plots, the go-to libraries are typically based   on Javascript and available in both R and Python. Some popular options   are: <code>plotly</code>, <code>leaflet</code> (R) or <code>folium</code> (Python), and <code>bokeh</code>.</p> </li> </ul>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#piping-and-chaining","title":"Piping and Chaining","text":"<p>Piping is a powerful feature in R that allows for a more streamlined and readable code. In Python, you can code in a similar style using a process called method chaining.</p> <p>GOTCHA ALERT: As the name implies, to use method chaining you need to be using a method of the object you\u2019re working with. There is a .pipe() method of pd.DataFrames that that allows you to use method chaining with functions that you wrote or imported.</p> <p>GOTCHA ALERT: Though code pipelines can be easier to read, they can be harder to debug. If you get an error in an R pipeline, you may not get the information you need about where that error occurred. You can avoid problems by testing your pipelines one line at a time.</p> <p>The syntax for piping is different from method chaining. In R, piping is done using the %&gt;% operator from the magrittr package. In Python, method chaining is done by applying methods to the results of previous methods, using that dot . again, making sure to surround the whole statement with parentheses so you can format it nicely. Both of these operators have the effect of taking whatever object is before them (on the left) and making it the first argument in the next function/method.</p> <p>R code without pipes:</p> Show the R code <pre><code>library(dplyr)\n\n# Create a data frame\ndf &lt;- data.frame(x = c(1,2,3), y = c(4,5,6))\n\n# Calculate the sum of x and y and the take the sum of the new column z\nsummarize(mutate(df, z = x + y), sum_z = sum(z))\n</code></pre> <pre><code>  sum_z\n1    21\n</code></pre> <p>R code with piping:</p> Show the R code <pre><code>library(dplyr)\n\n# Create a data frame\ndf &lt;- data.frame(x = c(1,2,3), y = c(4,5,6))\n\n\ndf %&gt;%\n  # Calculate the vector sum of column x and y\n  mutate(z = x + y) %&gt;%\n  # Calculate the sum of the new column z\n  summarize(sum_z = sum(z))\n</code></pre> <pre><code>  sum_z\n1    21\n</code></pre> <p>Python code without chaining:</p> Show the Python code <pre><code>import pandas as pd\n\n# create a DataFrame\ndf = pd.DataFrame({'x': [1,2,3], 'y': [4,5,6]})\n\n# Calculate the sum of column x and y\ndf_with_sum = df.assign(z = df.x + df.y)\n\n# Calculate the sum of the new column z\ndf_with_sum.agg(sum_z = ('z', 'sum'))\n</code></pre> <pre><code>        z\nsum_z  21\n</code></pre> <p>Python code with chaining:</p> Show the Python code <pre><code>import pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({'x': [1,2,3], 'y': [4,5,6]})\n\n(\n    df\n    # Calculate the sum of column x and y\n    .assign(z = df.x + df.y)\n    # Calculate the sum o the new column z\n    .agg(sum_z = ('z', 'sum'))\n)\n</code></pre> <pre><code>        z\nsum_z  21\n</code></pre> <p>As we can see, the syntax for piping is slightly different between R and Python, but the concept remains the same. Piping can make our code more readable and easier to follow, which is an important aspect of creating efficient and effective code. It avoids obtuse many-layer nested functions and also keeping lots of intermediate processing steps in memory as variables.</p> <p>R code:</p> Show the R code <pre><code>library(dplyr)\nlibrary(ggplot2)\n\ndata(iris)\n\niris %&gt;%\n  filter(Species == \"setosa\") %&gt;%\n  group_by(Sepal.Width) %&gt;%\n  summarise(mean.Petal.Length = mean(Petal.Length)) %&gt;%\n  mutate(Sepal.Width = as.factor(Sepal.Width)) %&gt;%\n  ggplot(aes(x = Sepal.Width, y = mean.Petal.Length)) +\n  geom_bar(stat = \"identity\", fill = \"dodgerblue\") +\n  labs(title = \"Mean Petal Length of Setosa by Sepal Width\",\n       x = \"Sepal Width\",\n       y = \"Mean Petal Length\")\n</code></pre> <p></p> <p>Python code:</p> Show the Python code <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\niris_df = sns.load_dataset('iris')\n\n(\n    iris_df\n    [iris_df.species=='setosa']\n    .groupby('sepal_width')\n    .agg(mean_petal_length=('petal_length', 'mean'))\n    .plot.bar(\n        title='Mean Petal Length of Petals by Sepal Width',\n        xlabel='Sepal Width',\n        y='mean_petal_length', ylabel='Mean Petal Length',\n        legend=False)\n)\nplt.show()\n</code></pre> <p></p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#linear-regression","title":"Linear regression","text":"<p>R Code:</p> Show the R code <pre><code># Load the \"ggplot2\" package for plotting\nlibrary(ggplot2)\n\n# Get some sample data\ndata('faithful')\nfaithful\n</code></pre> <pre><code>    eruptions waiting\n1       3.600      79\n2       1.800      54\n3       3.333      74\n4       2.283      62\n5       4.533      85\n6       2.883      55\n7       4.700      88\n8       3.600      85\n9       1.950      51\n10      4.350      85\n11      1.833      54\n12      3.917      84\n13      4.200      78\n14      1.750      47\n15      4.700      83\n16      2.167      52\n17      1.750      62\n18      4.800      84\n19      1.600      52\n20      4.250      79\n21      1.800      51\n22      1.750      47\n23      3.450      78\n24      3.067      69\n25      4.533      74\n26      3.600      83\n27      1.967      55\n28      4.083      76\n29      3.850      78\n30      4.433      79\n31      4.300      73\n32      4.467      77\n33      3.367      66\n34      4.033      80\n35      3.833      74\n36      2.017      52\n37      1.867      48\n38      4.833      80\n39      1.833      59\n40      4.783      90\n41      4.350      80\n42      1.883      58\n43      4.567      84\n44      1.750      58\n45      4.533      73\n46      3.317      83\n47      3.833      64\n48      2.100      53\n49      4.633      82\n50      2.000      59\n51      4.800      75\n52      4.716      90\n53      1.833      54\n54      4.833      80\n55      1.733      54\n56      4.883      83\n57      3.717      71\n58      1.667      64\n59      4.567      77\n60      4.317      81\n61      2.233      59\n62      4.500      84\n63      1.750      48\n64      4.800      82\n65      1.817      60\n66      4.400      92\n67      4.167      78\n68      4.700      78\n69      2.067      65\n70      4.700      73\n71      4.033      82\n72      1.967      56\n73      4.500      79\n74      4.000      71\n75      1.983      62\n76      5.067      76\n77      2.017      60\n78      4.567      78\n79      3.883      76\n80      3.600      83\n81      4.133      75\n82      4.333      82\n83      4.100      70\n84      2.633      65\n85      4.067      73\n86      4.933      88\n87      3.950      76\n88      4.517      80\n89      2.167      48\n90      4.000      86\n91      2.200      60\n92      4.333      90\n93      1.867      50\n94      4.817      78\n95      1.833      63\n96      4.300      72\n97      4.667      84\n98      3.750      75\n99      1.867      51\n100     4.900      82\n101     2.483      62\n102     4.367      88\n103     2.100      49\n104     4.500      83\n105     4.050      81\n106     1.867      47\n107     4.700      84\n108     1.783      52\n109     4.850      86\n110     3.683      81\n111     4.733      75\n112     2.300      59\n113     4.900      89\n114     4.417      79\n115     1.700      59\n116     4.633      81\n117     2.317      50\n118     4.600      85\n119     1.817      59\n120     4.417      87\n121     2.617      53\n122     4.067      69\n123     4.250      77\n124     1.967      56\n125     4.600      88\n126     3.767      81\n127     1.917      45\n128     4.500      82\n129     2.267      55\n130     4.650      90\n131     1.867      45\n132     4.167      83\n133     2.800      56\n134     4.333      89\n135     1.833      46\n136     4.383      82\n137     1.883      51\n138     4.933      86\n139     2.033      53\n140     3.733      79\n141     4.233      81\n142     2.233      60\n143     4.533      82\n144     4.817      77\n145     4.333      76\n146     1.983      59\n147     4.633      80\n148     2.017      49\n149     5.100      96\n150     1.800      53\n151     5.033      77\n152     4.000      77\n153     2.400      65\n154     4.600      81\n155     3.567      71\n156     4.000      70\n157     4.500      81\n158     4.083      93\n159     1.800      53\n160     3.967      89\n161     2.200      45\n162     4.150      86\n163     2.000      58\n164     3.833      78\n165     3.500      66\n166     4.583      76\n167     2.367      63\n168     5.000      88\n169     1.933      52\n170     4.617      93\n171     1.917      49\n172     2.083      57\n173     4.583      77\n174     3.333      68\n175     4.167      81\n176     4.333      81\n177     4.500      73\n178     2.417      50\n179     4.000      85\n180     4.167      74\n181     1.883      55\n182     4.583      77\n183     4.250      83\n184     3.767      83\n185     2.033      51\n186     4.433      78\n187     4.083      84\n188     1.833      46\n189     4.417      83\n190     2.183      55\n191     4.800      81\n192     1.833      57\n193     4.800      76\n194     4.100      84\n195     3.966      77\n196     4.233      81\n197     3.500      87\n198     4.366      77\n199     2.250      51\n200     4.667      78\n201     2.100      60\n202     4.350      82\n203     4.133      91\n204     1.867      53\n205     4.600      78\n206     1.783      46\n207     4.367      77\n208     3.850      84\n209     1.933      49\n210     4.500      83\n211     2.383      71\n212     4.700      80\n213     1.867      49\n214     3.833      75\n215     3.417      64\n216     4.233      76\n217     2.400      53\n218     4.800      94\n219     2.000      55\n220     4.150      76\n221     1.867      50\n222     4.267      82\n223     1.750      54\n224     4.483      75\n225     4.000      78\n226     4.117      79\n227     4.083      78\n228     4.267      78\n229     3.917      70\n230     4.550      79\n231     4.083      70\n232     2.417      54\n233     4.183      86\n234     2.217      50\n235     4.450      90\n236     1.883      54\n237     1.850      54\n238     4.283      77\n239     3.950      79\n240     2.333      64\n241     4.150      75\n242     2.350      47\n243     4.933      86\n244     2.900      63\n245     4.583      85\n246     3.833      82\n247     2.083      57\n248     4.367      82\n249     2.133      67\n250     4.350      74\n251     2.200      54\n252     4.450      83\n253     3.567      73\n254     4.500      73\n255     4.150      88\n256     3.817      80\n257     3.917      71\n258     4.450      83\n259     2.000      56\n260     4.283      79\n261     4.767      78\n262     4.533      84\n263     1.850      58\n264     4.250      83\n265     1.983      43\n266     2.250      60\n267     4.750      75\n268     4.117      81\n269     2.150      46\n270     4.417      90\n271     1.817      46\n272     4.467      74\n</code></pre> Show the R code <pre><code># Perform linear regression\nmodel &lt;- lm(waiting ~ eruptions, data=faithful)\n\n# Print the model summary\nsummary(model)\n</code></pre> <pre><code>Call:\nlm(formula = waiting ~ eruptions, data = faithful)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.0796  -4.4831   0.2122   3.9246  15.9719\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  33.4744     1.1549   28.98   &lt;2e-16 ***\neruptions    10.7296     0.3148   34.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.914 on 270 degrees of freedom\nMultiple R-squared:  0.8115,    Adjusted R-squared:  0.8108 \nF-statistic:  1162 on 1 and 270 DF,  p-value: &lt; 2.2e-16\n</code></pre> Show the R code <pre><code># Plot the data and regression line\nfaithful %&gt;%\n  ggplot(aes(x = eruptions, y = waiting)) +\n  # Training data\n  geom_point(color='blue') +\n  # Linear regression\n  geom_smooth(method = \"lm\", se = FALSE, color='black') +\n  labs(x = 'Eruption Duration (min.)', y = 'Waiting Time to Next Eruption (min.)')\n</code></pre> <pre><code>`geom_smooth()` using formula = 'y ~ x'\n</code></pre> <p></p> <p>Python code:</p> Show the Python code <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\nimport statsmodels.formula.api as smf\n\nfaithful_df = sns.load_dataset('geyser')\n\n# Perform linear regression\nmodel = smf.ols('waiting ~ duration', data=faithful_df).fit()\n\n# Print the model summary\nprint(model.summary())\n\n# Plot the data with a regression line\n</code></pre> <pre><code>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                waiting   R-squared:                       0.811\nModel:                            OLS   Adj. R-squared:                  0.811\nMethod:                 Least Squares   F-statistic:                     1162.\nDate:                Thu, 18 May 2023   Prob (F-statistic):          8.13e-100\nTime:                        12:06:55   Log-Likelihood:                -868.38\nNo. Observations:                 272   AIC:                             1741.\nDf Residuals:                     270   BIC:                             1748.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     33.4744      1.155     28.985      0.000      31.201      35.748\nduration      10.7296      0.315     34.089      0.000      10.110      11.349\n==============================================================================\nOmnibus:                        5.492   Durbin-Watson:                   2.543\nProb(Omnibus):                  0.064   Jarque-Bera (JB):                4.683\nSkew:                           0.237   Prob(JB):                       0.0962\nKurtosis:                       2.567   Cond. No.                         12.6\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</code></pre> Show the Python code <pre><code>(\n    so.Plot(faithful_df, x='duration', y='waiting')\n    # Training data\n    .add(so.Dots())\n    # Linear regression\n    .add(so.Line(color='black'), so.PolyFit(order=1))\n    .label(x = 'Eruption Duration (min.)', y = 'Waiting Time to Next Eruption (min.)')\n    .show()\n)\n</code></pre> <pre><code>/Users/elsa/opt/miniconda3/envs/earth-analytics-python/lib/python3.8/_collections_abc.py:832: MatplotlibDeprecationWarning: \nThe savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n/Users/elsa/opt/miniconda3/envs/earth-analytics-python/lib/python3.8/_collections_abc.py:832: MatplotlibDeprecationWarning: \nThe savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n</code></pre> <p></p> <p>In both cases, we generate some sample data with a linear relationship between x and y, and then perform a simple linear regression to estimate the slope and intercept of the line. We then plot the data and regression line to visualize the fit.</p> <p>There are a few differences in the syntax and functionality between the two approaches:</p> <ul> <li>Library and package names: In R, we use the <code>lm()</code> function from the   <code>base</code> package to perform linear regression, while in Python, we use   the <code>olm()</code> function from the <code>statsmodels</code> library.</li> </ul>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#random-forest","title":"Random Forest","text":"<p>R Code:</p> Show the R code <pre><code># Load the \"randomForest\" package\nlibrary(randomForest)\n\n# Load the \"iris\" dataset\ndata(iris)\n\n# Split the data into training and testing sets\nset.seed(123)\ntrain_idx &lt;- sample(1:nrow(iris), nrow(iris) * 0.7, replace = FALSE)\ntrain_data &lt;- iris[train_idx, ]\ntest_data &lt;- iris[-train_idx, ]\n\n# Build a random forest model\nrf_model &lt;- randomForest(Species ~ ., data = train_data, ntree = 500)\n\n# Make predictions on the testing set\npredictions &lt;- predict(rf_model, test_data)\n\n# Calculate accuracy of the model\naccuracy &lt;- sum(predictions == test_data$Species) / nrow(test_data)\nprint(paste(\"Accuracy:\", accuracy))\n</code></pre> <pre><code>[1] \"Accuracy: 0.977777777777778\"\n</code></pre> <p>Python code:</p> Show the Python code <pre><code>import pandas as pd\nimport seaborn.objects as so\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# Load the \"iris\" dataset\niris = load_iris()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, test_size=0.3, random_state=123)\n\n# Build a random forest model\nrf_model = RandomForestClassifier(\n    n_estimators=500, random_state=123, oob_score=True)\nrf_model.fit(X_train, y_train)\n\n# Make predictions on the testing set\n</code></pre> <pre><code>RandomForestClassifier(n_estimators=500, oob_score=True, random_state=123)\n</code></pre> Show the Python code <pre><code>predictions = rf_model.predict(X_test)\n\n# OOB score\nprint('OOB score: ', rf_model.oob_score_)\n\n# Accuracy on test data\n</code></pre> <pre><code>OOB score:  0.9428571428571428\n</code></pre> Show the Python code <pre><code>accuracy = sum(predictions == y_test) / len(y_test)\nprint(\"Accuracy:\", accuracy)\n\n# Confusion Matrix\n</code></pre> <pre><code>Accuracy: 0.9555555555555556\n</code></pre> Show the Python code <pre><code>print(confusion_matrix(y_test, predictions))\n\n# Plot feature permutation importances\n</code></pre> <pre><code>[[18  0  0]\n [ 0 10  0]\n [ 0  2 15]]\n</code></pre> Show the Python code <pre><code>importances = permutation_importance(\n    rf_model, X_test, y_test, \n    n_repeats=10, random_state=123)\n(\n    so.Plot(\n        pd.DataFrame(\n            {\n                'importance': importances.importances_mean, \n                'feature': iris.feature_names\n            }\n        ),\n        x='feature',\n        y='importance')\n    .add(so.Bar())\n    .label(x='', y='Permutation Importance')\n    .show()\n)\n</code></pre> <pre><code>/Users/elsa/opt/miniconda3/envs/earth-analytics-python/lib/python3.8/_collections_abc.py:832: MatplotlibDeprecationWarning: \nThe savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n/Users/elsa/opt/miniconda3/envs/earth-analytics-python/lib/python3.8/_collections_abc.py:832: MatplotlibDeprecationWarning: \nThe savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n</code></pre> <p></p> <p>In both cases, we load the iris dataset and split it into training and testing sets. We then build a random forest model using the training data and evaluate its accuracy on the testing data.</p> <p>There are a few differences in the syntax and functionality between the two approaches:</p> <ul> <li>Library and package names: In R, we use the randomForest package to   build random forest models, while in Python, we use the   RandomForestClassifier class from the sklearn.ensemble module. We also   use different libraries for loading and manipulating data (pandas and   numpy in Python, and built-in datasets in R). Model parameters: The   syntax for setting model parameters is slightly different in R and   Python. For example, in R, we specify the number of trees using the   ntree parameter, while in Python, we use the n_estimators parameter.   Data format: In R, we use a data frame to store the input data, while   in Python, we use numpy arrays.</li> </ul>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#basic-streetmap-from-open-street-map","title":"Basic streetmap from Open Street Map","text":"<p>R Code:</p> Show the R code <pre><code># Load the \"osmdata\" package for mapping\nlibrary(osmdata)\nlibrary(tmap)\n\n# Define the map bounding box\nbbox &lt;- c(left = -0.16, bottom = 51.49, right = -0.13, top = 51.51)\n\n# Get the OpenStreetMap data\nosm_data &lt;- opq(bbox) %&gt;% \n  add_osm_feature(key = \"highway\") %&gt;% \n  osmdata_sf()\n\n# Plot the map using tmap\ntm_shape(osm_data$osm_lines) + \n  tm_lines()\n</code></pre> <p></p> <p>Python code:</p> Show the Python code <pre><code># Load the \"osmnx\" package for mapping\nimport osmnx as ox\n\n# Define the map bounding box\nbbox = {'south': 51.49, 'west': -0.16, 'north': 51.51, 'east': -0.13}\n\n# Get the OpenStreetMap data\nosm_graph = ox.graph_from_bbox(**bbox)\n\n# Plot the map using osmnx\nox.plot_graph(osm_graph, node_size=0)\n</code></pre> <pre><code>(&lt;Figure size 800x800 with 0 Axes&gt;, &lt;AxesSubplot:&gt;)\n</code></pre> <p></p> <p>In both cases, we define the map bounding box, retrieve the OpenStreetMap data using the specified bounding box, and plot the map.</p> <p>The main differences between the two approaches are:</p> <ul> <li> <p>Package names and syntax: In R, we use the osmdata package and its   syntax to download and process the OpenStreetMap data, while in   Python, we use the osmnx package and its syntax. We also could have   access the OSM API directly, but these packages make it convenient to   get boundaries and roads with only a few lines of code.</p> </li> <li> <p>Mapping libraries: In R, we use the tmap package to create a static   map of the OpenStreetMap data, while in Python, we use the built-in   ox.plot_graph function from the osmnx package to plot the map.</p> </li> <li> <p>In both Python and R, we define the bounding box as new type of object   (for this notebook). In R, we use a named vector; in Python, we use a   dictionary. There are some (very) subtle differences between the   implementation of these objects, but for now we can treat them as the   same. Using named formats for data like named vectors,   dictionaries, and dataframes help us to write clean and readable code.   For example, in this case each coordinate could be either latitude or   longitude and the names help distinguish that.</p> </li> </ul> <p>GOTCHA ALERT: In Python, asterisks are not only used for multiplication like in R. As in the code above, we can use a single asterisk before a list to unpack it (provide each element separately), or a double asterisk to unpack a dictionary.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#for-loops","title":"for loops","text":"<p>Here is an example of a for loop in R:</p> <p>R code</p> Show the R code <pre><code># Create a vector of numbers\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\n# Use a for loop to print out each number in the vector\nfor (i in numbers) {\n  print(i)\n}\n</code></pre> <pre><code>[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n</code></pre> <p>In this example, the for loop iterates over each element in the numbers vector, assigning the current element to the variable <code>i</code>. The <code>print(i)</code> function is then executed for each iteration, outputting the value of <code>i</code>.</p> <p>Here is the equivalent example in Python:</p> <p>Python code:</p> Show the Python code <pre><code># Create a list of numbers\nnumbers = [1, 2, 3, 4, 5]\n\n# Use a for loop to print out each number in the list\nfor i in numbers:\n    print(i)\n</code></pre> <pre><code>1\n2\n3\n4\n5\n</code></pre> <p>Both languages also support nested for loops, which can be used to perform iterations over multiple dimensions, such as looping through a 2D array.</p> <p>GOTCHA ALERT: In both R and Python, the looping variable (in this case <code>i</code>) is created by the <code>for</code> loop, which is why you can use it even though you didn\u2019t define it. However, <code>i</code> is not deleted after the <code>for</code> loop! Be careful about using looping variables with the same name as other variables in your code, because they can write over each other - we recommend not reusing the same variable within the same notebook as you get started.</p> <p>GOTCHA ALERT: In both Python and R, <code>for</code> loops are not generally the most efficient way of dealing with large arrays. Instead, use built-in vectorized functions in R and the <code>numpy</code> package in Python to take advantage of optimized linear algebra libraries.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#functions","title":"Functions","text":"<p>Both R and Python are powerful languages for writing functions that can take input, perform a specific task, and return output.</p> <p>R Code:</p> Show the R code <pre><code># Define a function that takes two arguments and returns their sum\nsum_r &lt;- function(a, b) {\n  return(a + b)\n}\n\n# Call the function with two arguments and print the result\nresult_r &lt;- sum_r(3, 5)\nprint(result_r)\n</code></pre> <pre><code>[1] 8\n</code></pre> <p>Python code:</p> Show the Python code <pre><code># Define a function that takes two arguments and returns their sum\ndef sum_py(a, b):\n    return a + b\n\n# Call the function with two arguments and print the result\nresult_py = sum_py(3, 5)\nprint(result_py)\n</code></pre> <pre><code>8\n</code></pre> <p>In both cases, we define a function that takes two arguments and returns their sum. In R, we use the function keyword to define a function, while in Python, we use the def keyword. The function body in R is enclosed in curly braces, while in Python it is indented.</p> <p>There are a few differences in the syntax and functionality between the two approaches:</p> <ul> <li> <p>Function arguments: In R, function arguments are separated by commas,   while in Python they are enclosed in parentheses. The syntax for   specifying default arguments and variable-length argument lists can   also differ between the two languages.</p> </li> <li> <p>Return: In R, <code>return()</code> is a function that needs parentheses. In   Python, it is a keyword, and so no parenthesed are used. This type of   syntax in Python is called a statement.</p> </li> <li> <p>Function names: In R there is a convention of using dots <code>.</code> instead   of underscores in function and variable names. This is impossible in   Python because of the nature of the dot <code>.</code> special character in   Python.</p> </li> </ul>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#parallel","title":"Parallel","text":"<p>Parallel computing is a technique used to execute multiple computational tasks simultaneously, which can significantly reduce the time or memory required to complete a task. In earth science, we are more often limited by memory than processing capability (CPUs) because we tend to perform simple operations on lots of spatial data, and parallel computing can help split up a problem so it takes longer but runs with less memory. However, cloud infrastructure with GPUs that are designed for images and ample memory could change the balance there.</p> <p>In general, when we talk about parallel computing, we mean something called embarrassingly parallel problems. Essentially, that means that we can split up our data into roughly equal chunks, perform our operation on all the chunks (which should take roughly the same amount of time), and then recombine \u2013 all without the parallel tasks needing to communicate with each other. There are tools, such as the Message Passing Interface (openmpi) that can be used with either programming language for more complex parallelization tasks.</p> <p>GOTCHA ALERT: It\u2019s a common misconception that parallelizing code can magically speed up code. In reality, using code that takes advantage of (blazingly fast) basic linear algebra libraries can speed up code more than parallelizing a (much) slower method. In addition, creating parallel tasks requires some overhead - time to split up data and start the processes. Be prepared to experiment with the number and types of tasks if you truly want to optimize your code for a particular system. On the other hand - often you can just find a bigger computer, especially if you make friends with the Cyverse folks ;)</p> <p>Both R and Python have built-in support for parallel computing. In this answer, we will introduce you to some options in each language.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#parallel-computing-in-r","title":"Parallel computing in R","text":"<p>In R, there are several packages that support parallel computing, such as parallel, foreach, and doParallel. The parallel package provides basic functionality for parallel computing, while foreach and doParallel provide higher-level abstractions that make it easier to write parallel code.</p> <p>Here is an example of using the <code>foreach</code> and <code>doParallel</code> packages to execute a loop in parallel. Notice that there are two steps here - setting up the cluster with certain parameters (in this case we ask for 4 tasks), and then running a <code>for</code> loop substitute that takes advantage of the cluster.:</p> <p>R code:</p> Show the R code <pre><code>library(foreach)\nlibrary(doParallel)\n</code></pre> <pre><code>Loading required package: iterators\n\nLoading required package: parallel\n</code></pre> Show the R code <pre><code># Define a function to apply in parallel\nmyfunc &lt;- function(x) {\n  # some computation here\n  return(x^2)\n}\n\n# Generate some data\nmydata &lt;- 1:100\n\n# Apply the function in series\nsingle_start &lt;- Sys.time()\nresult = c()\nfor (i in mydata) {\n  #Sys.sleep(.1)\n  append(result, i)\n}\n\n# Calculate rough wall time\nsingle_time &lt;- Sys.time() - single_start\n\n# Time vectorized\nvector_start &lt;- Sys.time()\nresult = lapply(mydata, myfunc)\nvector_time &lt;- Sys.time() - single_start\n\ncluster_start &lt;- Sys.time()\n# Set up a parallel backend with 4 workers\ncl &lt;- makeCluster(4)\nregisterDoParallel(cl)\n\n# Apply the function to the data in parallel\nresult &lt;- foreach(i = mydata) %dopar% {\n  myfunc(i)\n}\n\n# Stop the cluster\nstopCluster(cl)\n\n# Calculate rough wall time\ncluster_time &lt;- Sys.time() - cluster_start\n\nprint(paste('Time without cluster: ', single_time))\n</code></pre> <pre><code>[1] \"Time without cluster:  0.014976978302002\"\n</code></pre> Show the R code <pre><code>print(paste('Time vectorized: ', vector_time))\n</code></pre> <pre><code>[1] \"Time vectorized:  0.0239660739898682\"\n</code></pre> Show the R code <pre><code>print(paste('Time with cluster: ', cluster_time))\n</code></pre> <pre><code>[1] \"Time with cluster:  0.632711172103882\"\n</code></pre> <p>Oops! Our parallel version takes more time. Let\u2019s try making our function take longer, to simulate what happens in a more complex situation. Uncomment the <code>Sys.sleep(.1)</code> line in <code>myfunc</code> above and run again. In this case, the vectorized version is slightly slower that the <code>for</code> loop, but that is probably not universally true!</p> <p>GOTCHA ALERT: This <code>foreach</code> method creates a new task for every number. Go ahead and try increasing to 100000 numbers. That\u2019s not a very efficient way to work with large lists because of the overhead required to make each process. We\u2019d rather use a technique that would split the numbers into larger chunks.</p> <p>In this example, we:</p> <ol> <li>use the makeCluster() function to set up a cluster with 4 workers</li> <li>registerDoParallel() function to register the cluster as the     parallel backend for foreach</li> <li>define a function myfunc() that takes an input x and returns x^2.</li> <li>generate some data</li> <li>use foreach to apply myfunc() to each element of mydata in parallel,     using the %dopar% operator to tell R to use the cluster we set up     earlier.</li> <li>Close the cluster - this prevents the code from continuing to use     unnecessary overhead</li> </ol>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#r-tidyverse-parallel","title":"R Tidyverse parallel","text":"<p>In R Tidyverse, we can use the furrr package for parallel computing. Here\u2019s an example of using furrr to parallelize a map function:</p> <p>R Tidy code:</p> Show the R code <pre><code>library(tidyverse)\nlibrary(furrr)\n\n# Generate a list of numbers\nnumbers &lt;- 1:100\n\n# Use the future_map function from furrr to parallelize the map function\nstart &lt;- Sys.time()\nplan(multisession)\nsquares &lt;- future_map(numbers, function(x) x^2)\nprint(paste('Run time: ', Sys.time() - start))\n</code></pre> <pre><code>[1] \"Run time:  4.94922685623169\"\n</code></pre> <p>In this example, we first load the Tidyverse and furrr libraries. We then generate a list of numbers from 1 to 10. We then use the plan function to set the parallelization strategy to \u201cmultisession\u201d, which will use multiple CPU cores to execute the code. Finally, we use the future_map function from furrr to apply the function x^2 to each number in the list in parallel.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#parallel-computing-in-python","title":"Parallel computing in Python","text":"<p>In Python, the standard library includes the <code>multiprocessing</code> module and the <code>threading</code> module, which provides basic support for parallel computing. By convention, these modules have matching classes like <code>Pool</code> below which are imported individually. This allows you to switch between threading (sharing memory resources) and multiprocessing (pre-allocated memory resources) without needing to change your code. This choice is beyond the scope of this workshop, but in both directions can be a cause of differences in performance.</p> <p>GOTCHA ALERT: Python can give you more control over your multitasking. That also means you have more power to mess things up. One infamous problem you can run into is called the GIL or Global Interpreter Lock. This happens when multiple threads try to run code at the same time (processes don\u2019t have this constraint) and Python won\u2019t let them. This is not a problem when your code is memory-limited, but can definitely be a problem with CPU-limited code. You can also run into problems with file access and the GIL.</p> <p>Here is an example of using the multiprocessing module to execute a loop in parallel:</p> <p>Python code:</p> Show the Python code <pre><code>import time\nfrom multiprocessing import Pool\n\n# Generate a list of numbers\nnumbers = list(range(1, 1000001))\n\n# Use a for loop\nfor_start = time.time()\n\nsquares = []\nfor n in numbers:\n    squares.append(n**2)\n\nprint('Run time with for loop: {}'.format(time.time() - for_start))\n\n# Use the map function and a pool of workers to parallelize the square function\n</code></pre> <pre><code>Run time with for loop: 0.4019489288330078\n</code></pre> Show the Python code <pre><code>start = time.time()\nwith Pool(4) as pool:\n    squares = pool.apply_async(pow, numbers, {'exp': 2})\n\nprint('Run time with Pool: {}'.format(time.time() - start))\n</code></pre> <pre><code>Run time with Pool: 0.2741971015930176\n</code></pre> <p>Like R, if we used fewer numbers we don\u2019t necessarily see and advantage to parallelizing.</p> <p>In this example, we define a function myfunc() that takes an input x and returns x^2. We generate some data mydata and use the Pool class from the multiprocessing module to set up a pool of 4 workers. We then use the map() method of the Pool class to apply myfunc() to each element of mydata in parallel.</p> <p>GOTCHA ALERT: When multiprocessing in Python, you sometimes can\u2019t use your own function unless you have imported it from another file. Go figure.</p> <p>Another option in Python is the <code>dask</code> library. <code>dask</code> is a sophisticated library for working with larger-than-memory datasets on a variety of different types of systems from personal computers to high-performance clusters, and is (some of us believe) by itself a great reason to use Python if certain types of multiprocessing are needed. We don\u2019t have an example here because <code>dask</code> is designed to work with objects like <code>pandas</code> <code>DataFrames</code> and <code>xarray</code> <code>DataArrays</code>. You can check out a range of examples on the dask documentation page.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#data-wrangling","title":"Data wrangling","text":"<p>Data wrangling is an important part of any data analysis project, and both R and Python provide tools and libraries for performing this task. Data wrangling can include:</p> <ul> <li> <p>Filtering data to study parameters</p> </li> <li> <p>Ensuring that nodata values are interpreted correctly</p> </li> <li> <p>Ensuring that data has the correct number and type of dimensions</p> </li> <li> <p>Converting units</p> </li> <li> <p>Reprojecting spatial coordinates</p> </li> <li> <p>Parsing dates and times</p> </li> <li> <p>Pivoting - Changing between long (or tidy) and wide data formats.   Tidy data contains only a single observation in each row. Different   statistical models require different data formats.</p> </li> <li> <p>Joining - combining multiple datasets on common column values or   key</p> </li> </ul>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#data-wrangling-in-r-tidyverse","title":"Data Wrangling in R Tidyverse","text":"<p>The tidyverse is a collection of R packages designed for data science, and it includes several packages that are useful for data wrangling. One of the most popular packages is dplyr, which provides a grammar of data manipulation for data frames.</p> <p>Here is an example of using dplyr to filter, mutate, and summarize a data frame:</p> <p>R code</p> Show the R code <pre><code>library(dplyr)\n</code></pre> <pre><code>Attaching package: 'dplyr'\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n</code></pre> Show the R code <pre><code># Load data\ndata(mtcars)\n\n\nmtcars %&gt;%\n  # Filter for cars with more than 100 horsepower\n  filter(hp &gt; 100) %&gt;%\n  # Add a new column with fuel efficiency in km per liter\n  mutate(kmpl = 0.425 * mpg) %&gt;%\n  # Group by number of cylinders and summarize\n  group_by(cyl) %&gt;%\n  summarize(mean_hp = mean(hp),\n            mean_kmpl = mean(kmpl))\n</code></pre> <pre><code># A tibble: 3 \u00d7 3\n    cyl mean_hp mean_kmpl\n  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1     4    111      11.0 \n2     6    122.      8.39\n3     8    209.      6.42\n</code></pre> <p>In this example, we first filter the mtcars data frame to only include cars with more than 100 horsepower. We then use mutate to create a new column with fuel efficiency in kilometers per liter. Finally, we group the data by the number of cylinders and calculate the mean horsepower and fuel efficiency.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#data-wrangling-in-python-pandas","title":"Data Wrangling in Python Pandas","text":"<p>Pandas is a popular library for data manipulation in Python. It provides a data frame object similar to R\u2019s data frames, along with a wide range of functions for data wrangling. The code below performs the same operations as the R code above:</p> Show the Python code <pre><code>(\n    r.mtcars\n    # Filter for cars with more than 100 horsepower\n    [r.mtcars.hp&gt;100]\n    # Add a new column with fuel efficiency in km per liter\n    .assign(kmpl=0.425*r.mtcars.mpg)\n    # Group by number of cylinders and summarize\n    .groupby('cyl')\n    .agg(mean_hp=('hp', 'mean'), \n         mean_kmpl=('kmpl', 'mean'))\n)\n</code></pre> <pre><code>        mean_hp  mean_kmpl\ncyl                       \n4.0  111.000000  11.007500\n6.0  122.285714   8.390714\n8.0  209.214286   6.417500\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#data-from-api","title":"Data from API","text":"<p>An Application Programming Interface (API) is a method of accessing data or services using automated code. When we talk about data APIs, we are usually referring to web APIs which are accessed over HTTP(S) using URLs (just like you put in your web browser).</p> <p>To decipher API documentation, you need to know a little about URLs. Take a look at the following graphic from mdn webdocs:</p> <p></p> <p>For an API, the main part you need to pay attention to is the parameters - these are going to tell the data provider which data we want to download.</p> <p>Most APIs require that you sign up for an API key or developer key. This allows them to track who is downloading the data and make sure than no one user is putting undue stress on their system. YOU SHOULD NOT PUBLISH YOUR API KEYS. Even if the data is open, there are web-scraping bots out there that can attack data providers, for example by making so many requests that the service can\u2019t respond to anyone (denial-of-service attack). Some data providers will discontinue access for you if your API key is used in an attack - even if it was inadvertent. We recommend storing your API key in a file in your home folder and importing it. You can also use the GitHub Secrets feature to store an API key without making it accessible on the open web.</p> <p>Retrieving data from an API is a common task in both R and Python. Sometimes, data managers provide a library for getting data, like the OpenStreetMap data earlier, which takes care of URL formatting for us. Other times, we must determine the correct HTTP(S) URL for the data using the API documentation. The examples below will show how to download data using API URLs in R and Python.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#openweathermap","title":"OpenWeatherMap","text":""},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#r","title":"R","text":"<p>In R, we can use the httr package to retrieve data from an API. Here\u2019s an example of how to retrieve weather data from the OpenWeatherMap API in R. You will need to replace <code>API_KEY</code> with an API key from the OpenWeatherMap site:</p> <p>R code:</p> Show the R code <pre><code>library(httr)\nlibrary(jsonlite)\n\n# Download\nurl &lt;- 'https://api.openweathermap.org/data/2.5/weather?q=London,uk&amp;appid=API_KEY'\n\nresponse &lt;- GET(url)\n\ndata &lt;- content(response, 'text')\n\n# Convert content to JSON\njson &lt;- fromJSON(content)\n\n# Convert JSON to a data frame\ndf &lt;- as.data.frame(json)\n\nprint(data)\n</code></pre> <p>We first load the httr library, then construct the API URL and use the GET() function to make a request to the API. We then extract the data from the response using the content() function and print the resulting data.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#python","title":"Python","text":"<p>To retrieve data from an API in Python, we can use the requests library. Here\u2019s an example of how to retrieve weather data from the OpenWeatherMap API.</p> <p>Python code:</p> Show the Python code <pre><code>import pandas as pd\nimport requests\n\nurl = 'https://api.openweathermap.org/data/2.5/weather?q=London,uk&amp;appid=API_KEY'\n\nresponse = requests.get(url)\n\njson_data = response.json()\n\npd.read_json(json_data)\n\nprint(data)\n</code></pre> <p>This code retrieves the current weather data for London from the OpenWeatherMap API. We first construct the API URL with the location and API key, then use the requests.get() function to make a request to the API. We then extract the JSON data from the response using the .json() method and print the resulting data.</p> <p>Retrieving Data from an API in R Tidyverse In R Tidyverse, we can use the httr and jsonlite packages to retrieve and process data from an API.</p> <p>Comparison</p> <ul> <li>Both R Tidyverse and Python provide powerful tools for retrieving and   processing data from an API. In terms of syntax, the two languages are   somewhat similar. In both cases, we use a library to retrieve data   from the API, extract the content of the response, and then process   the JSON data. However, there are some differences in the specific   functions and methods used. For example, in R Tidyverse, we use the   content() function to extract the content of the response, whereas in   Python, we use the content attribute. Additionally, in R Tidyverse, we   use the fromJSON() function to convert the JSON data to a list,   whereas in Python, we use the loads() function.</li> </ul>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#census-data","title":"Census data","text":"<p>Retrieving USA census data in R, R Tidy, and Python can be done using different packages and libraries. Here are some working examples in code for each language:</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#r_1","title":"R:","text":"<p>To retrieve census data in R, we can use the tidycensus package. Here\u2019s an example of how to retrieve the total population for the state of California:</p> <p>R code:</p> Show the R code <pre><code>library(tidycensus)\nlibrary(tidyverse)\n\n# Set your Census API key\ncensus_api_key(\"your_api_key\")\n\n# Get the total population for the state of California\nca_pop &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01003_001\",\n  state = \"CA\"\n) %&gt;% \n  rename(total_population = estimate) %&gt;% \n  select(total_population)\n\n# View the result\nca_pop\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#python_1","title":"Python:","text":"<p>To retrieve census data in Python, we can use the census library. Here\u2019s an example of how to retrieve the total population for the state of California:</p> <p>Python code:</p> Show the Python code <pre><code>from census import Census\nfrom us import states\nimport pandas as pd\n\n# Set your Census API key\nc = Census(\"your_api_key\")\n\n# Get the total population for the state of California\nca_pop = c.acs5.state((\"B01003_001\"), states.CA.fips, year=2019)\n\n# Convert the result to a Pandas DataFrame\nca_pop_df = pd.DataFrame(ca_pop)\n\n# Rename the column\nca_pop_df = ca_pop_df.rename(columns={\"B01003_001E\": \"total_population\"})\n\n# Select only the total population column\nca_pop_df = ca_pop_df[[\"total_population\"]]\n\n# View the result\nca_pop_df\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#lidar-data","title":"Lidar data","text":"<p>To find Lidar data in R and Python, you typically need to start by identifying sources of Lidar data and then accessing them using appropriate packages and functions. Here are some examples of how to find Lidar data in R and Python:</p> <p>R:</p> <p>Identify sources of Lidar data:</p> <p>The USGS National Map Viewer provides access to Lidar data for the United States. You can also find Lidar data on state and local government websites, as well as on commercial data providers\u2019 websites. Access the data: You can use the lidR package in R to download and read Lidar data in the LAS format. For example, the following code downloads and reads Lidar data for a specific area:</p> <p>R code:</p> Show the R code <pre><code>library(lidR)\n\n# Download Lidar data\nLASfile &lt;- system.file(\"extdata\", \"Megaplot.laz\", package=\"lidR\")\nlidar &lt;- readLAS(LASfile)\n\n# Visualize the data\nplot(lidar)\n</code></pre> <p>Python:</p> <p>Identify sources of Lidar data: The USGS 3DEP program provides access to Lidar data for the United States. You can also find Lidar data on state and local government websites, as well as on commercial data providers\u2019 websites. Access the data: You can use the pylastools package in Python to download and read Lidar data in the LAS format. For example, the following code downloads and reads Lidar data for a specific area:</p> <p>Python code:</p> <pre><code>py_install(\"requests\")\npy_install(\"pylas\")\npy_install(\"laspy\")\n</code></pre> Show the Python code <pre><code>import zipfile\n\nimport requests\nfrom pylas import read\nimport laspy\nimport numpy as np\n\n# Download Lidar data\nurl = (\"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/\"\n       \"USGS_LPC_CA_SanFrancisco_2016_LAS_2018.zip\")\n\n# Unzip\nwith\n\nlasfile = \"USGS_LPC_CA_SanFrancisco_2016_LAS_2018.las\"\nr = requests.get(url, allow_redirects=True)\n\n\nopen(lasfile, 'wb').write(r.content)\n\n# Read the data\nlidar = read(lasfile)\n\n# Visualize the data\nlaspy.plot.plot(lidar)\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#data-for-black-lives","title":"Data for black lives","text":"<p>Data for Black Lives (https://d4bl.org/) is a movement that uses data science to create measurable change in the lives of Black people. While the Data for Black Lives website provides resources, reports, articles, and datasets related to racial equity, it doesn\u2019t provide a direct API for downloading data.</p> <p>Instead, you can access the Data for Black Lives GitHub repository (https://github.com/Data4BlackLives) to find datasets and resources to work with. In this example, we\u2019ll use a sample dataset available at https://github.com/Data4BlackLives/covid-19/tree/master/data. The dataset \u201cCOVID19_race_data.csv\u201d contains COVID-19 race-related data.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#r_2","title":"R:","text":"<p>In R, we\u2019ll use the \u2018readr\u2019 and \u2018dplyr\u2019 packages to read, process, and analyze the dataset.</p> <p>R code:</p> Show the R code <pre><code># Install and load necessary libraries\n\nlibrary(readr)\nlibrary(dplyr)\n\n# Read the CSV file\nurl &lt;- \"https://raw.githubusercontent.com/Data4BlackLives/covid-19/master/data/COVID19_race_data.csv\"\ndata &lt;- read_csv(url)\n\n# Basic information about the dataset\nprint(dim(data))\nprint(head(data))\n\n# Example analysis: calculate the mean of 'cases_total' by 'state'\ndata %&gt;%\n  group_by(state) %&gt;%\n  summarize(mean_cases_total = mean(cases_total, na.rm = TRUE)) %&gt;%\n  arrange(desc(mean_cases_total))\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#python_2","title":"Python:","text":"<p>In Python, we\u2019ll use the \u2018pandas\u2019 library to read, process, and analyze the dataset.</p> <p>Python code:</p> Show the Python code <pre><code>import pandas as pd\n\n# Read the CSV file\nurl = \"https://raw.githubusercontent.com/Data4BlackLives/covid-19/master/data/COVID19_race_data.csv\"\ndata = pd.read_csv(url)\n\n# Basic information about the dataset\nprint(data.shape)\nprint(data.head())\n\n# Example analysis: calculate the mean of 'cases_total' by 'state'\nmean_cases_total = data.groupby(\"state\")[\"cases_total\"].mean().sort_values(ascending=False)\nprint(mean_cases_total)\n</code></pre> <p>In conclusion, both R and Python provide powerful libraries and tools for downloading, processing, and analyzing datasets, such as those found in the Data for Black Lives repository. The \u2018readr\u2019 and \u2018dplyr\u2019 libraries in R offer a simple and intuitive way to read and manipulate data, while the \u2018pandas\u2019 library in Python offers similar functionality with a different syntax. Depending on your preferred programming language and environment, both options can be effective in working with social justice datasets.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#propublica-congress-api","title":"Propublica Congress API","text":"<p>The ProPublica Congress API provides information about the U.S. Congress members and their voting records. In this example, we\u2019ll fetch data about the current Senate members and calculate the number of members in each party.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#r_3","title":"R:","text":"<p>In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the ProPublica Congress API.</p> <p>R code:</p> Show the R code <pre><code># load necessary libraries\nlibrary(httr)\nlibrary(jsonlite)\n\n# Replace 'your_api_key' with your ProPublica API key\napi_key &lt;- 'your_api_key'\n\n# Fetch data about the current Senate members\nurl &lt;- \"https://api.propublica.org/congress/v1/117/senate/members.json\"\nresponse &lt;- GET(url, add_headers(`X-API-Key` = api_key))\n\n# Check if the request was successful\nif (http_status(response)$category == \"Success\") {\n  data &lt;- content(response, \"parsed\")\n  members &lt;- data$results[[1]]$members\n\n  # Calculate the number of members in each party\n  party_counts &lt;- table(sapply(members, function(x) x$party))\n  print(party_counts)\n} else {\n  print(http_status(response)$message)\n}\n</code></pre> <pre><code>[1] \"Server error: (500) Internal Server Error\"\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#python_3","title":"Python","text":"<p>In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the ProPublica Congress API and \u2018pandas\u2019 library to process the data.</p> <p>python code:</p> Show the Python code <pre><code># Install necessary libraries\n\nimport requests\nimport pandas as pd\n\n# Replace 'your_api_key' with your ProPublica API key\napi_key = \"your_api_key\"\nheaders = {\"X-API-Key\": api_key}\n\n# Fetch data about the current Senate members\nurl = \"https://api.propublica.org/congress/v1/117/senate/members.json\"\nresponse = requests.get(url, headers=headers)\n\n# Check if the request was successful\nif response.status_code == 200:\n    data = response.json()\n    members = data[\"results\"][0][\"members\"]\n\n    # Calculate the number of members in each party\n    party_counts = pd.DataFrame(members)[\"party\"].value_counts()\n    print(party_counts)\nelse:\n    print(f\"Error: {response.status_code}\")\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the ProPublica Congress API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like table() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the ProPublica Congress API.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#nonprofit-explorer-api-by-propublica","title":"Nonprofit Explorer API by ProPublica","text":"<p>The Nonprofit Explorer API by ProPublica provides data on tax-exempt organizations in the United States. In this example, we\u2019ll search for organizations with the keyword \u201ceducation\u201d and analyze the results.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#r_4","title":"R","text":"<p>In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the Nonprofit Explorer API.</p> <p>R code:</p> Show the R code <pre><code># Install and load necessary libraries\nlibrary(httr)\nlibrary(jsonlite)\n\n# Fetch data for organizations with the keyword \"education\"\nurl &lt;- \"https://projects.propublica.org/nonprofits/api/v2/search.json?q=education\"\nresponse &lt;- GET(url)\n\n# Check if the request was successful\nif (http_status(response)$category == \"Success\") {\n  data &lt;- content(response, \"parsed\")\n  organizations &lt;- data$organizations\n\n  # Count the number of organizations per state\n  state_counts &lt;- table(sapply(organizations, function(x) x$state))\n  print(state_counts)\n} else {\n  print(http_status(response)$message)\n}\n</code></pre> <pre><code>     AZ      CA      CO      DC      FL      GA      HI      IA      ID      IL \n      3      19       6       2       4       1       1       1       1       5 \nIndiana      LA      MA      MD      MI      MN      MO      MP      MS      NC \n      1       2       1       1       2       5       3       1       2       2 \n     ND      NE      NJ      NY      OH  Oregon      PA      TX      UT      VA \n      1       1       3       1       6       1       2      13       2       3 \n     WA      ZZ \n      2       2\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#python_4","title":"Python","text":"<p>In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the Nonprofit Explorer API and \u2018pandas\u2019 library to process the data.</p> <p>Python code:</p> Show the Python code <pre><code># Install necessary libraries\nimport requests\nimport pandas as pd\n\n# Fetch data for organizations with the keyword \"education\"\nurl = \"https://projects.propublica.org/nonprofits/api/v2/search.json?q=education\"\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    data = response.json()\n    organizations = data[\"organizations\"]\n\n    # Count the number of organizations per state\n    state_counts = pd.DataFrame(organizations)[\"state\"].value_counts()\n    print(state_counts)\nelse:\n    print(f\"Error: {response.status_code}\")\n</code></pre> <pre><code>CA         19\nTX         13\nOH          6\nCO          6\nIL          5\nMN          5\nFL          4\nMO          3\nNJ          3\nVA          3\nAZ          3\nDC          2\nMS          2\nWA          2\nMI          2\nUT          2\nNC          2\nLA          2\nPA          2\nZZ          2\nIndiana     1\nNE          1\nNY          1\nOregon      1\nHI          1\nGA          1\nMP          1\nMD          1\nIA          1\nID          1\nND          1\nMA          1\nName: state, dtype: int64\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the Nonprofit Explorer API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like table() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the Nonprofit Explorer API.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#campaign-finance-api-by-propublica","title":"Campaign Finance API by ProPublica","text":"<p>The Campaign Finance API by the Federal Election Commission (FEC) provides data on campaign finance in U.S. federal elections. In this example, we\u2019ll fetch data about individual contributions for the 2020 election cycle and analyze the results.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#r_5","title":"R","text":"<p>In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the Campaign Finance API.</p> <p>R code:</p> Show the R code <pre><code># Install and load necessary libraries\nlibrary(httr)\nlibrary(jsonlite)\n\n# Fetch data about individual contributions for the 2020 election cycle\nurl &lt;- \"https://api.open.fec.gov/v1/schedules/schedule_a/?api_key='OGwpkX7tH5Jihs1qQcisKfVAMddJzmzouWKtKoby'&amp;two_year_transaction_period=2020&amp;sort_hide_null=false&amp;sort_null_only=false&amp;per_page=20&amp;page=1\"\nresponse &lt;- GET(url)\n\n# Check if the request was successful\nif (http_status(response)$category == \"Success\") {\n  data &lt;- content(response, \"parsed\")\n  contributions &lt;- data$results\n\n  # Calculate the total contributions per state\n  state_totals &lt;- aggregate(contributions$contributor_state, by = list(contributions$contributor_state), FUN = sum)\n  colnames(state_totals) &lt;- c(\"State\", \"Total_Contributions\")\n  print(state_totals)\n} else {\n  print(http_status(response)$message)\n}\n</code></pre> <pre><code>[1] \"Client error: (403) Forbidden\"\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#python_5","title":"Python","text":"<p>In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the Campaign Finance API and \u2018pandas\u2019 library to process the data.</p> <p>Python code:</p> Show the Python code <pre><code># Install necessary libraries\n\nimport requests\nimport pandas as pd\n\n# Fetch data about individual contributions for the 2020 election cycle\nurl = \"https://api.open.fec.gov/v1/schedules/schedule_a/?api_key=your_api_key&amp;two_year_transaction_period=2020&amp;sort_hide_null=false&amp;sort_null_only=false&amp;per_page=20&amp;page=1\"\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    data = response.json()\n    contributions = data[\"results\"]\n\n    # Calculate the total contributions per state\n    df = pd.DataFrame(contributions)\n    state_totals = df.groupby(\"contributor_state\")[\"contribution_receipt_amount\"].sum()\n    print(state_totals)\nelse:\n    print(f\"Error: {response.status_code}\")\n</code></pre> <pre><code>Error: 403\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the Campaign Finance API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like aggregate() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the Campaign Finance API.</p> <p>Note: Remember to replace your_api_key with your actual FEC API key in the code examples above.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#historic-redlining","title":"Historic Redlining","text":"<p>Historic redlining data refers to data from the Home Owners\u2019 Loan Corporation (HOLC) that created residential security maps in the 1930s, which contributed to racial segregation and disinvestment in minority neighborhoods. One popular source for this data is the Mapping Inequality project (https://dsl.richmond.edu/panorama/redlining/).</p> <p>In this example, we\u2019ll download historic redlining data for Philadelphia in the form of a GeoJSON file and analyze the data in R and Python.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#r_6","title":"R","text":"<p>In R, we\u2019ll use the \u2018sf\u2019 and \u2018dplyr\u2019 packages to read and process the GeoJSON data.</p> <p>R code:</p> Show the R code <pre><code># Install and load necessary libraries\nlibrary(sf)\nlibrary(dplyr)\n\n# Download historic redlining data for Philadelphia\nurl &lt;- \"https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/PAPhiladelphia1937.geojson\"\nphilly_geojson &lt;- read_sf(url)\n\n# Count the number of areas per HOLC grade\ngrade_counts &lt;- philly_geojson %&gt;%\n  group_by(holc_grade) %&gt;%\n  summarize(count = n())\n\nplot(grade_counts)\n</code></pre> <p></p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#python_6","title":"Python","text":"<p>In Python, we\u2019ll use the \u2018geopandas\u2019 library to read and process the GeoJSON data.</p> <p>Python code:</p> Show the Python code <pre><code># Install necessary libraries\n\n\nimport geopandas as gpd\n\n# Download historic redlining data for Philadelphia\nurl = \"https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/PAPhiladelphia1937.geojson\"\nphilly_geojson = gpd.read_file(url)\n\n# Count the number of areas per HOLC grade\ngrade_counts = philly_geojson[\"holc_grade\"].value_counts()\nprint(grade_counts)\n</code></pre> <pre><code>B    28\nD    26\nC    18\nA    10\nName: holc_grade, dtype: int64\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to download and process historic redlining data in the form of GeoJSON files. The \u2018sf\u2019 package in R provides a simple way to read and manipulate spatial data, while the \u2018geopandas\u2019 library in Python offers similar functionality. The \u2018dplyr\u2019 package in R can be used for data manipulation and analysis, and Python\u2019s built-in functions like value_counts() can be used for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with historic redlining data.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#indian-entities-recognized-and-eligible-to-receive-services-by-bia","title":"Indian Entities Recognized and Eligible To Receive Services by BIA","text":"<p>The Bureau of Indian Affairs (BIA) provides a PDF document containing a list of Indian Entities Recognized and Eligible To Receive Services. To analyze the data, we\u2019ll first need to extract the information from the PDF. In this example, we\u2019ll extract the names of the recognized tribes and count the number of tribes per state.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#r_7","title":"R","text":"<p>In R, we\u2019ll use the \u2018pdftools\u2019 package to extract text from the PDF and the \u2018stringr\u2019 package to process the text data.</p> <p>R code:</p> Show the R code <pre><code># Install and load necessary libraries\nlibrary(pdftools)\nlibrary(stringr)\nlibrary(dplyr)\n\n# Download the BIA PDF\nurl &lt;- \"https://www.govinfo.gov/content/pkg/FR-2022-01-28/pdf/2022-01789.pdf\"\ntemp_file &lt;- tempfile(fileext = \".pdf\")\ndownload.file(url, temp_file, mode = \"wb\")\n\n# Extract text from the PDF\npdf_text &lt;- pdf_text(temp_file)\ntribe_text &lt;- pdf_text[4:length(pdf_text)]\n\n# Define helper functions\ntribe_state_extractor &lt;- function(text_line) {\n  regex_pattern &lt;- \"(.*),\\\\s+([A-Z]{2})$\"\n  tribe_state &lt;- str_match(text_line, regex_pattern)\n  return(tribe_state)\n}\n\nis_valid_tribe_line &lt;- function(text_line) {\n  regex_pattern &lt;- \"^\\\\d+\\\\s+\"\n  return(!is.na(str_match(text_line, regex_pattern)))\n}\n\n# Process text data to extract tribes and states\ntribe_states &lt;- sapply(tribe_text, tribe_state_extractor)\nvalid_lines &lt;- sapply(tribe_text, is_valid_tribe_line)\ntribe_states &lt;- tribe_states[valid_lines, 2:3]\n\n# Count the number of tribes per state\ntribe_data &lt;- as.data.frame(tribe_states)\ncolnames(tribe_data) &lt;- c(\"Tribe\", \"State\")\nstate_counts &lt;- tribe_data %&gt;%\n  group_by(State) %&gt;%\n  summarise(Count = n())\n\nprint(state_counts)\n</code></pre> <pre><code># A tibble: 0 \u00d7 2\n# \u2139 2 variables: State &lt;chr&gt;, Count &lt;int&gt;\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#python_7","title":"Python","text":"<p>In Python, we\u2019ll use the \u2018PyPDF2\u2019 library to extract text from the PDF and the \u2018re\u2019 module to process the text data.</p> <p>Python code:</p> Show the Python code <pre><code># Install necessary libraries\nimport requests\nimport PyPDF2\nimport io\nimport re\nfrom collections import Counter\n\n# Download the BIA PDF\nurl = \"https://www.bia.gov/sites/bia.gov/files/assets/public/raca/online-tribal-leaders-directory/tribal_leaders_2021-12-27.pdf\"\nresponse = requests.get(url)\n\n# Extract text from the PDF\npdf_reader = PyPDF2.PdfFileReader(io.BytesIO(response.content))\ntribe_text = [pdf_reader.getPage(i).extractText() for i in range(3, pdf_reader.numPages)]\n\n# Process text data to extract tribes and states\ntribes = [re.findall(r'^\\d+\\s+(.+),\\s+([A-Z]{2})', line) for text in tribe_text for line in text.split('\\n') if line]\ntribe_states = [state for tribe, state in tribes]\n\n# Count the number of tribes per state\nstate_counts = Counter(tribe_states)\nprint(state_counts)\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to download and process the list of Indian Entities Recognized and Eligible To Receive Services from the BIA. The \u2018pdftools\u2019 package in R provides a simple way to extract text from PDF files, while the \u2018PyPDF2\u2019 library in Python offers similar functionality. The \u2018stringr\u2019 package in R and the \u2018re\u2019 module in Python can be used to process and analyze text data. Depending on your preferred programming language and environment, both options can be effective for working with BIA data.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#national-atlas-indian-lands-of-the-united-states-dataset","title":"National Atlas - Indian Lands of the United States dataset","text":"<p>In this example, we will download and analyze the National Atlas - Indian Lands of the United States dataset in both R and Python. We will read the dataset and count the number of Indian lands per state.</p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#r_8","title":"R","text":"<p>In R, we\u2019ll use the \u2018sf\u2019 package to read the Shapefile and the \u2018dplyr\u2019 package to process the data.</p> <p>R code:</p> Show the R code <pre><code># Install and load necessary libraries\n\nlibrary(sf)\nlibrary(dplyr)\n\n# Download the Indian Lands dataset\nurl &lt;- \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/indlanp010g.shp_nt00968.tar.gz\"\ntemp_file &lt;- tempfile(fileext = \".tar.gz\")\ndownload.file(url, temp_file, mode = \"wb\")\nuntar(temp_file, exdir = tempdir())\n\n# Read the Shapefile\nshapefile_path &lt;- file.path(tempdir(), \"indlanp010g.shp\")\nindian_lands &lt;- read_sf(shapefile_path)\n\n# Count the number of Indian lands per state\n# state_counts &lt;- indian_lands %&gt;%\n#   group_by(STATE) %&gt;%\n#   summarize(count = n())\n\nplot(indian_lands)\n</code></pre> <pre><code>Warning: plotting the first 9 out of 23 attributes; use max.plot = 23 to plot\nall\n</code></pre> <p></p>"},{"location":"2_R_and_Py_bilingualism/code/code_demo/bilingualism/#python_8","title":"Python","text":"<p>In Python, we\u2019ll use the \u2018geopandas\u2019 and \u2018pandas\u2019 libraries to read the Shapefile and process the data.</p> <p>Python code:</p> Show the Python code <pre><code>import geopandas as gpd\nimport pandas as pd\nimport requests\nimport tarfile\nimport os\nfrom io import BytesIO\n\n# Download the Indian Lands dataset\nurl = \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/indlanp010g.shp_nt00966.tar.gz\"\nresponse = requests.get(url)\ntar_file = tarfile.open(fileobj=BytesIO(response.content), mode='r:gz')\n\n# Extract Shapefile\ntemp_dir = \"temp\"\nif not os.path.exists(temp_dir):\n    os.makedirs(temp_dir)\n\ntar_file.extractall(path=temp_dir)\nshapefile_path = os.path.join(temp_dir, \"indlanp010g.shp\")\n\n# Read the Shapefile\nindian_lands = gpd.read_file(shapefile_path)\n\n# Count the number of Indian lands per state\nstate_counts = indian_lands.groupby(\"STATE\").size().reset_index(name=\"count\")\n\nprint(state_counts)\n</code></pre> <p>Both R and Python codes download the dataset and read the Shapefile using the respective packages. They then group the data by the \u2018STATE\u2019 attribute and calculate the count of Indian lands per state.</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/","title":"Overview of R and Python","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#i-introduction-5-minutes","title":"I. Introduction (5 minutes)","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#a-brief-overview-of-r-and-python","title":"A. Brief overview of R and Python:","text":"<p>R is a programming language and software environment for statistical computing and graphics, commonly used by statisticians, data scientists, and researchers. It provides a wide range of statistical and graphical techniques and is highly extensible. Python, on the other hand, is a versatile, high-level programming language that has a strong focus on readability and simplicity. It is widely used for various applications, including web development, automation, and scientific computing, making it a popular choice for data analysis as well.</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#b-importance-of-being-bilingual-in-r-and-python","title":"B. Importance of being bilingual in R and Python:","text":"<p>Becoming proficient in both R and Python offers several advantages. First, it broadens your skill set, making you more marketable and versatile in the job market. Second, it allows you to choose the most appropriate language for a given task, leveraging the strengths of each language. For instance, you might use R for advanced statistical modeling, while Python might be your go-to for machine learning or integrating with web services. Finally, bilingualism helps you to collaborate more effectively with colleagues who may prefer one language over the other, enhancing teamwork and productivity.</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#c-course-objectives","title":"C. Course objectives:","text":"<p>The goal of this one-hour course is to introduce the essential similarities and differences between R and Python, focusing on data structures, data manipulation, and data visualization. By the end of the course, participants should have a basic understanding of how to perform common data analysis tasks in both languages, as well as an appreciation for the unique strengths of each language. The course aims to provide a foundation for further exploration and learning in both R and Python.</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#ii-language-basics-10-minutes","title":"II. Language Basics (10 minutes)","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#a-r","title":"A. R","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#1-rstudio-and-r-environment","title":"1. RStudio and R environment:","text":"<p>RStudio is an integrated development environment (IDE) for R that provides a user-friendly interface and a comprehensive set of tools for coding, debugging, and visualization. The R environment is an interactive programming platform that allows you to write and execute R code, manage packages, and explore data structures. RStudio is a popular choice among R users, as it simplifies many tasks and improves overall workflow.</p> <p>R code:</p> <pre><code># Example of R code\n     x &lt;- c(1, 2, 3, 4, 5)\n     y &lt;- x * 2\n     print(y)\n</code></pre> <pre><code>[1]  2  4  6  8 10\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#2-r-syntax","title":"2. R syntax:","text":"<p>R syntax is characterized by its use of assignment operators (e.g., <code>&lt;-</code>), functions, and control structures (e.g., <code>if</code>, <code>for</code>, <code>while</code>). R is case-sensitive, and comments are denoted by the pound sign (<code>#</code>). One notable feature of R syntax is its preference for the use of vectors and vectorized operations, which can lead to concise and efficient code.</p> <p>R code:</p> <pre><code># Example of R syntax\n     x &lt;- seq(1, 10)\n     y &lt;- x^2\n     if (mean(y) &gt; 20) {\n       print(\"Mean is greater than 20\")\n     }\n</code></pre> <pre><code>[1] \"Mean is greater than 20\"\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#b-python","title":"B. Python","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#1-python-environment-eg-jupyter-notebook-ides","title":"1. Python environment (e.g., Jupyter Notebook, IDEs):","text":"<p>Python can be run in various environments, such as Jupyter Notebook, a browser-based interactive computing platform that allows you to write, run, and share code, as well as create rich-text documents with embedded code, equations, and visualizations. Other popular Python IDEs include Visual Studio Code, PyCharm, and Spyder, which offer syntax highlighting, code completion, and debugging features.</p> <p>R code:</p> <pre><code> # Example of R code\n     x &lt;- c(1, 2, 3, 4, 5)\n     y &lt;- x * 2\n     print(y)\n</code></pre> <pre><code>[1]  2  4  6  8 10\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#2-python-syntax","title":"2. Python syntax:","text":"<p>Python syntax is designed to be readable and concise, using indentation to denote code blocks instead of curly braces or keywords. Python is case-sensitive, and comments are denoted by the hash sign (<code>#</code>). Like R, Python supports vectorized operations, especially when using libraries like NumPy or pandas.</p> <p>Python code:</p> <pre><code># Example of Python code\nx = [1, 2, 3, 4, 5]\ny = [i * 2 for i in x]\nprint(y)\n</code></pre> <pre><code>[2, 4, 6, 8, 10]\n</code></pre> <p>Python syntax is designed to be readable and concise, using indentation to denote code blocks instead of curly braces or keywords. Python is case-sensitive, and comments are denoted by the hash sign (<code>#</code>). Like R, Python supports vectorized operations, especially when using libraries like NumPy or pandas.</p> <p>Python code:</p> <pre><code> # Example of Python syntax\nx = list(range(1, 11))\ny = [i**2 for i in x]\nif sum(y) / len(y) &gt; 20:\n  print(\"Mean is greater than 20\")\n</code></pre> <pre><code>Mean is greater than 20\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#iii-data-structures-10-minutes","title":"III. Data Structures (10 minutes)","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#a-r_1","title":"A. R","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#1-vectors","title":"1. Vectors:","text":"<p>Vectors are one-dimensional arrays that can store elements of the same data type (e.g., numeric, character, logical). They are the most basic data structure in R and are used extensively in R programming. Vectors can be created using the c() function.</p> <p>R code:</p> <pre><code># Example of creating a numeric vector in R\nnum_vector &lt;- c(1, 2, 3, 4, 5)\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#2-matrices","title":"2. Matrices:","text":"<pre><code> Matrices are two-dimensional arrays that can store elements of the same data type. They are useful for performing linear algebra operations and can be created using the `matrix()` function.\n</code></pre> <p>R code:</p> <pre><code># Example of creating a matrix in R\nmat &lt;- matrix(1:9, nrow = 3, ncol = 3)\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#3-lists","title":"3. Lists:","text":"<pre><code> Lists are ordered collections that can store elements of different data types, including other lists. They are similar to Python's lists but can contain heterogeneous data. Lists can be created using the `list()` function.\n</code></pre> <p>R code:</p> <pre><code># Example of creating a list in R\nmy_list &lt;- list(\"apple\", 42, c(1, 2, 3), TRUE)\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#4-data-frames","title":"4. Data frames:","text":"<pre><code> Data frames are tabular data structures that can store elements of different data types in columns. They are similar to Python's pandas DataFrame and can be created using the `data.frame()` function.\n</code></pre> <p>R code:</p> <pre><code># Example of creating a data frame in R\ndf &lt;- data.frame(name = c(\"Alice\", \"Bob\", \"Carol\"),\n        age = c(25, 30, 35), height = c(162, 175, 168))\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#b-python_1","title":"B. Python","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#1-lists","title":"1. Lists:","text":"<p>Lists are ordered, mutable collections that can store elements of different data types. They are similar to R\u2019s lists and can be created using square brackets [].</p> <p>Python code:</p> <pre><code># Example of creating a list in Python\nmy_list = [\"apple\", 42, [1, 2, 3], True]\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#2-tuples","title":"2. Tuples:","text":"<p>Tuples are ordered, immutable collections that can store elements of different data types. They are similar to Python\u2019s lists but cannot be modified after creation. Tuples can be created using parentheses <code>()</code>.</p> <p>Python code:</p> <pre><code># Example of creating a tuple in Python\nmy_tuple = (\"apple\", 42, (1, 2, 3), True)\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#3-sets","title":"3. Sets:","text":"<p>Sets are unordered collections of unique elements. They can store elements of different data types but do not maintain their order. Sets can be created using the <code>set()</code> function or curly braces <code>{}</code>.</p> <p>Python code:</p> <pre><code># Example of creating a set in Python\nmy_set = {1, 2, 3, 4, 5}\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#4-dictionaries","title":"4. Dictionaries:","text":"<p>Dictionaries are unordered collections of key-value pairs. They are similar to R\u2019s named lists and can store elements of different data types. Dictionaries can be created using curly braces <code>{}</code> with key-value pairs separated by colons</p> <p>Python code:</p> <pre><code># Example of creating a dictionary in Python\nmy_dict = {\"name\": \"Alice\", \"age\": 25, \"height\": 162}\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#5-pandas-dataframe","title":"5. pandas DataFrame:","text":"<p>pandas DataFrames are two-dimensional, mutable, and heterogeneous tabular data structures with labeled axes (rows and columns). They are similar to R\u2019s data frames and are widely used for data manipulation and analysis in Python. pandas DataFrames can be created by importing data from various sources like CSV files, SQL databases, or even from Python dictionaries or lists.</p> <p>Python code:</p> <pre><code># Example of creating a pandas DataFrame in Python\nimport pandas as pd\n\ndata = {'name': ['Alice', 'Bob', 'Carol'],\n        'age': [25, 30, 35],\n        'height': [162, 175, 168]}\n\ndf = pd.DataFrame(data)\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#iv-data-manipulation-15-minutes","title":"IV. Data Manipulation (15 minutes)","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#a-r_2","title":"A. R","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#1-base-r-functions","title":"1. Base R functions:","text":"<p>Base R functions provide essential tools for data manipulation, such as subsetting, reshaping, and sorting data. These functions allow you to perform various data manipulation tasks, including filtering rows or columns, merging and joining data frames, and aggregating data. Some commonly used base R functions for data manipulation include subset(), merge(), aggregate(), and order().</p> <p>R code:</p> <pre><code># Example of filtering rows using base R functions\ndf &lt;- data.frame(name = c(\"Alice\", \"Bob\", \"Carol\",        \"David\"),age = c(25, 30, 35, 28), height = c(162, 175,   168, 172))\n\n# Filter rows where age is greater than 27\nfiltered_df &lt;- df[df$age &gt; 27, ]\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#2-dplyr-and-the-tidyverse","title":"2. dplyr and the tidyverse:","text":"<p>dplyr is a popular R package for data manipulation that is part of the tidyverse, a collection of R packages designed for data science. dplyr provides a set of intuitive and easy-to-read functions for data manipulation, such as <code>filter()</code>, <code>select()</code>, <code>mutate()</code>, <code>summarise()</code>, and <code>group_by()</code>. dplyr functions can be chained together using the pipe operator (<code>%&gt;%</code>) to create a more readable and efficient workflow.</p> <p>R code:</p> <pre><code># Example of filtering rows using dplyr\nlibrary(dplyr)\n\ndf &lt;- data.frame(name = c(\"Alice\", \"Bob\", \"Carol\",        \"David\"),age = c(25, 30, 35, 28),height = c(162, 175,   168, 172))\n\n# Filter rows where age is greater than 27\nfiltered_df &lt;- df %&gt;% filter(age &gt; 27)\nhead(filtered_df)\n</code></pre> <pre><code>   name age height\n1   Bob  30    175\n2 Carol  35    168\n3 David  28    172\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#b-python_2","title":"B. Python","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#1-built-in-functions","title":"1. Built-in functions:","text":"<p>Python has built-in functions that allow for basic data manipulation, such as filtering, sorting, and aggregating data. However, these functions are less flexible and less efficient when compared to specialized libraries like pandas. Some commonly used built-in functions for data manipulation include filter(), map(), and sorted().</p> <p>Python code:</p> <pre><code> ```python\n # Example of filtering a list using a built-in function\n data = [{\"name\": \"Alice\", \"age\": 25, \"height\": 162},\n         {\"name\": \"Bob\", \"age\": 30, \"height\": 175},\n         {\"name\": \"Carol\", \"age\": 35, \"height\": 168},\n         {\"name\": \"David\", \"age\": 28, \"height\": 172}]\n\n # Filter items where age is greater than 27\n filtered_data = list(filter(lambda x: x[\"age\"] &gt; 27, data))\n ```\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#2-pandas-library","title":"2. pandas library:","text":"<p>The pandas library is a powerful and flexible data manipulation tool in Python. It provides various functions for cleaning, transforming, and analyzing data in a DataFrame format, making it easy to work with structured data. Some common pandas functions for data manipulation include <code>loc[]</code>, <code>iloc[]</code>, <code>query()</code>, <code>merge()</code>, <code>groupby()</code>, and <code>agg()</code>.</p> <p>Python code:</p> <pre><code># Example of filtering rows using pandas\nimport pandas as pd\n\ndata = {'name': ['Alice', 'Bob', 'Carol', 'David'],\n             'age': [25, 30, 35, 28],\n             'height': [162, 175, 168, 172]}\n\ndf = pd.DataFrame(data)\n\n# Filter rows where age is greater than 27\nfiltered_df = df[df['age'] &gt; 27]\n</code></pre>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#v-data-visualization-10-minutes","title":"V. Data Visualization (10 minutes)","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#a-r_3","title":"A. R","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#1-base-r-plotting","title":"1. Base R plotting:","text":"<p>Base R provides built-in plotting functions that enable users to create simple visualizations directly from data structures like vectors and data frames. These functions include plot(), hist(), boxplot(), and barplot(). While base R plotting can create a wide range of visualizations, the syntax and customization options may be less intuitive compared to more specialized libraries like ggplot2.</p> <p>R code:</p> <pre><code># Example of a scatter plot using base R plotting\ndf &lt;- data.frame(x = rnorm(50), y = rnorm(50))\n\nplot(df$x, df$y, main = \"Scatter Plot\", xlab = \"X-axis\", ylab = \"Y-axis\", col = \"blue\")\n</code></pre> <p></p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#2-ggplot2","title":"2. ggplot2:","text":"<p>ggplot2 is a popular R package for data visualization and is part of the tidyverse. It implements the \u201cGrammar of Graphics\u201d concept, which allows users to create complex and customizable visualizations using a consistent and intuitive syntax. With ggplot2, users can create a wide range of visualizations, including scatter plots, bar charts, histograms, and more, using functions like <code>ggplot()</code>, <code>geom_point()</code>, <code>geom_bar()</code>, and <code>geom_histogram()</code>.</p> <p>R code:</p> <pre><code># Example of a scatter plot using ggplot2\nlibrary(ggplot2)\n\ndf &lt;- data.frame(x = rnorm(50), y = rnorm(50))\n\nggplot(df, aes(x = x, y = y)) +\n    geom_point(color = \"blue\") +\n    ggtitle(\"Scatter Plot\") +\n    xlab(\"X-axis\") +\n    ylab(\"Y-axis\")\n</code></pre> <p></p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#b-python_3","title":"B. Python","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#1-matplotlib","title":"1. matplotlib:","text":"<p>matplotlib is a widely-used data visualization library in Python that provides a comprehensive set of tools for creating static, animated, and interactive visualizations. It is inspired by MATLAB\u2019s plotting system and offers a simple and flexible interface for creating a wide range of visualizations, such as line plots, scatter plots, bar plots, and histograms. Some commonly used matplotlib functions include plot(), scatter(), bar(), and hist().</p> <p>Python code:</p> <pre><code># Example of a scatter plot using matplotlib\nimport matplotlib.pyplot as plt\n\nx = [i for i in range(50)]\ny = [i**2 for i in x]\n\nplt.scatter(x, y, color='blue')\nplt.title('Scatter Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n</code></pre> <p></p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#2-seaborn","title":"2. seaborn:","text":"<p>seaborn is a Python data visualization library based on matplotlib that provides a high-level interface for creating informative and attractive statistical graphics. It comes with several built-in themes and color palettes to make it easy to create aesthetically pleasing and consistent visualizations. seaborn offers functions for a wide range of visualizations, such as scatter plots, line plots, bar plots, and box plots, using functions like <code>scatterplot()</code>, <code>lineplot()</code>, <code>barplot()</code>, and <code>boxplot()</code>.</p> <p>Python code:</p> <pre><code># Example of a scatter plot using seaborn\nimport seaborn as sns\n\ndf = pd.DataFrame({'x': [i for i in range(50)], 'y': [i**2 for i in range(50)]})\n\nsns.scatterplot(data=df, x='x', y='y', color='blue')\nplt.title('Scatter Plot')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n</code></pre> <p></p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#vi-conclusions-and-recommendations-5-minutes","title":"VI. Conclusions and Recommendations (5 minutes)","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#a-similarities-between-r-and-python","title":"A. Similarities between R and Python","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#1-both-are-high-level-programming-languages","title":"1. Both are high-level programming languages:","text":"<p>R and Python are both high-level programming languages, meaning they provide a layer of abstraction that simplifies the programming process. This allows users to focus on solving problems or implementing algorithms without worrying about low-level details such as memory management or hardware interaction. Both languages are also interpreted, so code is executed directly without the need for compilation.</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#2-both-support-vectorized-operations","title":"2. Both support vectorized operations:","text":"<p>R and Python both offer support for vectorized operations, which enable users to perform element-wise operations on arrays or other data structures without the need for explicit loops. This feature leads to more concise and readable code and can also improve performance. For example, in R, you can add two vectors element-wise using the <code>+</code> operator, while in Python, you can use the NumPy library to perform similar operations.</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#3-both-have-strong-data-manipulation-and-visualization-capabilities","title":"3. Both have strong data manipulation and visualization capabilities:","text":"<p>R and Python are both widely used in data science due to their strong data manipulation and visualization capabilities. R is known for its packages such as dplyr and ggplot2, while Python has powerful libraries like pandas, matplotlib, and seaborn. These tools allow users to efficiently clean, analyze, and visualize data, making it easier to extract insights and make data-driven decisions. For example, in R, you can use dplyr to filter a data frame and ggplot2 to create a scatter plot, while in Python, you can use pandas to filter a DataFrame and seaborn to create a scatter plot. capabilities</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#vi-similarities-and-differences-10-minutes","title":"VI. Similarities and Differences (10 minutes)","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#b-differences-between-r-and-python","title":"B. Differences between R and Python","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#1-syntax-and-language-structure","title":"1. Syntax and language structure:","text":"<p>R and Python have different syntax and language structures, which can be a challenge for those learning both languages. R uses \\&lt;- for assignment, while Python uses =. R\u2019s syntax is also more functional, whereas Python is more object-oriented. Familiarizing oneself with the syntax and structure of each language is essential for working efficiently in both.</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#2-r-is-more-focused-on-statistical-analysis-while-python-is-a-general-purpose-language","title":"2. R is more focused on statistical analysis, while Python is a general-purpose language:","text":"<p>R was specifically designed for statistical analysis and data manipulation, while Python is a general-purpose programming language that can be used for various applications, including data science, web development, and software development. As a result, R has more built-in statistical functions and a more comprehensive set of packages for data analysis, while Python offers a broader range of functionality and libraries for other purposes.</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#3-r-has-a-more-comprehensive-set-of-statistical-packages-while-python-has-more-diverse-libraries","title":"3. R has a more comprehensive set of statistical packages, while Python has more diverse libraries:","text":"<p>Although both R and Python offer a rich ecosystem of libraries and packages, R has a more comprehensive set of statistical packages, which makes it the preferred choice for many statisticians and data analysts. On the other hand, Python has a more diverse set of libraries, making it suitable for a wide range of applications, including machine learning, computer vision, and natural language processing.</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#c-recommendations-for-learning-both-languages","title":"C. Recommendations for learning both languages","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#1-focus-on-learning-the-core-concepts-and-structures-in-both-languages","title":"1. Focus on learning the core concepts and structures in both languages:","text":"<p>To become proficient in both R and Python, focus on understanding the core concepts and structures of each language. This includes learning the basic syntax, data structures, and built-in functions. By mastering these fundamentals, you\u2019ll be better equipped to apply your knowledge to various data science tasks.</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#2-practice-using-both-languages-in-different-contexts","title":"2. Practice using both languages in different contexts:","text":"<p>One of the best ways to become proficient in both R and Python is to practice using them in different contexts. This might include working on data manipulation, data visualization, statistical analysis, or machine learning tasks. By applying both languages to a variety of problems, you\u2019ll gain a deeper understanding of their strengths and weaknesses, which will help you decide when to use each language.</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#3-engage-with-online-resources-and-communities-for-both-r-and-python","title":"3. Engage with online resources and communities for both R and Python:","text":"<p>There are numerous online resources and communities available for learning both R and Python. Engage with these resources, such as tutorials, blogs, forums, and online courses, to further your understanding of both languages. Additionally, participating in the R and Python communities can provide valuable insights, as well as opportunities for networking and collaboration.</p>"},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#vii-qa-and-resources-5-minutes","title":"VII. Q&amp;A and Resources (5 minutes)","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#a-address-any-questions-from-participants","title":"A. Address any questions from participants","text":""},{"location":"2_R_and_Py_bilingualism/code/outline/outline/#b-share-resources-for-further-learning-and-exploration","title":"B. Share resources for further learning and exploration","text":"<p>Python Resources:</p> <ol> <li>Earth Data Science (https://www.earthdatascience.org) - Offers a     wide range of tutorials and resources on Python for Earth data     science, covering topics such as data manipulation, visualization,     and geospatial analysis.</li> <li>Real Python (https://realpython.com) - Provides a comprehensive     collection of tutorials, articles, and guides on various aspects of     Python programming, suitable for beginners and experienced     developers alike.</li> <li>Python.org (https://docs.python.org/3/tutorial/index.html) - The     official Python documentation, featuring an extensive tutorial     covering the basics of the language.</li> <li>DataCamp     (https://www.datacamp.com/courses/intro-to-python-for-data-science) -     Offers an introductory course on Python for data science, covering     topics like data manipulation and visualization using libraries like     NumPy, pandas, and matplotlib.</li> <li>Kaggle (https://www.kaggle.com/learn/python) - Provides a free     Python course that covers fundamental programming concepts, with a     focus on data science applications.</li> </ol> <p>R Resources:</p> <ol> <li>R for Data Science (https://r4ds.had.co.nz) - A free online book by     Hadley Wickham and Garrett Grolemund, covering the essentials of     data science using R, including data manipulation, visualization,     and modeling with the tidyverse.</li> <li>DataCamp (https://www.datacamp.com/courses/free-introduction-to-r) -     Offers a free introductory course on R, covering the basics of the     language, data structures, and data manipulation.</li> <li>R-bloggers (https://www.r-bloggers.com) - A blog aggregator that     features articles and tutorials on R from various authors, covering     a wide range of topics and applications.</li> <li>The R Project for Statistical Computing     (https://www.r-project.org) - The official website of the R     programming language, featuring documentation, resources, and a list     of recommended books.</li> <li>RStudio Community (https://community.rstudio.com) - An online forum     where users can ask questions, share knowledge, and engage with the     R community.</li> </ol> <p>By exploring these resources, you\u2019ll gain a deeper understanding of R and Python and enhance your skills in data science and programming.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/","title":"Tools for Reproducible and Open Science","text":"<p>This page is organized into three distinct segments:</p> <ul> <li>Introduction to Open Science (~50 minutes)</li> <li>Documentation &amp; Communication (~25 minutes)</li> <li>Repeatability and Reproducibility (~ 45 minutes)</li> </ul>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#introduction-to-open-science","title":"Introduction to Open Science","text":"<p>Objectives</p> <pre><code>After this section, you should be able to:\n\n* Explain what Open Science is\n* Explain the components of Open Science\n* Describe the behaviors of Open Science\n* Explain why Open Science matters in education, research, and society\n* Understand the advantages and the challenges to Open Science\n* Identify who the practitioners of Open Science are\n* Understand the underlying Ethos of Open Science\n</code></pre>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#what-is-open-science","title":"What is Open Science?","text":"<p>If you ask a dozen researchers this question, you will probably get just as many answers.</p> <p>This means that Open Science isn't necessarily a set of checkboxes you need to tick, but rather a holistic approach to doing science. In that spirit, it can also be useful to think about Open Science as a spectrum, from less to more open. </p> Definitions <p>\"Open Science is defined as an inclusive construct that combines various movements and practices aiming to make multilingual scientific knowledge openly  available,  accessible  and  reusable  for  everyone,  to  increase  scientific  collaborations  and  sharing of information for the benefits of science and society, and to open the processes of scientific knowledge creation, evaluation and communication to societal actors beyond the traditional scientific community.\" - UNESCO Definition</p> <ul> <li>UNESCO's Recommendation on Open Science</li> </ul> <p>\"Open Science is the movement to make scientific research (including publications, data, physical samples, and software) and its dissemination accessible to all levels of society, amateur or professional...\"   Wikipedia definition</p> <p>Open and Collaborative Science Network's Open Science Manifesto</p> Six Pillars  of Open Science <p> Open Access Publications</p> <p> Open Data</p> <p> Open Educational Resources</p> <p> Open Methodology</p> <p> Open Peer Review</p> <p> Open Source Software</p> Wait, how many pillars  of Open Science Really Are There? <p>The number can be from 4  to 8 </p> Foster Open Science Diagram <p> </p> <p>Graphic by Foster Open Science</p> <pre><code>flowchart LR\n\nid1([open science]) --&gt; id3([publishing]) &amp; id4([data]) &amp; id5([open source software])\n\nid3([publishing]) --&gt;  id41([access]) &amp; id42([reviews]) &amp; id43([methods]) &amp; id44([educational resources]) \n\nid5([open source software]) --&gt; id13([container registries]) &amp; id10([services]) &amp; id101([workflows]) &amp; id12([version control systems])\n\nid12([version control systems]) --&gt; id101([workflows])\n\nid13([container registries]) --&gt; id101([workflows])\n\nid14([public data registry]) --&gt; id101([workflows])\n\nid10([services]) --&gt; id101([workflows]) \n\nid44([educational resources]) --&gt; id21([university libraries])\n\nid21([university libraries]) --&gt; id101([workflows])\n\nid22([federal data archives]) --&gt; id101([workflows]) \n\nid4([data]) --&gt; id21([university libraries]) &amp; id22([federal data archives]) &amp; id14([public data registries]) \n\nid101([workflows]) --&gt; id15([on-premises]) &amp; id16([commercial cloud]) &amp; id17([public cloud])\n</code></pre> <p>Mermaid Diagram: Conceptual relationships of Open Science and cyberinfrastructure</p> <p> Awesome Lists of Open Science</p> <p>Awesome lists were started on GitHub by Sindre Sorhus and typically have a badge associated with them  </p> <p>(There is even a Searchable Index of Awesome Lists)</p> <p>We have created our own Awesome Open Science List here which may be valuable to you.</p> <p>To help move the scientific community toward open science, NASA has declared 2023 is the year of Open Science! </p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#open-access-publications","title":"Open Access Publications","text":"Definitions <p>\"Open access is a publishing model for scholarly communication that makes research information available to readers at no cost, as opposed to the traditional subscription model in which readers have access to scholarly information by paying a subscription (usually via libraries).\" -- OpenAccess.nl</p> New Open Access Mandates in US <p>The White House Office of Science and Technology (OSTP) has recently released a policy stating that tax-payer funded research must by open access by 2026.</p> Open Access Publishing <p>Major publishers have provided access points for publishing your work </p> <p>AAAS</p> <p>Nature</p> <p>American Geophysical Union</p> <p>Commonwealth Scientific and Industrial Research Organisation (CSIRO)</p> <p>Open Research Europe</p> <p>PLOS</p> <p>MDPI</p> <p>Ecosphere</p> Financial Support for Open Access Publishing Fees <p>There are mechanisms for helping to pay for the additional costs of publishing research as open access:</p> <p>SciDevNet</p> <p>Health InterNetwork Access to Research Initiative (HINARI)</p> <p>Some institutions offer support for managing publishing costs (check to see if your institution has such support):</p> <p>University of Arizona Open Access Investment Fund</p> <p>Colorado University at Boulder Open Access Fund</p> <p>Max Planck Digital Library - German authors can have OA fees in Springer Nature research journals paid for.</p> <p>Bibsam Consortium - Swedish authors can have OA fees in Springer Nature research journals paid for.</p> Pre-print Services <p>ASAPbio Pre-Print Server List - ASAPbio is a scientist-driven non-profit promoting transparency and innovation comprehensive list of pre-print servers inthe field of life science communication.</p> <p>ESSOar - Earth and Space Science Open Archive hosted by the American Geophysical Union.</p> <p>Peer Community In (PCI) a free recommendation process of scientific preprints based on peer reviews</p> <p>OSF.io Preprints are partnered with numerous projects under the \"-rXivs\"</p> The rXivs <p>AfricArXiv</p> <p>AgrirXiv</p> <p>Arabixiv</p> <p>arXiv - is a free distribution service and an open-access archive for 2,086,431 scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.</p> <p>BioHackrXiv</p> <p>BioRxiv -  is an open access preprint repository for the biological sciences.</p> <p>BodorXiv</p> <p>EarthArXiv - is an open access preprint repository for the Earth sciences.</p> <p>EcsArXiv - a free preprint service for electrochemistry and solid state science and technology</p> <p>EdArXiv - for the education research community</p> <p>EngrXiv for the engineering community</p> <p>EvoEcoRxiv - is an open acccess preprint repository for Evolutionary and Ecological sciences.</p> <p>MediArXiv for Media, Film, &amp; Communication Studies</p> <p>MedRxiv - is an open access preprint repository for Medical sciences.</p> <p>PaleorXiv - is an open access preprint repository for Paleo Sciences</p> <p>PsyrXiv - is an open access preprint repository for Psychological sciences.</p> <p>SocArXiv - is an open access preprint repository for Social sciences.</p> <p>SportrXiv - is an open access preprint for Sports sciences.</p> <p>ThesisCommons - open Theses</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#open-data","title":"Open Data","text":"<p>Open Data are a critical aspect of open science. There are three key attributes of Open Data:</p> <ul> <li>Availability and accessibility</li> <li>Reusability</li> <li>Inclusivity</li> </ul> Definitions <p>\u201cOpen data and content can be freely used, modified, and shared by anyone for any purpose\u201d - The Open Definition</p> <p>\"Open data is data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and sharealike.\" - Open Data Handbook</p> <p> Wikipedia definition</p> DIKW Pyramid <p>Data are the basis of our understanding the natural world. The Data-Information-Knowledge-Wisdom (DIKW) pyramid describes for us how data are refined into information and knowledge.</p> <p></p> FAIR &amp; CARE Principles <p>FAIR Principles</p> <p>In 2016, the FAIR Guiding Principles for scientific data management and stewardship were published in Scientific Data. Read it.</p> <p>Findable</p> <ul> <li>F1. (meta)data are assigned a globally unique and persistent identifier</li> <li>F2. data are described with rich metadata (defined by R1 below)</li> <li>F3. metadata clearly and explicitly include the identifier of the data it describes</li> <li>F4. (meta)data are registered or indexed in a searchable resource</li> </ul> <p>Accessible</p> <ul> <li>A1. (meta)data are retrievable by their identifier using a     standardized communications protocol</li> <li>A1.1 the protocol is open, free, and universally implementable</li> <li>A1.2 the protocol allows for an authentication and authorization     procedure, where necessary</li> <li>A2. metadata are accessible, even when the data are no longer     available</li> </ul> <p>Interoperable</p> <ul> <li>I1. (meta)data use a formal, accessible, shared, and broadly     applicable language for knowledge representation.</li> <li>I2. (meta)data use vocabularies that follow FAIR principles</li> <li>I3. (meta)data include qualified references to other (meta)data</li> </ul> <p>Reusable</p> <ul> <li>R1. meta(data) are richly described with a plurality of accurate     and relevant attributes</li> <li>R1.1. (meta)data are released with a clear and accessible data     usage license</li> <li>R1.2. (meta)data are associated with detailed provenance</li> <li>R1.3. (meta)data meet domain-relevant community standard</li> </ul> <p>Open vs. Public vs. FAIR</p> <pre><code>FAIR does not demand that data be open: See one definition of open:\nhttp://opendefinition.org/\n</code></pre> <p>Why Principles?</p> <pre><code>FAIR is a collection of principles. Ultimately, different\ncommunities within different scientific disciplines must work to\ninterpret and implement these principles. Because technologies\nchange quickly, focusing on the desired end result allows FAIR to be\napplied to a variety of situations now and in the foreseeable\nfuture.\n</code></pre> <p>CARE Principles</p> <p>The CARE Principles for Indigenous Data Governance were drafted at the International Data Week and Research Data Alliance Plenary co-hosted event \"Indigenous Data Sovereignty Principles for the Governance of Indigenous Data Workshop,\" 8 November 2018, Gaborone, Botswana.</p> <p>Collective Benefit</p> <ul> <li>C1. For inclusive development and innovation</li> <li>C2. For improved governance and citizen engagement</li> <li>C3. For equitable outcomes</li> </ul> <p>**Authority to Control*</p> <ul> <li>A1. Recognizing rights and interests</li> <li>A2. Data for governance</li> <li>A3. Governance of data</li> </ul> <p>Responsibility</p> <ul> <li>R1. For positive relationships</li> <li>R2. For expanding capability and capacity</li> <li>R3. For Indigenous languages and worldviews</li> </ul> <p>Ethics</p> <ul> <li>E1. For minimizing harm and maximizing benefit</li> <li>E2. For justice</li> <li>E3. For future use</li> </ul> <p>FAIR - TLC</p> <p>Traceable, Licensed, and Connected</p> <ul> <li>The need for metrics: https://zenodo.org/record/203295#.XkrzTxNKjzI</li> </ul> <p>How to get to FAIR?</p> <p>This is a question that only you can answer, that is because it depends on (among other things)</p> <ol> <li>Your scientific discipline: Your datatypes and existing standards     for what constitutes acceptable data management will vary.</li> <li>The extent to which your scientific community has implemented     FAIR: Some disciplines have significant guidelines on FAIR, while     others have not addressed the subject in any concerted way.</li> <li>Your level of technical skills: Some approaches to implementing     FAIR may require technical skills you may not yet feel comfortable     with.</li> </ol> <p>While a lot is up to you, the first step is to evaluate how FAIR you think your data are:</p> Exercise <p>Thinking about a dataset you work with, complete the ARDC FAIR assessment.</p> Resources <ul> <li>The FAIR Guiding Principles for scientific data management and stewardship</li> <li>Wilkinson et al. (2016) established the guidelines to improve the Findability, Accessibility, Interoperability, and Reuse (FAIR) of digital assets for research. </li> <li>Go-FAIR website</li> <li>Carroll et al. (2020) established the CARE Principles for Indigenous Data Governance. full document </li> <li>Indigenous Data Sovereignty Networks</li> </ul> LOD Cloud <p>The Linked Open Data Cloud shows how data are linked to one another forming the basis of the semantic web .</p> <p></p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#open-educational-resources","title":"Open Educational Resources","text":"Definitions <p>\"Open Educational Resources (OER) are learning, teaching and research materials in any format and medium that reside in the public domain or are under copyright that have been released under an open license, that permit no-cost access, re-use, re-purpose, adaptation and redistribution by others.\" - UNESCO</p> <p> Wikipedia definition</p> Digital Literacy Organizations <p>The Carpentries - teaches foundational coding and data science skills to researchers worldwide  </p> <p>EdX - Massively Online Online Courses (not all open) hosted through University of California Berkeley</p> <p>EveryoneOn - mission is to unlock opportunity by connecting families in underserved communities to affordable internet service and computers, and delivering digital skills trainings </p> <p>ConnectHomeUSA - is a movement to bridge the digital divide for HUD-assisted housing residents in the United States under the leadership of national nonprofit EveryoneOn</p> <p>Global Digital Literacy Council -  has dedicated more than 15 years of hard work to the creation and maintenance of worldwide standards in digital literacy</p> <p>IndigiData - training and engaging tribal undergraduate and graduate students in informatics</p> <p>National Digital Equity Center a 501c3 non-profit, is a nationally recognized organization with a mission to close the digital divide across the United States</p> <p>National Digital Inclusion Allaince - advances digital equity by supporting community programs and equipping policymakers to act</p> <p>Net Literacy</p> <p>Open Educational Resources Commons</p> <p>Project Pythia is the education working group for Pangeo and is an educational resource for the entire geoscience community</p> <p>Research Bazaar - is a worldwide festival promoting the digital literacy emerging at the centre of modern research</p> <p>TechBoomers - is an education and discovery website that provides free tutorials of popular websites and Internet-based services in a manner that is accessible to older adults and other digital technology newcomers</p> Educational Materials <p>Teach Together by Greg Wilson</p> <p>DigitalLearn</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#open-methodology","title":"Open Methodology","text":"<p>The use of version control systems like GitHub and GitLab present one of the foremost platforms for sharing open methods for digital research.</p> Definitions <p>\"An open methodology is simply one which has been described in sufficient detail to allow other researchers to repeat the work and apply it elsewhere.\" - Watson (2015)</p> <p>\"Open Methodology refers to opening up methods that are used by researchers to achieve scientific results and making them publicly available.\" - Open Science Network Austria</p> Protocols and Bench Techniques <p>BioProtocol</p> <p>Current Protocols</p> <p>Gold Biotechnology Protocol list</p> <p>JoVE - Journal of Visualized Experiments</p> <p>Nature Protocols</p> <p>OpenWetWare</p> <p>Protocol Exchange</p> <p>Protocols Online</p> <p> Protocols</p> <p>SciGene</p> <p>Springer Nature Experiments</p> Concept of Preregistration <p>In response to the Reproducibility Crisis, many researchers, particularly in fields like psychology, have begun to advocate for preregistration of studies. </p> <p>This involves writing out and publishing your entire research plan, from data collection to analysis and publication, for the sake of avoiding practices like p-hacking or HARKing. </p> <p>What preregistration also does is make the process of your work more open, including many of the small decisions and tweaks you make to a project that probably wouldn't make it into a manuscript. </p> <p>To learn more about preregistration, you can check out the Open Science Foundation, a project that provides a preregistration platform and other Open Science tools. You can also read this publication</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#open-peer-review","title":"Open Peer Review","text":"<p>Pros and Cons of Open Peer Review</p> Definitions <p>Ross-Hellauer et al. (2017) ask What is Open Peer Review? and state that there is no single agreed upon definition</p> <p> Wikipedia's definition</p> <p>A manuscript review process that includes some combination of Open Identities, Open Reports, Open Participation, and even Open Interaction</p> Open Peer Review Resources <p>F1000Research the first open research publishing platform. Offering open peer review rapid publication</p> <p>PREreview provides a space for open peer reviews, targeted toward early career researchers.</p> <p>ASAPbio Accelerating Science and Publication in Biology, an open peer review source for biologists and life scientists.</p> <p>PubPeer platform for post-publication of peer reviews.</p> <p>Sciety platform for evaluating preprints.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#open-source-software","title":"Open Source Software","text":"Definitions <p>\"Open source software is code that is designed to be publicly accessible\u2014anyone can see, modify, and distribute the code as they see fit. Open source software is developed in a decentralized and collaborative way, relying on peer review and community production.\" -  Red Hat</p> <p> Open Source Initiative definition</p> <p> Wikipedia definition</p> <p>Awesome list</p> Liceses Definitions <p>Licensing is a crucial aspect of Open Science, as it helps define the terms under which research outputs, such as data, software, publications, and other digital resources, can be accessed, used, shared, and reused by others. Licensing enables the dissemination of knowledge, fosters collaboration, and ensures that research outputs are properly attributed and protected.</p> <p>In the context of Open Science, the most commonly used licese models are the  Creative Commons Licences and the  Open Source Initiative Licenses. In summary:</p> <ul> <li>Creative Commons licenses allow content creators to retain certain rights while granting others the freedom to use and distribute their work under specific conditions, including work outside of science.</li> <li>Open Science Initiative licenses specifically focus on licensing scientific research outputs, including data, software, publications, and related materials. It is tailored to meet the unique requirements and challenges of the scientific community, taking into account FAIR and CARE priciples.</li> </ul>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#why-do-open-science","title":"WHY do Open Science?","text":"<p>There are many reasons to do Open Science, and presumably one or more of them brought you to this workshop. </p> <p>Whether you feel an ethical obligation, want to improve the quality of your work, or want to look better to funding agencies, many of the same approaches to Open Science apply.</p> <p>A paper from Bartling &amp; Friesike (2014) posits that there are 5 main schools of thought in Open Science, which represent 5 underlying motivations:</p> <ol> <li> <p>Democratic school: primarily concerned with making scholarly work freely available to everyone</p> </li> <li> <p>Pragmatic school: primarily concerned with improving the quality of scholarly work by fostering collaboration and improving critiques</p> </li> <li> <p>Infrastructure school: primarily focused on the platforms, tools, and services necessary to conduct efficient research, collaboration, and communication</p> </li> <li> <p>Public school: primarily concerned with societal impact of scholarly work, focusing on engagement with broader public via citizen science, understandable scientific communication, and less formal communication</p> </li> <li> <p>Measurement school: primarily concerned with the existing focus on journal publications as a means of measuring scholarly output, and focused on developing alternative measurements of scientific impact</p> </li> </ol> <p>  In Bartling &amp; Friesike (2014) Open Science: One Term, Five Schools of Thought </p> <p>While many researchers may be motivated by one or more of these aspects, we will not necessarily focus on any of them in particular. If anything, FOSS may be slightly more in the Infrastructure school, because we aim to give you the tools to do Open Science based on your own underlying motivations.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#ethos-of-open-science","title":"Ethos of Open Science","text":"<p>Doing Open Science requires us to understand the ethics of why working with data which do not belong to us is privileged.</p> <p>We must also anticipate how these could be re-used in ways contrary to the interests of humanity. </p> <p>Ensure the use of Institutional Review Boards (IRB) or your local ethical committee. </p> <p>Areas to consider: </p> <p> </p> <p>Source: UK Statistics Authority </p> <ul> <li>Geolocation (survey, land ownership, parcel data), see UK Statistics Authority Ethical Considerations </li> <li>Personal identification information  US Personal Identifiable Information (PII), General Data Protection Regulation (GDPR)</li> <li>Health information US HIPAA , EU GDPR</li> <li>Protected and Endangered Species (US Endangered Species Act)</li> <li>Indigenous data sovereignty: CARE Principles for Indigenous Data Governance , Global Indigenous Data Alliance (GIDA), First Nations OCAP\u00ae (Ownership Control Access and Possession), Circumpolar Inuit Protocols for Equitable and Ethical Engagement </li> <li>Artificial intelligence/machine learning Assessment List Trustworthy AI (ALTAI) from the European AI Alliance</li> </ul> <p> \"Nothing about us, without us\"</p> <ul> <li>Funnel et al. (2019)</li> </ul> <p>For more information (training):</p> <p>Ethics and Data Access (General Information with BioMedical and Life Sciences Data) includes a legal and ethical checklist lesson for researchers around \"FAIR Plus\".</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#recommended-open-science-communities","title":"Recommended Open Science Communities","text":"The Turing Way NASA Transform to Open Science Foster Open Science The Carpentries Center of Open Science <p> Open Scholarship Grassroots Community Networks</p>  International Open Science Networks <p>Center for Scientific Collaboration and Community Engagement (CSCCE)</p> <p>Center for Open Science (COS)</p> <p>Eclipse Science Working Group</p> <p>eLife</p> <p>NumFocus</p> <p>Open Access Working Group</p> <p>Open Research Funders Group</p> <p>Open Science Foundation</p> <p>Open Science Network</p> <p>pyOpenSci</p> <p>R OpenSci</p> <p>Research Data Alliance (RDA)</p> <p>The Turing Way</p> <p>UNESCO Global Open Science Partnership</p> <p>World Wide Web Consortium (W3C)</p>  US-based Open Science Networks <p>CI Compass - provides expertise and active support to cyberinfrastructure practitioners at USA NSF Major Facilities in order to accelerate the data lifecycle and ensure the integrity and effectiveness of the cyberinfrastructure upon which research and discovery depend.</p> <p>Earth Science Information Partners (ESIP) Federation -  is a 501\u00a9(3) nonprofit supported by NASA, NOAA, USGS and 130+ member organizations.</p> <p>Internet2 - is a community providing cloud solutions, research support, and services tailored for Research and Education. </p> <p>Minority Serving Cyberinfrastructure Consortium (MS-CC) envisions a transformational partnership to promote advanced cyberinfrastructure (CI) capabilities on the campuses of Historically Black Colleges and Universities (HBCUs), Hispanic-Serving Institutions (HSIs), Tribal Colleges and Universities (TCUs), and other Minority Serving Institutions (MSIs). </p> <p>NASA Transform to Open Science (TOPS) - coordinates efforts designed to rapidly transform agencies, organizations, and communities for Earth Science</p> <p>OpenScapes - is an approach for doing better science for future us</p> <p>The Quilt - non-profit regional research and education networks collaborate to develop, deploy and operate advanced cyberinfrastructure that enables innovation in research and education.</p>  Oceania Open Science Networks <p>New Zealand Open Research Network - New Zealand Open Research Network (NZORN) is a collection of researchers and research-associated workers in New Zealand.</p> <p>Australia &amp; New Zealand Open Research Network - ANZORN is a network of local networks distributed without Australia and New Zealand.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#self-assessment-1","title":"Self Assessment 1","text":"What motivates you to do Open Science? What characteristics might a paper, project, lab group require to qualify as doing Open Science What are some limitations to you, your lab group, or your domain? True or False: All research papers published in the top journals, like Science and Nature, are always Open Access? Failure <p>False</p> <p>Major Research journals like Science and Nature have an \"Open Access\" option when a manuscript is accepted, but they charge an extra fee to the authors to make those papers Open Access.</p> <p>These high page costs are exclusionary to the majority of global scientists who cannot afford to front these costs out of pocket.</p> <p>This will soon change, at least in the United States. The Executive Branch of the federal government recently mandated that future federally funded research be made Open Access after 2026.</p> True or False: an article states all of the research data used in the experiments \"are available upon request from the corresponding author(s),\" meaning the data are \"Open\" Failure <p>False</p> <p>In order for research to be open, the data need to be freely available from a digital repository, like Data Dryad, Zenodo.org, or CyVerse.</p> <p>Data that are 'available upon request' do not meet the FAIR data principles. </p> True or False: Online Universities and Data Science Boot Camps like UArizona Online, Coursera, Udemy, etc. promote digital literacy and are Open Educational Resources? Failure <p>False</p> <p>These examples are for-profit programs which teach data science and computer programming online. Some may be official through public or private universities and offer credits toward a degree or a certificate. Some of these programs are known to be predatory.</p> <p>The organizations we have listed above are Open Educational Resources - they are free and available to anyone who wants to work with them asynchronously, virtually, or in person.</p> Using a version control system to host the analysis code and computational notebooks, and including these in your Methods section or Supplementary Materials, is an example of an Open Methodology? Success <p>Yes!</p> <p>Using a VCS like GitHub or GitLab is a great step towards making your research more reproducible. </p> <p>Ways to improve your open methology can include documentation of your physical bench work, and even video recordings and step-by-step guides for every part of your project.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#documentation-and-communication","title":"Documentation and Communication","text":"<p>Learning Objectives</p> <p>After this section, you should be able to:</p> <ul> <li>Identify and explain different types of project documentation (both internal and external)</li> <li>Describe tools and approaches to creating your own documentation</li> <li>Describe best practices for maintaining documentation</li> <li>Identify and explain different communication strategies for working in a team (virtual and in person)</li> <li>Create your own GitHub Pages website (!)</li> </ul> <p>Peer-reviewed manuscripts or conference preceedings / presentations /posters are one of the primary ways of communicating science, but they are far from the only avenues of communcation available to us as researchers and educators.</p> <p>As our methods become more complicated and customized, open science means giving people a better understanding of our approaches and tools than may be required in most journals. </p> <p>Communicating amongst a team of researchers that may span institutions, time zones, or continents also requires more modern approaches. </p> <p>Strong frameworks for internal communication and documentation can make collaboration easier, improve the quality of your science, and reduce the hassle of collaborative work.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#project-documentation","title":"Project Documentation","text":"<p>The documentation system, by Divio, categorizes the different types of documentation into 4 quadrants:</p> <p></p> <p>Explaining the documentation system quadrants</p> <ul> <li>Tutorials: Lessons! Tutorials are lessons that take the reader by the hand through a series of steps to complete a project of some kind. They are what your project needs in order to show a beginner that they can achieve something with it.</li> <li>How-to-guides: Recipes! How-to-guides take the reader through the steps required to solve a real-world problem.</li> <li>References: Guides! References offer technical descriptions of the machinery and how to operate it. References have one job only: to describe. They are code-determined, because ultimately that\u2019s what they describe: key classes, functions, APIs, and so they should list things like functions, fields, attributes and methods, and set out how to use them.</li> <li>Explanation: Discussions! The aims of explanations are to clarify and illuminate a particular topic by broadening the documentation\u2019s coverage of a topic.</li> </ul> <p>You can read more in depth on this chart by visiting https://documentation.divio.com/introduction/.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#public-repositories-for-documentation","title":"Public Repositories for Documentation","text":"<p>This website is rendered using  GitHub Pages using  MkDocs and the Material theme for MkDocs. </p> <p>Other popular website generators for GitHub Pages are  Jekyll Theme or  Bootstrap.js.</p> <p> ReadTheDocs.org has become a popular tool for developing web-based documentation. Think of RTD as \"Continuous Documentation\".</p> <p> Bookdown is an open-source R package that facilitates writing books and long-form articles/reports with R Markdown.</p> <p> Quarto is an open-source scientific and technical publishing system built on Pandoc</p> <p> Confluence Wikis (CyVerse) are another tool for documenting your workflow.</p> <p>Things to remember about Documentation</p> <ul> <li> <p>Documentation should be written in such a way that people who did not write the documentation can read and then use or read and then teach others in the applications of the material.</p> </li> <li> <p>Documentation is best treated as a living document, but version control is necessary to maintain it</p> </li> <li> <p>Technology changes over time, expect to refresh documentation every 3-5 years as your projects age and progress.</p> </li> </ul> <p> GitHub Pages</p> <ul> <li>You can pull templates from other GitHub users for your website,     e.g.  Jekyll themes</li> <li>GitHub pages are free, fast, and easy to build, but limited in use     of subdomain or URLs.</li> </ul> <p> ReadTheDocs</p> <ul> <li>publishing websites via     ReadTheDocs.com costs money.</li> <li>You can work in an offline state, where you develop the materials     and publish them to your localhost using     Sphinx</li> <li>You can work on a website template in a GitHub repository, and     pushes are updated in near real time using ReadTheDocs.com.</li> </ul> <p> Material MkDocs</p> <ul> <li>publish via GitHub Actions</li> <li>Uses open source Material or ReadTheDocs Themes</li> </ul> <p> Bookdown</p> <ul> <li>Bookdown websites can be hosted by RStudio     Connect</li> <li>You can publish a Bookdown website using Github     Pages</li> </ul> <p> Quarto</p> <ul> <li>Build a website using Quarto's template builder</li> <li>Build with Github Pages</li> </ul> <p> JupyterBook</p> <ul> <li>Based on Project Jupyter <code>ipynb</code> and MarkDown</li> <li>Uses <code>conda</code> package management</li> </ul> <p> GitBook</p> <ul> <li>GitBook websites use MarkDown syntax</li> <li>Free for open source projects, paid plans are available</li> </ul>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#websites-to-host-methods-protocols","title":"Websites to Host Methods &amp; Protocols","text":"<p>Open Science Framework for free. OSF can be directly linked to your ORCID.</p> <ul> <li>Integrated project management tools</li> <li>Uses templates to create a project website</li> <li>Can publish preprints from within project management tools</li> </ul> <p>Protocols.io - collaborative platform and preprint server for: science methods, computational workflows, clinical trials, operational procedures, safety checklists, and instructions / manuals.</p> <p>QUBES - community of math and biology educators who share resources and methods for preparing students to tackle real, complex, biological problems.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#other-github-pages-website-quickstarts","title":"Other GitHub Pages Website Quickstarts","text":"GitHub Pages ReadTheDocs.org Material MkDocs Bookdown 1.  Create a GitHub account  2.  Clone the repo  3.  Create an <code>index.html</code>  4.  Push it back to GitHub 1.  Install  2.  Use Github  3.  Create a ReadTheDocs account 1. Install Material  2. Clone a repository with an existing template or create a new repo with <code>mkdocs new .</code>  3. Run <code>python -m mkdocs serve</code> to build and serve locally  4. Open your browser to preview the build at https://localhost:8000 1.  Install R and RStudio  2.  Install Bookdown package with <code>install.packages(\"bookdown\", dependencies=TRUE)</code>  3.  Open the Bookdown demo and get started Quarto JupyterBook GitBook 1. Build locally  2. Push to GitHub (or use Actions) 1. Create your first book 1. Follow Template builder (requires account)"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#communication","title":"Communication","text":""},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#internal-project","title":"Internal Project","text":"<p>Choosing which software to use for your internal lab communication can be complicated by the cost of setting up, the cost of maintaining, and simply by the sheer number of platforms that are out there.</p> <p>For this workshop, we use  SLACK (Searchable Log of All Conversation &amp; Knowledge). Microsoft's competitor to SLACK is  Microsoft Teams.</p> <p>Remember, the intention of these platforms are to improve productivity &amp; not become a distraction.</p> <p> SLACK</p> <ul> <li>Slack has plenty of apps for coordinating     multiple services, i.e. Calendars, Github, GoogleDrive, Box, etc.</li> <li>Slack is limiting unless you're willing to pay for the professional     support.</li> </ul> <p> Microsoft Teams</p> <ul> <li>Teams is used by many R1 research universities as part of their     campus wide license agreement for Office 365 Business and Education.</li> </ul> <p>Other popular alternatives</p> <ul> <li> BaseCamp</li> <li> Discord</li> <li> Mastodon</li> <li> Mattermost</li> </ul> <p>Useful links for creating a SLACK workspace</p> <pre><code>1.  [Create a new Workspace](https://get.slack.help/hc/en-us/articles/206845317-Create-a-Slack-workspace){target=_blank}\n2.  [Create channels, add apps &amp; tools](https://get.slack.help/hc/en-us/articles/217626298-tips-for-team-creators-and-admins){target=_blank}\n</code></pre>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#external-public","title":"External (Public)","text":"<p>Although we didn't cover it explicitly in the announcement for the workshop, communicating with the public and other members of your science community is one of the most important parts of your science!</p> <p>There are many ways scientists use social media and the web to share their data science ideas:</p> <ol> <li> \"Science Twitter\" -     is really just regular Twitter, but with a     focus on following other scientists and organizations, and tweeting     about research you're interested in. By building up a significant     following, more people will know you, know about your work, and     you'll have a higher likelihood of meeting other new collaborators.</li> <li> Blogs - there are     numerous platforms for blogging about research, the older platforms     tend to dominate this space. Other platforms like,     Medium offer a place for     reseachers to create personalized reading spaces and self publish.</li> <li>Community groups - There are lists (and lists of lists) of     nationals research organizations,     in which a researcher can become involved. These older organziations     still rely on official websites, science journal blogs, and email     lists to communicate with their members. In the earth sciences there     are open groups which focus on communication like the Earth Science     Information Partners (ESIP) with     progressive ideas about how data and science can be done. Other     groups, like The Carpentries and     Research Bazaar are     focused on data science training and digital literacy.</li> </ol> <p>Important</p> <pre><code>**Remember** Personal and Professional Accounts are Not Isolated\n\nYou decide what you post on the internet. Your scientist identity may be\na part of your personal identity on social media, it might be separate.\nA future employer or current employer can see your old posts. What you\npost in your personal accounts can be considered a reflection of the\norganization you work for and may be used in decisions about hiring or\ndismissal.\n</code></pre>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#self-paced-material","title":"Self-Paced Material","text":"<ul> <li>15 Data Science Communities to Join</li> <li>Python &amp; Slack</li> <li>Slack CLI notifications</li> <li>Meetups</li> </ul>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#self-assessment-2","title":"Self Assessment 2","text":"True or False: Tutorials and How-to-Guides are the same <p>False</p> <p>Tutorials are in general introductory and longer than How-to-Guides and are intended for teaching learners a new concept by describing applications and providing justifications. </p> <p>How-to-Guides are more like cooking recipes which include step-by-step instructions for a specific task.</p> True or False: Teams should communicate over a single messaging platform. <p>False</p> <p>While it may be advisable to push informal communication toward a platform like SLACK or Microsoft Teams, there is no one-platform-fits-all solution for managing a diverse science team.</p> What is the best communication platform for team science? <p>There is no best platform, but there are some best practices</p> <p>In general, communications amongst team members may be best suited for messaging services like SLACK, Teams, or Chat.</p> <p>For software development, GitHub Issues are one of the primary means of documenting changes and interactions on the web.</p> <p>Formal communication over email is preferred, and is necessary for legal, budgetary, and institutional interactions.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#repeatability-and-reproducibility","title":"Repeatability and Reproducibility","text":"<p>Objectives</p> <p>After this section, you should be able to:</p> <ul> <li>Describe what reproducibility is</li> <li>Discriminate between reproducibility, replicability, and repeatability</li> <li>Explain why reproducible research is valuable </li> <li>Set up a software project with an environment</li> </ul> <p>The so-called reproducibility crisis (see 1 , 2 , 3) is something you have probably heard about (and maybe one of the reasons you have come to FOSS). Headlines in the media (such as Most scientists can't replicate studies by their peers) definitely give pause to researchers and ordinary citizens who hope that the science used to recommend a course of medical treatment or design self-driving cars is sound.</p> <p>Before we go further, it's actually important to ask what is reproducibility?</p> <p>Question</p> <pre><code>How do you define reproducible science?\n</code></pre> Answer <p>In Reproducibility vs. Replicability, Hans Plesser gives the following useful definitions:</p> <ul> <li>Repeatability (Same team, same experimental setup): The measurement     can be obtained with stated precision by the same team using the same     measurement procedure, the same measuring system, under the same operating     conditions, in the same location on multiple trials. For computational     experiments, this means that a researcher can reliably repeat her own     computation.</li> <li>Replicability (Different team, same experimental setup): The     measurement can be obtained with stated precision by a different team     using the same measurement procedure, the same measuring system, under the     same operating conditions, in the same or a different location on multiple     trials. For computational experiments, this means that an independent group     can obtain the same result using the author's own artifacts.</li> <li>Reproducibility (Different team, different experimental setup): The     measurement can be obtained with stated precision by a different team,     a different measuring system, in a different location on multiple trials.     For computational experiments, this means that an independent group can     obtain the same result using artifacts which they develop completely     independently.</li> </ul> <p>The paper goes on to further simplify:</p> <ul> <li>Methods reproducibility: provide sufficient detail about procedures     and data so that the same procedures could be exactly repeated.</li> <li>Results reproducibility: obtain the same results from an independent     study with procedures as closely matched to the original study as     possible.</li> <li>Inferential reproducibility: draw the same conclusions from either an     independent replication of a study or a reanalysis of the original study.</li> </ul>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#defining-reproducibility","title":"Defining Reproducibility","text":"<p>Discussion Question</p> <pre><code>How do these definitions apply to your research/teaching?\n\nWork with your fellow learners to develop a shortlist of ways reproducibility\nrelates to your work. Try to identify challenges and even successes you'd\nlike to share.\n</code></pre> <p>Often, when we say \"reproducibility\" we mean all or at least several of the concepts the proceeding discussion encompasses. Really, reproducibility can be thought of as set values such as some laboratories express in a code of conduct, see for example Ross-Ibarra Lab code of conduct or Bahlai Lab Policies. Reproducibility comes from our obligations and desires to work ethically, honestly, and with confidence that the data and knowledge we produce is done has integrity. Reproducibility is also a \"spectrum of practices\", not a single step. (See figure below from Peng, 2011).</p> <p></p> <p>Assuming you have taken in the potentially anxiety inducing information above, the most important thing to know is that there is a lot of help to make reproducibility a foundation of all of your research.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#repeatability-a-first-step","title":"Repeatability: a first step","text":"<p>A big first step on the road to reproducibility is repeatability. In the context of computation, this means that you should be able to reliably generate the same results.</p> <p>In many ways, this is the biggest hurdle to reproducibility, as it often requires the biggest leap in skills. You can think of repeatability in a few ways.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#software-management","title":"Software Management","text":"<p>Have you ever tried to run a script, only to realize you had updated a package without knowing, and now the script doesn't work? </p> <p>Package managers can be extremely helpful in keeping software versions aligned with projects.</p> <p>In Python, it is common to use <code>pip</code> and a <code>requirements.txt</code> file, and in R, the <code>renv</code> package can be used to keep package versions stable within individual projects.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#automation","title":"Automation","text":"<p>In the process of making your work more repeatable, you will often be trying to reduce the amount of work you're doing \"by hand\". Reducing the human input necessary at each step of a project is a key to reliably reproducing the same results, but it can also help save you a lot of time in the long run.</p> <p>Have you ever manually edited a figure for a manuscript, only to be asked to change something that negated all your manual edits? Well, in the short run, it may have been quicker to just tinker with the graph by hand, but in the long run, figuring out how to use code to generate the whole thing would have saved you time. </p> <p>Automating tasks often comes with an up-front cost, but it is important for the eventual reproducibility of the work, and will often save you time in the short run. </p> <p>Automation also tends to make tasks scale more easily (editing 10 rows of data by hand is fine, editing 10,000 is much harder), adapt to new scenarios, and extend to future projects.</p> <p>While we often think about writing scripts to clean data, run analyses, and generate figures, there are even more parts of a research project that can be automated. Here are a few examples:</p> <ul> <li>data validation</li> <li>model checking/validation</li> <li>software installation</li> <li>report/manuscript generation</li> <li>citation management</li> <li>email/GitHub/Slack notifications</li> <li>workflow itself (using things like make, Snakemake, Nextflow, targets)</li> </ul> <p>Code can be thought of as a set of machine-actionable instructions, or instructions that we write for a computer to follow. What other sets of instructions do you have, either written down or in your head? How can you turn them into something machine-actionable?</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#disposability","title":"Disposability","text":"<p>A great approach to repeatability/reproducbility is to ask \"could I generate my results if I lost X?\"</p> <p>What might happen to your work if:</p> <ul> <li>you changed some code and your script broke?</li> <li>you couldn't find a figure when a journal asked for it?</li> <li>some software got uninstalled from your computer?</li> <li>your laptop got stolen?</li> <li>some software or computing provider stopped being maintained?</li> </ul>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#get-off-your-own-machine","title":"Get off your own machine","text":"<p>More and more work is being done somewhere other than a personal computer. This could be an HPC cluster at a university or a cloud computing provider. \"Cloud\" just means somebody else is handling the computers, and you get to use them when you need to, typically for a price.</p> <p>Non-local computing resources have varying levels of complexity, flexibility, cost, and scale. Some services like Binder, Colab, and VICE try to abstract more of the computational details away, letting you focus on your code (ideally). Others, like Gitpod, Codespaces, or GitHub Actions, have more limited uses (in a good way).</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#dependency-hell","title":"Dependency Hell","text":"<p>Think for a moment about all the branching possibilities for how a computer could be set up:</p> <ul> <li>hardware: CPUs, GPUs, RAM</li> <li>Operating system: many flavors of Linux, MacOS, Windows</li> <li>Software versions: R, Python, etc.</li> <li>Package versions: specific R or Python packages, etc.</li> </ul> <p>Simply trying to get the same setup as anyone else is difficult enough, but you can also run into all sorts of dependencies. Let's say you try to update a package to match the version someone else used for a project. However, after updating it, you realize you need to update 3 other packages. After that, you realize you need a newer version of R. You finally manage to get everything set up, but when you go back to a different project the next week, nothing works! All those updates made your code for your other project break. You spend a week fixing your code to work with the newer software, and you're finally done... but now your advisor gives you a dataset 10x the size and says you'll need to run it on the cloud. You throw your laptop out the window and move to the woods to live the life of a hermit.</p> <p>All jokes aside, dealing with software dependencies can be extremely frustrating, and so can setting stuff up on a remote location. It can be even more frustrating if you're trying to reproduce results but you don't actually know the entire software stack used to generate them.</p> <p>There is a way to handle all of these frustrations at once:</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#containers","title":"Containers","text":"<p>Ok, to be fair, working with containers will also be frustrating. But the beautiful thing about working with containers is that you can handle all of the hard stuff at the start of a project, and you won't have to worry about things changing later on. </p> <p>What are containers?</p> <p>Containers are reproducible computing environments that contain an operating system (OS), software, and even code needed to run analyses. Containers are similar to virtual machines (VMs), but are smaller and easier to share. A big distinction between Containers and VMs is what is within each environment: VMs require the OS to be present within the image, whilst containers rely solemnly on the host OS (and the container engine). </p> <p></p> <p>Source: Microsoft Cloudblogs</p> <p>A popular container platform is Docker(wikipedia, \"what is a Docker container?\"), hosting user created containers on DockerHub, and providing a cross-OS user-friendly toolset for container creation and deployment.</p> <p>RStudio has a number of available Docker containers, each for different use cases and maintained by the Rocker Project. </p> <p>Apptainer (formerly, Singularity), is another popular container engine, which allows you to deploy containers on HPC clusters.</p>"},{"location":"3_Tools_for_reproducible_and_open_science/openscience/#self-assessment-3","title":"Self Assessment 3","text":"<p>What methods have you employed to ensure repeatablility?</p> Has anyone been able to successfully reproduce your work? <p>If not, what could have you done to make your come more reproducible?</p>"}]}